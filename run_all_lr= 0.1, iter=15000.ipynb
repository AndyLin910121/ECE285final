{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ad5b18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhlin/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jhlin/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/494221578_027f51cdf4.jpg: 640x448 1 person, 1 frisbee, 81.5ms\n",
      "Speed: 4.2ms preprocess, 81.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 494221578_027f51cdf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425662680_41c7c50e8d.jpg: 480x640 7 persons, 10 horses, 91.4ms\n",
      "Speed: 4.7ms preprocess, 91.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3425662680_41c7c50e8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2826030193_4278ccb833.jpg: 544x640 4 persons, 1 bed, 77.7ms\n",
      "Speed: 4.3ms preprocess, 77.7ms inference, 1.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2826030193_4278ccb833.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/129599450_cab4e77343.jpg: 480x640 1 person, 1 backpack, 1 handbag, 9.3ms\n",
      "Speed: 3.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 129599450_cab4e77343.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1679557684_50a206e4a9.jpg: 480x640 2 dogs, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1679557684_50a206e4a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/541063419_a5f3672d59.jpg: 480x640 2 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 541063419_a5f3672d59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1557451043_f5c91ff6f4.jpg: 448x640 1 person, 95.3ms\n",
      "Speed: 3.7ms preprocess, 95.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1557451043_f5c91ff6f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866093652_aa717eb9ce.jpg: 640x576 6 persons, 1 baseball bat, 2 skateboards, 78.8ms\n",
      "Speed: 4.8ms preprocess, 78.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2866093652_aa717eb9ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425846980_912943b4f9.jpg: 640x512 8 persons, 1 sports ball, 1 tennis racket, 78.3ms\n",
      "Speed: 3.3ms preprocess, 78.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425846980_912943b4f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2495931537_9b8d4474b6.jpg: 416x640 1 cat, 2 dogs, 76.3ms\n",
      "Speed: 3.2ms preprocess, 76.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2495931537_9b8d4474b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708748633_e7e3cf4e84.jpg: 640x384 1 person, 80.9ms\n",
      "Speed: 2.9ms preprocess, 80.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3708748633_e7e3cf4e84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2943079526_e9033a6556.jpg: 640x480 4 persons, 1 handbag, 1 tie, 1 cell phone, 74.0ms\n",
      "Speed: 3.2ms preprocess, 74.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2943079526_e9033a6556.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1446933195_8fe9725d62.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1446933195_8fe9725d62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2825668136_107223182c.jpg: 640x480 1 person, 1 bicycle, 1 skateboard, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2825668136_107223182c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251234434_d01e25a50a.jpg: 544x640 8 persons, 1 dog, 1 handbag, 6.9ms\n",
      "Speed: 2.2ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3251234434_d01e25a50a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375843443_8d9b242aa5.jpg: 448x640 4 persons, 1 skateboard, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3375843443_8d9b242aa5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2838125339_3dd314e315.jpg: 640x448 2 persons, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2838125339_3dd314e315.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1427391496_ea512cbe7f.jpg: 640x480 2 persons, 1 bench, 1 chair, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1427391496_ea512cbe7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539667015_fd0a3bea07.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 539667015_fd0a3bea07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2657484284_daa07a3a1b.jpg: 480x640 4 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2657484284_daa07a3a1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3206919175_e3a11b6874.jpg: 480x640 2 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3206919175_e3a11b6874.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1355935187_2c99648138.jpg: 448x640 1 teddy bear, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1355935187_2c99648138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2925760802_50c1e84936.jpg: 640x480 11 persons, 1 chair, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2925760802_50c1e84936.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/561940436_64d6fc125d.jpg: 640x480 5 persons, 3 handbags, 1 cup, 1 cell phone, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 561940436_64d6fc125d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/381052465_722e00807b.jpg: 448x640 4 persons, 1 bus, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 3.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 381052465_722e00807b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351357065_a6a9b3d485.jpg: 480x640 3 persons, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3351357065_a6a9b3d485.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159447439_c1496cbaea.jpg: 448x640 1 person, 3 dogs, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3159447439_c1496cbaea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437273677_47d4462974.jpg: 480x640 3 persons, 2 cups, 1 fork, 1 knife, 1 bowl, 1 dining table, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3437273677_47d4462974.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/191592626_477ef5e026.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 191592626_477ef5e026.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690348036_a01f243fb0.jpg: 640x448 2 persons, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3690348036_a01f243fb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/430803349_a66c91f64e.jpg: 640x448 1 dog, 9.8ms\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 430803349_a66c91f64e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3413973568_6630e5cdac.jpg: 416x640 1 person, 2 surfboards, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3413973568_6630e5cdac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218743570_9d6614c51c.jpg: 640x448 1 dog, 1 couch, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2218743570_9d6614c51c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516299821_8f0375d221.jpg: 448x640 3 persons, 1 sports ball, 1 baseball bat, 1 baseball glove, 13.0ms\n",
      "Speed: 3.2ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3516299821_8f0375d221.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2251992614_0c601fae2c.jpg: 480x640 12 persons, 2 umbrellas, 1 handbag, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2251992614_0c601fae2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329169877_175cb16845.jpg: 448x640 1 person, 1 snowboard, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3329169877_175cb16845.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/271637337_0700f307cf.jpg: 640x640 1 dog, 9.7ms\n",
      "Speed: 3.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 271637337_0700f307cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3680031186_c3c6698f9d.jpg: 640x512 8 persons, 1 baseball bat, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3680031186_c3c6698f9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/957230475_48f4285ffe.jpg: 480x640 2 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 957230475_48f4285ffe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099923914_fd450f6d51.jpg: 640x480 10 persons, 1 bottle, 1 cup, 1 chair, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3099923914_fd450f6d51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1056338697_4f7d7ce270.jpg: 448x640 1 person, 2 cars, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1056338697_4f7d7ce270.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3485657956_7481582565.jpg: 480x640 3 dogs, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3485657956_7481582565.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/207584893_63e73c5c28.jpg: 480x640 1 person, 1 kite, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 207584893_63e73c5c28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2555521861_fc36fd3ab0.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2555521861_fc36fd3ab0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3392019836_c7aeebca1c.jpg: 480x640 3 persons, 1 clock, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3392019836_c7aeebca1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2594042571_2e4666507e.jpg: 512x640 2 persons, 76.3ms\n",
      "Speed: 2.4ms preprocess, 76.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2594042571_2e4666507e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346923_18bd84bea4.jpg: 448x640 11 persons, 1 sports ball, 9.2ms\n",
      "Speed: 3.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346923_18bd84bea4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3307667255_26bede91eb.jpg: 448x640 20 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3307667255_26bede91eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229898555_16877f5180.jpg: 640x544 1 cat, 55.1ms\n",
      "Speed: 2.4ms preprocess, 55.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3229898555_16877f5180.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2087317114_cf06df5aa5.jpg: 416x640 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2087317114_cf06df5aa5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320209694_db579cb607.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3320209694_db579cb607.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3284899112_f11ab3cfe6.jpg: 448x640 2 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3284899112_f11ab3cfe6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3278581900_8ce75a5332.jpg: 480x640 2 persons, 1 surfboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3278581900_8ce75a5332.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/408233586_f2c1be3ce1.jpg: 480x640 1 bear, 6.5ms\n",
      "Speed: 2.2ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 408233586_f2c1be3ce1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3103185190_eb8729c166.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3103185190_eb8729c166.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3145967309_b33abe4d84.jpg: 640x448 3 persons, 1 remote, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3145967309_b33abe4d84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1980315248_82dbc34676.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1980315248_82dbc34676.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3369258147_03db257f0f.jpg: 608x640 11 persons, 2 chairs, 1 couch, 1 potted plant, 56.7ms\n",
      "Speed: 2.4ms preprocess, 56.7ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3369258147_03db257f0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2525716531_e6dedee421.jpg: 640x448 1 person, 1 dog, 7.1ms\n",
      "Speed: 2.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2525716531_e6dedee421.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3655074079_7df3812bc5.jpg: 448x640 15 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3655074079_7df3812bc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934325103_e9b8d7430f.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2934325103_e9b8d7430f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330333217_1a69497a74.jpg: 480x640 4 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3330333217_1a69497a74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2420546021_4a59790da6.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2420546021_4a59790da6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/56489627_e1de43de34.jpg: 480x640 1 person, 2 horses, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 56489627_e1de43de34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376898612_41c91de476.jpg: 448x640 5 persons, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376898612_41c91de476.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335692531_dd4a995f91.jpg: 448x640 2 dogs, 1 elephant, 1 clock, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3335692531_dd4a995f91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3317073508_7e13565c1b.jpg: 448x640 7 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3317073508_7e13565c1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2490863987_715383944a.jpg: 640x448 2 persons, 1 car, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2490863987_715383944a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/523327429_af093fc7cf.jpg: 640x448 6 persons, 1 handbag, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 523327429_af093fc7cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3539817989_5353062a39.jpg: 480x640 1 person, 2 birds, 2 cats, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3539817989_5353062a39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3476709230_6439305bf2.jpg: 640x448 4 persons, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3476709230_6439305bf2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/892340814_bdd61e10a4.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 892340814_bdd61e10a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/86412576_c53392ef80.jpg: 512x640 3 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 86412576_c53392ef80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921578694_a46ae0d313.jpg: 448x640 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2921578694_a46ae0d313.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2136455112_202c093ba4.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2136455112_202c093ba4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3476237185_9389c536a3.jpg: 448x640 1 dog, 1 bear, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3476237185_9389c536a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/275401000_8829250eb3.jpg: 544x640 2 dogs, 1 sports ball, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 275401000_8829250eb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2330843604_b8d75d6ac7.jpg: 640x448 1 person, 7 cars, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2330843604_b8d75d6ac7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259160693_067ec7ebc3.jpg: 448x640 18 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259160693_067ec7ebc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/925491651_57df3a5b36.jpg: 448x640 1 bear, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 925491651_57df3a5b36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3460551728_63255cec18.jpg: 480x640 8 persons, 3 cows, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3460551728_63255cec18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2477121456_1ac5c6d3e4.jpg: 448x640 1 bear, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2477121456_1ac5c6d3e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410040397_1a161a1146.jpg: 480x640 1 dog, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2410040397_1a161a1146.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3160531982_37f5636b8a.jpg: 480x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3160531982_37f5636b8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497234632_6ec740fc1e.jpg: 448x640 6 persons, 2 bicycles, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3497234632_6ec740fc1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544673666_ffc7483c96.jpg: 640x576 1 dog, 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3544673666_ffc7483c96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1440024115_129212c988.jpg: 448x640 3 persons, 1 bench, 1 dog, 3 frisbees, 1 sports ball, 1 baseball glove, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1440024115_129212c988.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2751466788_4fab701cc3.jpg: 640x480 8 persons, 3 bicycles, 1 traffic light, 1 umbrella, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2751466788_4fab701cc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470887791_86d5a08a38.jpg: 512x640 6 persons, 1 cell phone, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 470887791_86d5a08a38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697003897_d8ac13be9a.jpg: 448x640 2 persons, 1 bench, 1 skateboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3697003897_d8ac13be9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2609900643_c07bcb0bae.jpg: 640x448 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2609900643_c07bcb0bae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/242558556_12f4d1cabc.jpg: 640x448 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 242558556_12f4d1cabc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/386656845_4e77c3e3da.jpg: 480x640 2 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 386656845_4e77c3e3da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724623861_2bb6c23641.jpg: 640x576 2 persons, 1 skateboard, 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3724623861_2bb6c23641.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3191982761_88793192ed.jpg: 448x640 5 persons, 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3191982761_88793192ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609027309_af75f773d9.jpg: 640x448 3 persons, 1 tv, 10.2ms\n",
      "Speed: 2.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3609027309_af75f773d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2714699748_c9270dd5aa.jpg: 448x640 2 persons, 2 dogs, 1 cow, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2714699748_c9270dd5aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3718305988_fe2c91fd44.jpg: 640x480 1 person, 1 skateboard, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3718305988_fe2c91fd44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2363006088_b3e3aa5c0b.jpg: 480x640 1 person, 1 bench, 4 birds, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2363006088_b3e3aa5c0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432634159_1eb9a73306.jpg: 512x640 9 persons, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3432634159_1eb9a73306.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638440337_6d5c19a8f0.jpg: 448x640 13 persons, 2 bicycles, 2 motorcycles, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3638440337_6d5c19a8f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1143373711_2e90b7b799.jpg: 448x640 5 persons, 1 bicycle, 4 cars, 2 trucks, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1143373711_2e90b7b799.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3493844822_c315a11275.jpg: 448x640 1 person, 1 surfboard, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3493844822_c315a11275.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2258951972_92763fddab.jpg: 416x640 2 dogs, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2258951972_92763fddab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2390778197_4d9d03d4b9.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2390778197_4d9d03d4b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2763601657_09a52a063f.jpg: 512x640 3 persons, 2 benchs, 11.9ms\n",
      "Speed: 3.2ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2763601657_09a52a063f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1466307489_cb8a74de09.jpg: 480x640 3 persons, 1 car, 1 truck, 10.3ms\n",
      "Speed: 2.6ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1466307489_cb8a74de09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516214924_c2a4364cb3.jpg: 640x512 (no detections), 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 516214924_c2a4364cb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1433397131_8634fa6664.jpg: 480x640 10 apples, 3 oranges, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1433397131_8634fa6664.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/55470226_52ff517151.jpg: 480x640 2 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 55470226_52ff517151.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666188047_e81e1d97a7.jpg: 448x640 2 persons, 1 donut, 1 remote, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3666188047_e81e1d97a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/55473406_1d2271c1f2.jpg: 448x640 1 person, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 55473406_1d2271c1f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2534502836_7a75305655.jpg: 480x640 1 dog, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2534502836_7a75305655.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204712107_5a06a81002.jpg: 512x640 2 persons, 1 car, 2 umbrellas, 1 chair, 2 potted plants, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3204712107_5a06a81002.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562470436_6e193643ce.jpg: 480x640 1 person, 4 dogs, 1 frisbee, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3562470436_6e193643ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2123932281_1a4fd38455.jpg: 640x608 1 person, 1 wine glass, 1 cup, 2 chairs, 2 dining tables, 75.9ms\n",
      "Speed: 2.6ms preprocess, 75.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2123932281_1a4fd38455.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/240583223_e26e17ee96.jpg: 640x512 1 dog, 12.0ms\n",
      "Speed: 5.2ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 240583223_e26e17ee96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2747436384_9470c56cb9.jpg: 448x640 4 persons, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2747436384_9470c56cb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3241965735_8742782a70.jpg: 640x448 1 skis, 14.6ms\n",
      "Speed: 3.8ms preprocess, 14.6ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3241965735_8742782a70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2707873672_15e6b5d54b.jpg: 480x640 2 persons, 11.5ms\n",
      "Speed: 2.8ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2707873672_15e6b5d54b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638374272_444f5e0457.jpg: 416x640 10 persons, 1 car, 1 truck, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3638374272_444f5e0457.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102724238_3cf921d7bb.jpg: 640x480 5 persons, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2102724238_3cf921d7bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249891130_b241591e89.jpg: 640x448 5 persons, 3 handbags, 2 refrigerators, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3249891130_b241591e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3502459991_fdec2da131.jpg: 640x448 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3502459991_fdec2da131.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/404890608_33f138aefa.jpg: 448x640 2 persons, 1 frisbee, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 404890608_33f138aefa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2762301555_48a0d0aa24.jpg: 448x640 7 persons, 1 umbrella, 6 chairs, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2762301555_48a0d0aa24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046430047_d7b10123d0.jpg: 512x640 4 persons, 2 cars, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3046430047_d7b10123d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873188959_ff023defa9.jpg: 640x416 1 person, 76.6ms\n",
      "Speed: 1.9ms preprocess, 76.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2873188959_ff023defa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215117062_6e07a86352.jpg: 480x640 6 persons, 3 cars, 1 tv, 9.1ms\n",
      "Speed: 3.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3215117062_6e07a86352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247778426_fd59734130.jpg: 640x448 1 dog, 9.1ms\n",
      "Speed: 2.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 247778426_fd59734130.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2667549961_57e5e2c0a5.jpg: 448x640 1 person, 1 bottle, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2667549961_57e5e2c0a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3661659196_6ed90f96c0.jpg: 480x640 2 persons, 1 suitcase, 1 remote, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3661659196_6ed90f96c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3092200805_dd1f83ddbe.jpg: 416x640 2 persons, 2 boats, 9.1ms\n",
      "Speed: 1.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3092200805_dd1f83ddbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300539993_eede2d6695.jpg: 480x640 1 dog, 2 bears, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 300539993_eede2d6695.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3148286846_40ae914172.jpg: 480x640 3 persons, 1 boat, 1 surfboard, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3148286846_40ae914172.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1055753357_4fa3d8d693.jpg: 480x640 2 persons, 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1055753357_4fa3d8d693.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562050678_4196a7fff3.jpg: 512x640 15 persons, 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3562050678_4196a7fff3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3451344589_6787bd06ef.jpg: 640x384 6 persons, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3451344589_6787bd06ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613667665_1881c689ea.jpg: 448x640 3 persons, 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3613667665_1881c689ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109124656_626b596d5e.jpg: 384x640 3 persons, 1 cup, 1 bowl, 2 couchs, 54.5ms\n",
      "Speed: 1.4ms preprocess, 54.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3109124656_626b596d5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227111573_c82f7d68b1.jpg: 384x640 3 persons, 7.6ms\n",
      "Speed: 2.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3227111573_c82f7d68b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428275562_4bde2bc5ea.jpg: 640x640 1 dog, 7.6ms\n",
      "Speed: 3.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2428275562_4bde2bc5ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2789648482_1df61f224a.jpg: 544x640 3 persons, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2789648482_1df61f224a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3470008804_0ca36a7a09.jpg: 416x640 1 person, 1 elephant, 13.3ms\n",
      "Speed: 3.1ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3470008804_0ca36a7a09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2149982207_5345633bbf.jpg: 640x480 1 person, 1 surfboard, 10.4ms\n",
      "Speed: 2.5ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2149982207_5345633bbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3411579899_0f8ed09142.jpg: 448x640 1 motorcycle, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3411579899_0f8ed09142.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3711611500_ea47b58b6f.jpg: 448x640 1 boat, 1 dog, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3711611500_ea47b58b6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496380034_d22aeeedb3.jpg: 448x640 2 dogs, 1 frisbee, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 496380034_d22aeeedb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/96973080_783e375945.jpg: 448x640 1 dog, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 96973080_783e375945.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/465994762_1760e83c5d.jpg: 448x640 1 cow, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 465994762_1760e83c5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/862054277_34b5a6f401.jpg: 640x480 1 person, 13.5ms\n",
      "Speed: 3.4ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 862054277_34b5a6f401.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3455898176_f0e003ce58.jpg: 480x640 6 persons, 1 dog, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3455898176_f0e003ce58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/467960888_6943257534.jpg: 448x640 1 bird, 1 dog, 13.4ms\n",
      "Speed: 3.2ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 467960888_6943257534.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256274183_4eab3b2322.jpg: 480x640 2 persons, 12 cars, 10.8ms\n",
      "Speed: 2.5ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3256274183_4eab3b2322.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3210457502_c6030ce567.jpg: 544x640 1 bird, 14.9ms\n",
      "Speed: 4.2ms preprocess, 14.9ms inference, 2.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3210457502_c6030ce567.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414352262_005ae90407.jpg: 480x640 2 persons, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2414352262_005ae90407.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3494723363_eaa6bc563b.jpg: 448x640 2 persons, 1 surfboard, 10.9ms\n",
      "Speed: 2.4ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3494723363_eaa6bc563b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2269021076_cefc9af989.jpg: 480x640 1 person, 1 bicycle, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2269021076_cefc9af989.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192802444_b14bb87b95.jpg: 640x448 7 persons, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2192802444_b14bb87b95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3361990489_92244a58ef.jpg: 640x480 1 person, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3361990489_92244a58ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2945036454_280fa5b29f.jpg: 448x640 1 dog, 1 frisbee, 9.7ms\n",
      "Speed: 2.0ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2945036454_280fa5b29f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/557101732_32bbc47c12.jpg: 480x640 2 persons, 2 bicycles, 3 cars, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 557101732_32bbc47c12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3471066276_fb1e82e905.jpg: 416x640 2 dogs, 9.5ms\n",
      "Speed: 1.9ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3471066276_fb1e82e905.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1468103286_96a6e07029.jpg: 640x448 7 persons, 14.2ms\n",
      "Speed: 3.6ms preprocess, 14.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1468103286_96a6e07029.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/447800028_0242008fa3.jpg: 480x640 1 dog, 12.4ms\n",
      "Speed: 3.0ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 447800028_0242008fa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3005126574_49c28ffbee.jpg: 640x640 1 person, 10.6ms\n",
      "Speed: 3.5ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3005126574_49c28ffbee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2707244524_d57120d74a.jpg: 448x640 1 dog, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2707244524_d57120d74a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2746910139_77ba5be2c5.jpg: 448x640 4 persons, 1 chair, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2746910139_77ba5be2c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2943384009_c8cf749181.jpg: 448x640 1 person, 1 motorcycle, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2943384009_c8cf749181.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855417531_521bf47b50.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2855417531_521bf47b50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211437611_bd4af3730b.jpg: 544x640 9 persons, 5 ties, 1 chair, 1 vase, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3211437611_bd4af3730b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2737729252_b3fd9c05b1.jpg: 640x448 3 persons, 1 bench, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2737729252_b3fd9c05b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3567604049_da9e1be4ba.jpg: 640x448 1 person, 1 chair, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3567604049_da9e1be4ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346401538_f5e8da66fc.jpg: 416x640 2 persons, 2 handbags, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2346401538_f5e8da66fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3465606652_f380a38050.jpg: 448x640 1 person, 1 car, 1 parking meter, 1 frisbee, 13.1ms\n",
      "Speed: 3.1ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3465606652_f380a38050.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/753578547_912d2b4048.jpg: 640x480 1 dog, 1 sports ball, 10.5ms\n",
      "Speed: 2.5ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 753578547_912d2b4048.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612538549_2828b45867.jpg: 640x448 1 person, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3612538549_2828b45867.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2056041678_d6b5b39b26.jpg: 416x640 1 person, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2056041678_d6b5b39b26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424506167_01f365726b.jpg: 608x640 2 dogs, 1 horse, 9.6ms\n",
      "Speed: 2.8ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 424506167_01f365726b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3635577874_48ebaac734.jpg: 640x448 1 person, 1 bicycle, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3635577874_48ebaac734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647826834_dc63e21bd0.jpg: 448x640 5 persons, 1 chair, 1 dining table, 9.5ms\n",
      "Speed: 2.0ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3647826834_dc63e21bd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/166433861_70b66cd381.jpg: 448x640 1 person, 1 motorcycle, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 166433861_70b66cd381.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95783195_e1ba3f57ca.jpg: 480x640 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 95783195_e1ba3f57ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482237861_605b4f0fd9.jpg: 640x448 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3482237861_605b4f0fd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/111537222_07e56d5a30.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 111537222_07e56d5a30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477672764_7f07657a26.jpg: 448x640 4 persons, 1 sports ball, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3477672764_7f07657a26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3745451546_fc8ec70cbd.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3745451546_fc8ec70cbd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523474077_16e14bc54c.jpg: 448x640 11 persons, 1 airplane, 1 backpack, 1 cell phone, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3523474077_16e14bc54c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/358559906_d5f3f584f4.jpg: 640x448 2 persons, 1 fire hydrant, 9.7ms\n",
      "Speed: 5.2ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 358559906_d5f3f584f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2619267133_53a5904ef4.jpg: 640x448 3 persons, 2 horses, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2619267133_53a5904ef4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426966595_c8c4e1e872.jpg: 416x640 4 persons, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3426966595_c8c4e1e872.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2708686056_1b8f356264.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2708686056_1b8f356264.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2936707421_8e629874b8.jpg: 448x640 2 persons, 1 sports ball, 1 kite, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2936707421_8e629874b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540416981_4e74f08cbb.jpg: 640x480 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3540416981_4e74f08cbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2541901399_0a57f4cc76.jpg: 416x640 2 persons, 1 bicycle, 1 bench, 12.8ms\n",
      "Speed: 3.1ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2541901399_0a57f4cc76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2333584535_1eaf9baf3e.jpg: 448x640 3 dogs, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2333584535_1eaf9baf3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2186367337_0ce9ce2104.jpg: 512x640 1 person, 1 dog, 10.4ms\n",
      "Speed: 2.6ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2186367337_0ce9ce2104.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225226381_9fe306fb9e.jpg: 320x640 2 dogs, 96.4ms\n",
      "Speed: 2.7ms preprocess, 96.4ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3225226381_9fe306fb9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/575636303_b0b8fd4eee.jpg: 480x640 10 persons, 9.3ms\n",
      "Speed: 3.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 575636303_b0b8fd4eee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3181599388_68559cfc17.jpg: 448x640 12 persons, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3181599388_68559cfc17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/394161692_2576920777.jpg: 512x640 7 persons, 1 bicycle, 3 umbrellas, 1 potted plant, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 394161692_2576920777.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/35506150_cbdb630f4f.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 35506150_cbdb630f4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/118187095_d422383c81.jpg: 480x640 13 persons, 1 skateboard, 13.7ms\n",
      "Speed: 3.8ms preprocess, 13.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 118187095_d422383c81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892467862_52a3c67418.jpg: 640x448 1 person, 11.3ms\n",
      "Speed: 2.7ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2892467862_52a3c67418.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346434_0527ea1c07.jpg: 448x640 8 persons, 1 frisbee, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346434_0527ea1c07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2546959333_23b957988f.jpg: 512x640 1 dog, 9.6ms\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2546959333_23b957988f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2196284168_76417efbec.jpg: 640x416 1 person, 9.5ms\n",
      "Speed: 2.0ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2196284168_76417efbec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540751930_d71c7f5622.jpg: 480x640 9 persons, 1 chair, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2540751930_d71c7f5622.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2474918824_88660c7757.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2474918824_88660c7757.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173215794_6bdd1f72d4.jpg: 384x640 11 persons, 1 handbag, 10.6ms\n",
      "Speed: 2.1ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3173215794_6bdd1f72d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3340575518_137ce2695f.jpg: 320x640 4 persons, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3340575518_137ce2695f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656030945_fa003bd696.jpg: 480x640 2 persons, 1 dog, 1 bed, 1 remote, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3656030945_fa003bd696.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351586010_7ffaa90ea8.jpg: 640x480 3 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3351586010_7ffaa90ea8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/235074044_c1358888ed.jpg: 480x640 18 persons, 2 handbags, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 235074044_c1358888ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1772859261_236c09b861.jpg: 448x640 1 dog, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1772859261_236c09b861.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/504765160_b4b083b293.jpg: 640x480 2 persons, 12.5ms\n",
      "Speed: 3.0ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 504765160_b4b083b293.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921112724_5cb85d7413.jpg: 480x640 4 persons, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2921112724_5cb85d7413.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560081723_62da0035bd.jpg: 640x448 2 persons, 1 motorcycle, 13.4ms\n",
      "Speed: 3.1ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3560081723_62da0035bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197917064_e679a44b8e.jpg: 480x640 1 dog, 11.1ms\n",
      "Speed: 2.7ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3197917064_e679a44b8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523559027_a65619a34b.jpg: 480x640 10 persons, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3523559027_a65619a34b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3413806271_17b7e102aa.jpg: 480x640 3 persons, 2 kites, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3413806271_17b7e102aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2660480624_45f88b3022.jpg: 448x640 2 persons, 8 cars, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2660480624_45f88b3022.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2052202553_373dad145b.jpg: 352x640 1 dog, 75.1ms\n",
      "Speed: 1.6ms preprocess, 75.1ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2052202553_373dad145b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3634828052_3b6aeda7d6.jpg: 640x480 6 persons, 7 cars, 9.4ms\n",
      "Speed: 3.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3634828052_3b6aeda7d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2748729903_3c7c920c4d.jpg: 448x640 3 persons, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2748729903_3c7c920c4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341961913_9a9b362f15.jpg: 448x640 2 persons, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3341961913_9a9b362f15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341489212_a879e1544a.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3341489212_a879e1544a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265864834_e0229020dd.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3265864834_e0229020dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085726719_a57a75dbe5.jpg: 640x608 3 persons, 2 cups, 1 cell phone, 9.2ms\n",
      "Speed: 4.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2085726719_a57a75dbe5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/463875230_f19e83d6df.jpg: 640x384 1 person, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 463875230_f19e83d6df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3712008738_1e1fa728da.jpg: 448x640 4 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3712008738_1e1fa728da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460781612_6815c74d37.jpg: 640x640 1 bird, 1 dog, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 460781612_6815c74d37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2510197716_fddca0ac75.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2510197716_fddca0ac75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443853670_6c79fcfcb2.jpg: 480x640 1 person, 1 motorcycle, 1 cow, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3443853670_6c79fcfcb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2534652796_c8a23288ab.jpg: 480x640 2 persons, 1 sports ball, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2534652796_c8a23288ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/486712504_36be449055.jpg: 512x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 486712504_36be449055.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468930779_8008d90e10.jpg: 480x640 2 bears, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 468930779_8008d90e10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3108732084_565b423162.jpg: 448x640 3 persons, 1 snowboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3108732084_565b423162.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2976537455_f3988c2d46.jpg: 640x576 1 person, 3 baseball bats, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2976537455_f3988c2d46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3725177385_62d5e13634.jpg: 480x640 5 persons, 3 cars, 1 motorcycle, 3 trucks, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3725177385_62d5e13634.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2188192752_09d9fc5431.jpg: 544x640 6 persons, 4 cars, 2 frisbees, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2188192752_09d9fc5431.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3167365436_c379bda282.jpg: 448x640 2 persons, 2 cell phones, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3167365436_c379bda282.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2226534154_cbcab7ba32.jpg: 640x608 6 persons, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2226534154_cbcab7ba32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673484638_dce87295fe.jpg: 640x448 11 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3673484638_dce87295fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899276965_a20b839cfd.jpg: 448x640 1 person, 1 truck, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2899276965_a20b839cfd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364258732_9942c557e5.jpg: 416x640 10 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3364258732_9942c557e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1466479163_439db855af.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1466479163_439db855af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106883334_419f3fb16f.jpg: 640x448 2 dogs, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3106883334_419f3fb16f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639428663_dae5e8146e.jpg: 480x640 7 persons, 1 suitcase, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3639428663_dae5e8146e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2559921948_06af25d566.jpg: 640x416 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2559921948_06af25d566.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2882589788_cb0b407a8d.jpg: 448x640 6 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2882589788_cb0b407a8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2937461473_11bbace28c.jpg: 448x640 12 persons, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2937461473_11bbace28c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3456579559_b5c8927938.jpg: 640x448 1 person, 1 skateboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3456579559_b5c8927938.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484841598_e26ee96aab.jpg: 448x640 4 persons, 1 baseball glove, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3484841598_e26ee96aab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2685139184_4ff45e0f76.jpg: 480x640 3 persons, 1 car, 1 motorcycle, 1 umbrella, 1 bowl, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2685139184_4ff45e0f76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3108544687_c7115823f5.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3108544687_c7115823f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3352199368_b35f25793e.jpg: 480x640 2 persons, 1 backpack, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3352199368_b35f25793e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2546667441_bbe87a6285.jpg: 480x640 1 dog, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2546667441_bbe87a6285.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2500826039_165e75b20c.jpg: 640x480 7 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2500826039_165e75b20c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441104823_33cdae5a56.jpg: 480x640 25 persons, 3 cars, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3441104823_33cdae5a56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251976937_20625dc2b8.jpg: 448x640 1 dog, 1 kite, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3251976937_20625dc2b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/313051099_1bb87d6c56.jpg: 480x640 1 bear, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 313051099_1bb87d6c56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2578003921_e23b78e85f.jpg: 640x448 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2578003921_e23b78e85f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2774362575_7543b8bf19.jpg: 544x640 1 dog, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2774362575_7543b8bf19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182558164_488b819f14.jpg: 640x448 7 persons, 1 sports ball, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3182558164_488b819f14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3715559023_70c41b31c7.jpg: 640x640 1 person, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3715559023_70c41b31c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690431163_1d81e19549.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3690431163_1d81e19549.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2922807898_b5a06d5c70.jpg: 480x640 1 car, 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2922807898_b5a06d5c70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227140905_1d7e30e4c4.jpg: 416x640 2 persons, 1 dining table, 2 remotes, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3227140905_1d7e30e4c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2631625732_75b714e685.jpg: 480x640 4 persons, 1 boat, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2631625732_75b714e685.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3067971348_69af5bb309.jpg: 448x640 8 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3067971348_69af5bb309.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578841731_f775cab089.jpg: 448x640 1 person, 1 car, 1 truck, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3578841731_f775cab089.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2385871317_44cde2f354.jpg: 448x640 1 person, 1 handbag, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2385871317_44cde2f354.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046259614_614394e024.jpg: 448x640 5 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3046259614_614394e024.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/288508162_1727873924.jpg: 640x480 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 288508162_1727873924.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2092870249_90e3f1855b.jpg: 448x640 1 person, 2 cars, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2092870249_90e3f1855b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587781729_bd21ce7b11.jpg: 640x448 2 persons, 3 cars, 1 bench, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3587781729_bd21ce7b11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/781118358_19087c9ec0.jpg: 480x640 1 person, 1 umbrella, 1 bottle, 1 bed, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 781118358_19087c9ec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460350019_af60511a3b.jpg: 448x640 1 person, 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 460350019_af60511a3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3455419642_894d03f153.jpg: 640x448 1 person, 1 dog, 1 sports ball, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3455419642_894d03f153.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1285067106_2adc307240.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1285067106_2adc307240.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3068735836_872fba3068.jpg: 448x640 12 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3068735836_872fba3068.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3255017708_2b02bfcdcf.jpg: 448x640 1 person, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3255017708_2b02bfcdcf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662537919_18a29fca8a.jpg: 640x448 1 person, 1 tennis racket, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2662537919_18a29fca8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2251747182_6b67a3ab8b.jpg: 640x448 3 persons, 1 sports ball, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2251747182_6b67a3ab8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1141718391_24164bf1b1.jpg: 640x608 2 persons, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1141718391_24164bf1b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3722507770_0d6cb7420e.jpg: 640x448 2 persons, 1 bicycle, 1 car, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3722507770_0d6cb7420e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2088532947_c628e44c4a.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2088532947_c628e44c4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613585080_36629d8157.jpg: 448x640 5 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3613585080_36629d8157.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490867290_13bcd3a7f0.jpg: 448x640 2 persons, 1 cow, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3490867290_13bcd3a7f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3047264032_14393ecea8.jpg: 512x640 4 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3047264032_14393ecea8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3725353555_75c346d7ec.jpg: 512x640 1 bird, 1 bear, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3725353555_75c346d7ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3507670136_2e5f94accf.jpg: 640x448 3 persons, 1 motorcycle, 1 boat, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3507670136_2e5f94accf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/345684566_235e8dfcc1.jpg: 512x640 1 dog, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 345684566_235e8dfcc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256085101_2c2617c5d0.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 256085101_2c2617c5d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2888408966_376c195b3f.jpg: 640x448 2 persons, 2 skateboards, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2888408966_376c195b3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3283626303_8e23d4a842.jpg: 480x640 5 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3283626303_8e23d4a842.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3128164023_ebe8da4c32.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3128164023_ebe8da4c32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2374652725_32f90fa15c.jpg: 480x640 1 dog, 1 bear, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2374652725_32f90fa15c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/97105139_fae46fe8ef.jpg: 480x640 2 persons, 1 car, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 97105139_fae46fe8ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694071771_ce760db4c7.jpg: 640x448 1 person, 1 bicycle, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3694071771_ce760db4c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/823675317_b5fbdb17b3.jpg: 448x640 13 persons, 1 sports ball, 1 cup, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 823675317_b5fbdb17b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126225245_96cd2c053f.jpg: 448x640 4 persons, 1 sports ball, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3126225245_96cd2c053f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2898304260_a4099a193a.jpg: 640x448 2 persons, 1 skateboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2898304260_a4099a193a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208999896_dab42dc40b.jpg: 480x640 9 persons, 2 ties, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3208999896_dab42dc40b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2865703567_52de2444f2.jpg: 640x512 2 persons, 1 skateboard, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2865703567_52de2444f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242919570_39a05aa2ee.jpg: 512x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3242919570_39a05aa2ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/296873864_4de75de261.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 296873864_4de75de261.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3157847991_463e006a28.jpg: 512x640 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3157847991_463e006a28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134387513_ceb75bea0a.jpg: 640x512 3 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3134387513_ceb75bea0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1425069590_570cc7c2d8.jpg: 640x448 1 person, 1 car, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1425069590_570cc7c2d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724738804_f00748a137.jpg: 640x512 4 persons, 1 chair, 12.4ms\n",
      "Speed: 3.5ms preprocess, 12.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3724738804_f00748a137.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2522809984_2e8a7df4fb.jpg: 448x640 1 bird, 1 bear, 10.7ms\n",
      "Speed: 2.6ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2522809984_2e8a7df4fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639547922_0b00fed5cd.jpg: 640x480 1 person, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3639547922_0b00fed5cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3674168459_6245f4f658.jpg: 448x640 11 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3674168459_6245f4f658.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2982928615_06db40f4cd.jpg: 448x640 1 dog, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2982928615_06db40f4cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2517082705_93bc9f73ec.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2517082705_93bc9f73ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1417882092_c94c251eb3.jpg: 640x448 3 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1417882092_c94c251eb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/295433203_8185c13e08.jpg: 512x640 1 person, 1 horse, 1 sheep, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 295433203_8185c13e08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1932314876_9cc46fd054.jpg: 480x640 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1932314876_9cc46fd054.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3688839836_ba5e4c24fc.jpg: 640x416 5 persons, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3688839836_ba5e4c24fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582914905_f58db879ae.jpg: 448x640 1 person, 1 surfboard, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3582914905_f58db879ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428094795_d3a8f46046.jpg: 480x640 2 persons, 1 motorcycle, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2428094795_d3a8f46046.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3366571152_20afb88ac1.jpg: 640x640 2 persons, 1 bench, 2 chairs, 1 tv, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3366571152_20afb88ac1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884092603_786b53a74b.jpg: 640x512 2 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2884092603_786b53a74b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271264741_aa8f73f87c.jpg: 480x640 2 dogs, 1 sheep, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2271264741_aa8f73f87c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439382048_d2e23b2b4c.jpg: 640x448 3 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3439382048_d2e23b2b4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500399969_f54ce5848f.jpg: 448x640 8 persons, 1 surfboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3500399969_f54ce5848f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1309330801_aeeb23f1ee.jpg: 448x640 2 persons, 1 boat, 13.1ms\n",
      "Speed: 3.8ms preprocess, 13.1ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1309330801_aeeb23f1ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/401079494_562454c4d6.jpg: 448x640 2 dogs, 10.6ms\n",
      "Speed: 2.7ms preprocess, 10.6ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 401079494_562454c4d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2328616978_fb21be2b87.jpg: 480x640 6 persons, 3 cars, 1 handbag, 1 suitcase, 10.6ms\n",
      "Speed: 2.9ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2328616978_fb21be2b87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2164363131_6930455d45.jpg: 480x640 1 person, 1 dog, 1 frisbee, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2164363131_6930455d45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330675488_8692476a4a.jpg: 480x640 2 persons, 1 skis, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3330675488_8692476a4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3463268965_f22884fc69.jpg: 480x640 2 persons, 3 cars, 1 bus, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3463268965_f22884fc69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1490670858_e122df2560.jpg: 512x640 1 person, 1 bench, 9.3ms\n",
      "Speed: 2.5ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1490670858_e122df2560.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514179514_cbc3371b92.jpg: 448x640 10 persons, 1 car, 1 sports ball, 1 baseball glove, 2 chairs, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514179514_cbc3371b92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482047956_9a29e9cee6.jpg: 480x640 4 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 482047956_9a29e9cee6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2731171552_4a808c7d5a.jpg: 640x512 3 persons, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2731171552_4a808c7d5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3434526008_02359881a0.jpg: 448x640 4 persons, 1 train, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3434526008_02359881a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2471493912_2d4746b834.jpg: 640x480 9 persons, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2471493912_2d4746b834.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/510791586_3913ade6a7.jpg: 480x640 4 persons, 1 boat, 1 kite, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 510791586_3913ade6a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2607383384_d9ce9de793.jpg: 416x640 13 persons, 1 baseball bat, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2607383384_d9ce9de793.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1676601498_7d59327523.jpg: 384x640 2 persons, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1676601498_7d59327523.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866254827_9a8f592017.jpg: 448x640 2 persons, 2 motorcycles, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2866254827_9a8f592017.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271671533_7538ccd556.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2271671533_7538ccd556.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2215136723_960edfea49.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2215136723_960edfea49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/487487795_54705c406e.jpg: 384x640 2 dogs, 14.6ms\n",
      "Speed: 3.2ms preprocess, 14.6ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 487487795_54705c406e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421104520_6a71185b3c.jpg: 448x640 1 person, 2 sports balls, 6 apples, 3 oranges, 14.2ms\n",
      "Speed: 3.4ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421104520_6a71185b3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1160034462_16b38174fe.jpg: 480x640 1 person, 1 bench, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1160034462_16b38174fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604384383_db6805d1b9.jpg: 448x640 1 person, 10.4ms\n",
      "Speed: 2.3ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3604384383_db6805d1b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405537503_f66ecc5073.jpg: 640x448 1 person, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 405537503_f66ecc5073.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726730085_2468ee9220.jpg: 640x448 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3726730085_2468ee9220.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677964239_6406ed096f.jpg: 640x640 3 persons, 1 car, 2 benchs, 2 handbags, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3677964239_6406ed096f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839932205_3c9c27cd99.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2839932205_3c9c27cd99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/435054077_3506dbfcf4.jpg: 480x640 2 persons, 3 dogs, 1 sheep, 1 cow, 1 handbag, 1 bottle, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 435054077_3506dbfcf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1312020846_5abb4a9be2.jpg: 480x640 2 dogs, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1312020846_5abb4a9be2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/344841963_8b0fa9784c.jpg: 448x640 3 persons, 4 cars, 1 umbrella, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 344841963_8b0fa9784c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3043685748_130db75e3b.jpg: 480x640 8 persons, 8 motorcycles, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3043685748_130db75e3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286236765_2a63eeb550.jpg: 448x640 2 persons, 5 cars, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2286236765_2a63eeb550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949337912_beba55698b.jpg: 256x640 1 person, 1 surfboard, 75.5ms\n",
      "Speed: 1.4ms preprocess, 75.5ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2949337912_beba55698b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2974501005_346f74e5d8.jpg: 640x448 1 person, 1 skateboard, 9.3ms\n",
      "Speed: 2.6ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2974501005_346f74e5d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1581822598_0ae23074f1.jpg: 640x480 1 dog, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1581822598_0ae23074f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1598085252_f3219b6140.jpg: 480x640 1 person, 1 cow, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1598085252_f3219b6140.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1143882946_1898d2eeb9.jpg: 480x640 1 person, 2 bicycles, 2 cars, 8.5ms\n",
      "Speed: 2.6ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1143882946_1898d2eeb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2525455265_f84ba72bd7.jpg: 544x640 1 person, 2 benchs, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2525455265_f84ba72bd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2452686995_621878f561.jpg: 640x480 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2452686995_621878f561.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/513269597_c38308feaf.jpg: 448x640 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 513269597_c38308feaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2084103826_ffd76b1e3e.jpg: 416x640 1 dog, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2084103826_ffd76b1e3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2947452329_08f2d2a467.jpg: 640x480 8 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2947452329_08f2d2a467.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2851198725_37b6027625.jpg: 640x480 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2851198725_37b6027625.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2786245676_0a69543832.jpg: 640x640 1 person, 1 bed, 7.1ms\n",
      "Speed: 2.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2786245676_0a69543832.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396043950_12783c5147.jpg: 480x640 15 persons, 5 chairs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3396043950_12783c5147.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376972502_35e3e119cd.jpg: 448x640 1 person, 1 kite, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376972502_35e3e119cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2923825744_ca125353f0.jpg: 448x640 2 persons, 1 baseball glove, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2923825744_ca125353f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3278189732_f750cb26b7.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3278189732_f750cb26b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/207237775_fa0a15c6fe.jpg: 416x640 2 persons, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 207237775_fa0a15c6fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3691622437_f13644273c.jpg: 480x640 10 persons, 1 chair, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3691622437_f13644273c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3095137758_bdd1e613dd.jpg: 448x640 5 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3095137758_bdd1e613dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2429978680_1e18a13835.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2429978680_1e18a13835.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143237785_93f81b3201.jpg: 480x640 2 persons, 1 dog, 1 sports ball, 14.1ms\n",
      "Speed: 4.1ms preprocess, 14.1ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 143237785_93f81b3201.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2490179961_e842fda5eb.jpg: 416x640 2 persons, 11.2ms\n",
      "Speed: 2.5ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2490179961_e842fda5eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1580671272_3e99d94305.jpg: 480x640 13 persons, 1 car, 1 truck, 2 horses, 1 cow, 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1580671272_3e99d94305.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3716277216_c04002be81.jpg: 480x640 5 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3716277216_c04002be81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107059919_0594269f72.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3107059919_0594269f72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629346153_155ba73ae9.jpg: 640x448 1 person, 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2629346153_155ba73ae9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884400562_e0851014fc.jpg: 640x448 1 person, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2884400562_e0851014fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3236447445_eecafdf4f0.jpg: 640x448 2 persons, 1 backpack, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3236447445_eecafdf4f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331797838_b3e33dbe17.jpg: 288x640 2 persons, 3 bicycles, 73.3ms\n",
      "Speed: 1.4ms preprocess, 73.3ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3331797838_b3e33dbe17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1379026456_153fd8b51b.jpg: 640x448 2 persons, 1 chair, 1 laptop, 9.1ms\n",
      "Speed: 2.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1379026456_153fd8b51b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517023411_a8fbd15230.jpg: 640x448 11 persons, 1 bicycle, 2 backpacks, 8.4ms\n",
      "Speed: 2.6ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3517023411_a8fbd15230.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616643090_4f2d2d1a44.jpg: 576x640 1 person, 75.4ms\n",
      "Speed: 3.8ms preprocess, 75.4ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2616643090_4f2d2d1a44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998024845_1529c11694.jpg: 640x448 1 person, 9.1ms\n",
      "Speed: 3.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2998024845_1529c11694.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2944025729_5aecda30ee.jpg: 448x640 1 airplane, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2944025729_5aecda30ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2900274587_f2cbca4c58.jpg: 640x448 4 persons, 1 boat, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2900274587_f2cbca4c58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3205214191_29b42b9b09.jpg: 448x640 3 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3205214191_29b42b9b09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2968216482_ede65b20a8.jpg: 480x640 2 persons, 3 motorcycles, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2968216482_ede65b20a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3711826708_bba64fb1e1.jpg: 640x384 1 dog, 8.4ms\n",
      "Speed: 1.5ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3711826708_bba64fb1e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3358380484_b99b48f0c9.jpg: 448x640 1 dog, 1 frisbee, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3358380484_b99b48f0c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3573436368_78f0ccdf01.jpg: 416x640 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3573436368_78f0ccdf01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2878578240_caf64c3b19.jpg: 640x512 4 persons, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2878578240_caf64c3b19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2751694538_fffa3d307d.jpg: 480x640 1 person, 1 truck, 13.5ms\n",
      "Speed: 3.8ms preprocess, 13.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2751694538_fffa3d307d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264650118_be7df266e7.jpg: 480x640 2 persons, 1 backpack, 10.4ms\n",
      "Speed: 3.0ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3264650118_be7df266e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2976350388_3984e3193d.jpg: 640x576 1 person, 1 bicycle, 9.9ms\n",
      "Speed: 3.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2976350388_3984e3193d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2103361407_4ed4fc46bf.jpg: 480x640 5 persons, 1 bench, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2103361407_4ed4fc46bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3664928753_7b0437fedf.jpg: 640x448 7 persons, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3664928753_7b0437fedf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3125158798_0743dae56e.jpg: 640x448 3 persons, 1 tie, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3125158798_0743dae56e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1813777902_07d1d4b00c.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1813777902_07d1d4b00c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1786425974_c7c5ad6aa1.jpg: 448x640 1 person, 1 dog, 1 cow, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1786425974_c7c5ad6aa1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2842439618_fb20fe2215.jpg: 416x640 2 persons, 1 surfboard, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2842439618_fb20fe2215.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540326842_bb26cec999.jpg: 448x640 9 persons, 1 handbag, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2540326842_bb26cec999.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263141261_db3a4798b5.jpg: 640x448 2 persons, 1 umbrella, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3263141261_db3a4798b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474176841_cde2bee67c.jpg: 640x448 1 person, 1 bicycle, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3474176841_cde2bee67c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/106490881_5a2dd9b7bd.jpg: 640x448 3 persons, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 106490881_5a2dd9b7bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/387830531_e89c192b92.jpg: 448x640 3 persons, 2 dogs, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 387830531_e89c192b92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2379150102_157d718d1d.jpg: 448x640 2 dogs, 1 horse, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2379150102_157d718d1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/282116218_7fd7583d6e.jpg: 576x640 1 dog, 1 frisbee, 1 sports ball, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 282116218_7fd7583d6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2698119128_62b4741043.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2698119128_62b4741043.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3599392711_8264881de2.jpg: 640x480 4 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3599392711_8264881de2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278559394_b23af734b9.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278559394_b23af734b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/438639005_208bc59b0b.jpg: 480x640 2 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 438639005_208bc59b0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2847615962_c330bded6e.jpg: 480x640 1 sheep, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2847615962_c330bded6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1469000260_5d473c8283.jpg: 384x640 1 dog, 8.8ms\n",
      "Speed: 1.8ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1469000260_5d473c8283.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371533654_73a12a35a4.jpg: 448x640 1 person, 1 car, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3371533654_73a12a35a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3707077198_efd6aa808d.jpg: 448x640 4 persons, 1 skateboard, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3707077198_efd6aa808d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2445783904_e6c38a3a3d.jpg: 512x640 1 dog, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2445783904_e6c38a3a3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/366713533_bd6d48cf02.jpg: 480x640 9 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 366713533_bd6d48cf02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457604528_302396c08c.jpg: 640x448 1 person, 3 cars, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3457604528_302396c08c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1895768965_43cd9d164f.jpg: 448x640 1 dog, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1895768965_43cd9d164f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2976946039_fb9147908d.jpg: 320x640 4 persons, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2976946039_fb9147908d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2142232919_c857a09dd7.jpg: 640x512 1 dog, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2142232919_c857a09dd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3386375153_20c56d0aae.jpg: 448x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3386375153_20c56d0aae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541162969_68fa4a60df.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3541162969_68fa4a60df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2823075967_be4c350e9e.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2823075967_be4c350e9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405113041_4b72c24801.jpg: 544x640 1 bird, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3405113041_4b72c24801.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2194797921_96af7a9467.jpg: 448x640 6 persons, 2 backpacks, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2194797921_96af7a9467.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3169276423_6918dd4da1.jpg: 448x640 2 persons, 2 toothbrushs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3169276423_6918dd4da1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3039675864_0b7961844d.jpg: 480x640 9 persons, 2 cars, 1 motorcycle, 1 truck, 1 bench, 2 birds, 1 handbag, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3039675864_0b7961844d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319870744_0e0138d205.jpg: 640x448 1 person, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 319870744_0e0138d205.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987121689_f9de6c479b.jpg: 544x640 4 persons, 2 sports balls, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2987121689_f9de6c479b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708172446_4034ddc5f6.jpg: 640x448 3 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3708172446_4034ddc5f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2295750198_6d152d7ceb.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2295750198_6d152d7ceb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2735158990_56ff6bf9b0.jpg: 448x640 1 bird, 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2735158990_56ff6bf9b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/124972799_de706b6d0b.jpg: 480x640 1 cat, 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 124972799_de706b6d0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502354602_f4d6dcf42d.jpg: 448x640 2 persons, 1 tie, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2502354602_f4d6dcf42d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2615811117_42b1838205.jpg: 640x480 2 persons, 1 chair, 1 couch, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2615811117_42b1838205.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/406642021_9ec852eccf.jpg: 448x640 1 person, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 406642021_9ec852eccf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042173467_14394234da.jpg: 448x640 5 persons, 1 tv, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3042173467_14394234da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3643684044_a131168127.jpg: 448x640 4 persons, 4 bottles, 5 cups, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3643684044_a131168127.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325578605_afa7f662ec.jpg: 448x640 1 boat, 1 bird, 1 kite, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3325578605_afa7f662ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/501684722_0f20c4e704.jpg: 480x640 2 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 501684722_0f20c4e704.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3270273940_61ef506f05.jpg: 640x480 2 persons, 1 snowboard, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3270273940_61ef506f05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/276356412_dfa01c3c9e.jpg: 480x640 1 cat, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 276356412_dfa01c3c9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2355578735_286af5b202.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2355578735_286af5b202.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3060519665_4d6b9a51b2.jpg: 448x640 3 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3060519665_4d6b9a51b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530087422_7eb2b2c289.jpg: 512x640 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3530087422_7eb2b2c289.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2263655670_517890f5b7.jpg: 416x640 1 dog, 1 sheep, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2263655670_517890f5b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2854207034_1f00555703.jpg: 480x640 2 backpacks, 1 kite, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2854207034_1f00555703.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2254913901_569f568926.jpg: 512x640 3 persons, 2 handbags, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2254913901_569f568926.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1937262236_cbf5bfa101.jpg: 448x640 2 dogs, 1 frisbee, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1937262236_cbf5bfa101.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1836335410_de8313a64e.jpg: 640x640 3 persons, 7.1ms\n",
      "Speed: 2.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1836335410_de8313a64e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2534424894_ccd091fcb5.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2534424894_ccd091fcb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207676216_48478bce97.jpg: 416x640 1 person, 1 dog, 1 sheep, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3207676216_48478bce97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/513116697_ad0f4dc800.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 513116697_ad0f4dc800.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/310715139_7f05468042.jpg: 448x640 1 dog, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 310715139_7f05468042.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95728660_d47de66544.jpg: 480x640 1 person, 1 bicycle, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 95728660_d47de66544.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2868668723_0741222b23.jpg: 448x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2868668723_0741222b23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3387630781_f421a94d9d.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3387630781_f421a94d9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2598012140_832863fcb9.jpg: 480x640 2 cats, 1 dog, 1 couch, 1 bed, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2598012140_832863fcb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3637013_c675de7705.jpg: 640x544 2 persons, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3637013_c675de7705.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2925577165_b83d31a7f6.jpg: 480x640 2 cars, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2925577165_b83d31a7f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3362049454_ea0c22e57b.jpg: 512x640 1 dog, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3362049454_ea0c22e57b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1541272333_1624b22546.jpg: 448x640 1 horse, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1541272333_1624b22546.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506808265_fe84ada926.jpg: 480x640 3 persons, 1 bottle, 1 cup, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 506808265_fe84ada926.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533394378_1513ec90db.jpg: 544x640 2 persons, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3533394378_1513ec90db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2708744743_e231f7fcf9.jpg: 480x640 24 persons, 2 frisbees, 1 sports ball, 13 chairs, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2708744743_e231f7fcf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1332823164_c70a5d930e.jpg: 448x640 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1332823164_c70a5d930e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3257103624_e76f25ff9e.jpg: 480x640 2 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3257103624_e76f25ff9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2810333931_47fd8dd340.jpg: 480x640 13 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2810333931_47fd8dd340.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/394136487_4fc531b33a.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 394136487_4fc531b33a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286406057_a1668655af.jpg: 448x640 4 persons, 10.5ms\n",
      "Speed: 3.1ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286406057_a1668655af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590294974_4ef98f013e.jpg: 352x640 1 person, 1 skateboard, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3590294974_4ef98f013e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2586028627_ddd054d8cc.jpg: 448x640 13 persons, 1 skateboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2586028627_ddd054d8cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487131146_9d3aca387a.jpg: 448x640 6 persons, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3487131146_9d3aca387a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3015863181_92ff43f4d8.jpg: 448x640 5 persons, 8.0ms\n",
      "Speed: 2.1ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3015863181_92ff43f4d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273325447_81c94000da.jpg: 480x640 1 person, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3273325447_81c94000da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2198484810_50a893824a.jpg: 640x512 1 dog, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2198484810_50a893824a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/133905560_9d012b47f3.jpg: 480x640 2 dogs, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 133905560_9d012b47f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2743709828_a795a75bfc.jpg: 448x640 2 dogs, 1 cow, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2743709828_a795a75bfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3692746368_ab7d97ab31.jpg: 480x640 4 persons, 3 boats, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3692746368_ab7d97ab31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2419186511_f0ce5f9685.jpg: 640x448 2 persons, 2 toothbrushs, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2419186511_f0ce5f9685.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2895403073_906768cafa.jpg: 416x640 2 dogs, 1 cow, 12.3ms\n",
      "Speed: 2.6ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2895403073_906768cafa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/978580450_e862715aba.jpg: 640x448 2 persons, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 978580450_e862715aba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185695861_86152b2755.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3185695861_86152b2755.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2282260240_55387258de.jpg: 640x480 1 person, 1 sports ball, 1 couch, 1 potted plant, 2 vases, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2282260240_55387258de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2017276266_566656c59d.jpg: 640x448 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2017276266_566656c59d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991375936_bf4b0a7dc0.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2991375936_bf4b0a7dc0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/378453580_21d688748e.jpg: 512x640 1 dog, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 378453580_21d688748e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126752627_dc2d6674da.jpg: 640x512 13 persons, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3126752627_dc2d6674da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/837919879_94e3dacd83.jpg: 640x416 2 persons, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 837919879_94e3dacd83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119903318_d032141839.jpg: 384x640 3 persons, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3119903318_d032141839.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/351876121_c7c0221928.jpg: 480x640 1 cat, 1 dog, 1 bed, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 351876121_c7c0221928.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/413737417_b0a8b445e9.jpg: 448x640 4 persons, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 413737417_b0a8b445e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2632111399_b3c1630f8e.jpg: 640x640 3 persons, 1 baseball glove, 8.9ms\n",
      "Speed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2632111399_b3c1630f8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2363540508_9dd1ccf7c7.jpg: 448x640 1 person, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2363540508_9dd1ccf7c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3108197858_441ff38565.jpg: 480x640 5 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3108197858_441ff38565.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/207275121_ee4dfa0bf2.jpg: 480x640 1 dog, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 207275121_ee4dfa0bf2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3398745929_8cd3bbb8a8.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3398745929_8cd3bbb8a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3254645823_a7c072481c.jpg: 448x640 6 persons, 2 snowboards, 2 kites, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3254645823_a7c072481c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211556865_d1d9becf69.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3211556865_d1d9becf69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589267801_5a222e3a60.jpg: 448x640 1 dog, 1 horse, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3589267801_5a222e3a60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2473737724_355599a263.jpg: 480x640 6 persons, 1 bench, 1 dog, 2 baseball bats, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2473737724_355599a263.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2348491126_30db0d46ef.jpg: 448x640 3 persons, 1 chair, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2348491126_30db0d46ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320756943_9d004f9824.jpg: 640x448 8 persons, 2 backpacks, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3320756943_9d004f9824.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2151056407_c9c09b0a02.jpg: 480x640 1 dog, 2 sheeps, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2151056407_c9c09b0a02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249278583_95cd8206da.jpg: 448x640 2 persons, 1 dog, 6 cows, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3249278583_95cd8206da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329289652_e09b80e2f3.jpg: 448x640 2 persons, 1 couch, 1 cell phone, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3329289652_e09b80e2f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425887426_bf60b8afa3.jpg: 640x512 3 persons, 1 sports ball, 1 tennis racket, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425887426_bf60b8afa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565618804_8d7ed87389.jpg: 480x640 11 persons, 1 umbrella, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2565618804_8d7ed87389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3307147971_5b3abf61f9.jpg: 448x640 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3307147971_5b3abf61f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468608014_09fd20eb9b.jpg: 448x640 1 dog, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 468608014_09fd20eb9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247598959_5b2348444c.jpg: 448x640 1 dog, 3 cows, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3247598959_5b2348444c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521983429_33218366bd.jpg: 640x576 1 person, 1 dog, 1 sheep, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2521983429_33218366bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1377668044_36398401dd.jpg: 640x448 1 person, 1 dog, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1377668044_36398401dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373544964_c9f1253b7d.jpg: 640x448 1 person, 3 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3373544964_c9f1253b7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1394396709_65040d97ab.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1394396709_65040d97ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/303795791_98ebc1d19a.jpg: 480x640 2 dogs, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 303795791_98ebc1d19a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173976185_8a50123050.jpg: 448x640 1 dog, 11.9ms\n",
      "Speed: 2.9ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3173976185_8a50123050.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1516714577_7d1c35a8d8.jpg: 640x480 1 person, 1 potted plant, 10.2ms\n",
      "Speed: 2.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1516714577_7d1c35a8d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602306033_2b3100d36b.jpg: 640x640 1 person, 1 dog, 9.4ms\n",
      "Speed: 2.9ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2602306033_2b3100d36b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3228793611_8f260ea500.jpg: 640x448 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3228793611_8f260ea500.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2701271123_6761bc5f26.jpg: 448x640 1 dog, 1 frisbee, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2701271123_6761bc5f26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2284894733_b710b9b106.jpg: 640x480 3 persons, 1 horse, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2284894733_b710b9b106.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2868575889_2c030aa8ae.jpg: 640x416 1 person, 2 skateboards, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2868575889_2c030aa8ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/276699720_fe6718fd03.jpg: 512x640 1 person, 1 motorcycle, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 276699720_fe6718fd03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217910740_d1d61c08ab.jpg: 640x448 4 persons, 2 chairs, 2 laptops, 1 keyboard, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3217910740_d1d61c08ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1472653060_7427d2865a.jpg: 480x640 6 persons, 2 cups, 1 knife, 9 bowls, 1 dining table, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1472653060_7427d2865a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649382413_58a4b1efe8.jpg: 640x320 4 persons, 73.9ms\n",
      "Speed: 1.5ms preprocess, 73.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 3649382413_58a4b1efe8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1396064003_3fd949c9dd.jpg: 640x512 1 person, 9.3ms\n",
      "Speed: 3.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1396064003_3fd949c9dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2518508760_68d8df7365.jpg: 512x640 1 dog, 1 horse, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2518508760_68d8df7365.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3546027589_253553252a.jpg: 448x640 6 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3546027589_253553252a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2612488996_9450de0e54.jpg: 512x640 7 persons, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2612488996_9450de0e54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856699493_65edef80a1.jpg: 640x448 2 persons, 1 backpack, 1 handbag, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2856699493_65edef80a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/441921713_1cafc7d7d2.jpg: 480x640 7 persons, 3 cars, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 441921713_1cafc7d7d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444047125_66b249287c.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 444047125_66b249287c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3407584080_c6abf71ae3.jpg: 640x448 1 person, 1 snowboard, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3407584080_c6abf71ae3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3538021517_b930dc76fc.jpg: 640x448 5 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3538021517_b930dc76fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3363750526_efcedc47a9.jpg: 544x640 1 dog, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3363750526_efcedc47a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046431231_dc48851062.jpg: 544x640 5 persons, 6.4ms\n",
      "Speed: 1.8ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3046431231_dc48851062.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2436081047_bca044c1d3.jpg: 480x640 1 person, 3 cars, 1 bench, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2436081047_bca044c1d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271385712_ffd34f2de5.jpg: 448x640 3 persons, 12.9ms\n",
      "Speed: 3.4ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3271385712_ffd34f2de5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/342872408_04a2832a1b.jpg: 448x640 2 persons, 9.9ms\n",
      "Speed: 2.5ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 342872408_04a2832a1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/875731481_a5a0a09934.jpg: 640x512 4 persons, 9.7ms\n",
      "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 875731481_a5a0a09934.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3314517351_69d70e62bd.jpg: 448x640 1 person, 2 surfboards, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3314517351_69d70e62bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286193613_fc046e8016.jpg: 640x480 1 giraffe, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3286193613_fc046e8016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436259762_43709321ff.jpg: 608x640 1 cat, 1 dog, 9.1ms\n",
      "Speed: 3.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3436259762_43709321ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262647146_a53770a21d.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3262647146_a53770a21d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286198467_8880be127e.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286198467_8880be127e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2947172114_b591f84163.jpg: 640x640 12 persons, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2947172114_b591f84163.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3615730936_23457575e9.jpg: 640x640 1 person, 1 bicycle, 8.7ms\n",
      "Speed: 2.8ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3615730936_23457575e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/252124738_796599e94b.jpg: 512x640 19 persons, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 252124738_796599e94b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2068465241_3bcabacfd7.jpg: 640x416 2 persons, 1 skateboard, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2068465241_3bcabacfd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/473220329_819a913bbb.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 473220329_819a913bbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2729685399_56c0e104b1.jpg: 640x480 8 persons, 2 cars, 1 backpack, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2729685399_56c0e104b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/369186134_5eef374112.jpg: 480x640 3 dogs, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 369186134_5eef374112.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2647593678_1fa3bb516c.jpg: 608x640 3 persons, 1 dog, 1 umbrella, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2647593678_1fa3bb516c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866686547_0a67eb899d.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2866686547_0a67eb899d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/326456451_effadbbe49.jpg: 544x640 1 dog, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 326456451_effadbbe49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159569570_dff24e7be9.jpg: 448x640 16 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3159569570_dff24e7be9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3006094603_c5b32d2758.jpg: 640x416 1 person, 1 cup, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3006094603_c5b32d2758.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3550763985_800cfee7e4.jpg: 640x448 1 person, 2 cars, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3550763985_800cfee7e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1562478713_505ab6d924.jpg: 480x640 1 person, 1 umbrella, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1562478713_505ab6d924.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/387974450_bcd205daac.jpg: 448x640 1 person, 4 birds, 1 dog, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 387974450_bcd205daac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1798215547_ef7ad95be8.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1798215547_ef7ad95be8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2848571082_26454cb981.jpg: 640x640 1 dog, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2848571082_26454cb981.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025315215_a5d367971a.jpg: 448x640 1 dog, 1 bed, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3025315215_a5d367971a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3355827928_c96c0c3e88.jpg: 640x448 1 person, 1 bicycle, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3355827928_c96c0c3e88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2374179071_af22170d62.jpg: 384x640 1 person, 8.1ms\n",
      "Speed: 1.4ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2374179071_af22170d62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891185857_54942809cf.jpg: 512x640 17 persons, 1 umbrella, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2891185857_54942809cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/335588286_f67ed8c9f9.jpg: 640x608 1 dog, 1 bear, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 335588286_f67ed8c9f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2736902411_a0010f89ae.jpg: 608x640 3 persons, 1 cell phone, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2736902411_a0010f89ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384626662_67cdd87694.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2384626662_67cdd87694.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3600909823_ce72c26e66.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3600909823_ce72c26e66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/361092202_3d70144ebd.jpg: 448x640 1 person, 1 backpack, 1 baseball bat, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 361092202_3d70144ebd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2537119659_fa01dd5de5.jpg: 480x640 2 persons, 1 giraffe, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2537119659_fa01dd5de5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2631300484_be8621d17b.jpg: 640x448 5 persons, 1 bench, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2631300484_be8621d17b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2109479807_eec8d72ca7.jpg: 480x640 7 persons, 1 motorcycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2109479807_eec8d72ca7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3504881781_6a842e043b.jpg: 384x640 4 persons, 1 suitcase, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3504881781_6a842e043b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2656749876_e32495bd8c.jpg: 640x416 1 person, 1 suitcase, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2656749876_e32495bd8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2496370758_a3fbc49837.jpg: 352x640 2 dogs, 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2496370758_a3fbc49837.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247618600_239eeac405.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 247618600_239eeac405.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225058391_a12d38d911.jpg: 448x640 10 persons, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3225058391_a12d38d911.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3611672054_45edd3e08f.jpg: 640x448 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3611672054_45edd3e08f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2224995194_518859d97d.jpg: 544x640 2 persons, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2224995194_518859d97d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3239866450_3f8cfb0c83.jpg: 640x448 1 teddy bear, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3239866450_3f8cfb0c83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3118534315_cc03e5ddab.jpg: 480x640 3 persons, 1 handbag, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3118534315_cc03e5ddab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3113322995_13781860f2.jpg: 640x480 1 person, 8 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3113322995_13781860f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3508637029_89f3bdd3a2.jpg: 640x608 2 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3508637029_89f3bdd3a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3730011219_588cdc7972.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3730011219_588cdc7972.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3090398639_68c0dfa9a5.jpg: 640x448 1 person, 1 bicycle, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3090398639_68c0dfa9a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3283368342_b96d45210e.jpg: 384x640 4 persons, 3 dogs, 1 sheep, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3283368342_b96d45210e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2935703360_4f794f7f09.jpg: 640x640 1 dog, 12.1ms\n",
      "Speed: 4.2ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2935703360_4f794f7f09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259991972_fce3ab18b2.jpg: 448x640 15 persons, 1 clock, 10.8ms\n",
      "Speed: 2.4ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259991972_fce3ab18b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95728664_06c43b90f1.jpg: 480x640 2 persons, 13.8ms\n",
      "Speed: 3.5ms preprocess, 13.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 95728664_06c43b90f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1860543210_47e94cf652.jpg: 640x480 1 person, 12.5ms\n",
      "Speed: 3.0ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1860543210_47e94cf652.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/511282305_dbab4bf4be.jpg: 448x640 4 persons, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 511282305_dbab4bf4be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3618932839_acd7d2c2ea.jpg: 448x640 1 person, 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3618932839_acd7d2c2ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3151860914_46e30cd5ea.jpg: 480x640 4 persons, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3151860914_46e30cd5ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498240367_cbd8c6efbf.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3498240367_cbd8c6efbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3551787566_b5ebbe2440.jpg: 448x640 1 person, 1 bicycle, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3551787566_b5ebbe2440.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192333873_2a0cbe849d.jpg: 544x640 3 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2192333873_2a0cbe849d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/455856615_f6361d9253.jpg: 448x640 3 persons, 1 skis, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 455856615_f6361d9253.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2730819220_b58af1119a.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2730819220_b58af1119a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514278386_de2343577e.jpg: 448x640 12 persons, 2 baseball gloves, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514278386_de2343577e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/259314892_a42b8af664.jpg: 448x640 2 persons, 1 dog, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 259314892_a42b8af664.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2703155733_19ac6f97a8.jpg: 416x640 3 persons, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2703155733_19ac6f97a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3291587911_81fc33300e.jpg: 640x448 2 persons, 1 dog, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3291587911_81fc33300e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487378989_c051d2715b.jpg: 640x480 2 persons, 1 bicycle, 11.0ms\n",
      "Speed: 2.6ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3487378989_c051d2715b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561334141_0aacefa5e7.jpg: 640x448 1 person, 1 dog, 1 sports ball, 10.1ms\n",
      "Speed: 2.1ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2561334141_0aacefa5e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/469259974_bb03c15c42.jpg: 448x640 1 person, 1 cup, 1 book, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 469259974_bb03c15c42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2962977152_9d6958fdd5.jpg: 576x640 9 persons, 1 sports ball, 9.4ms\n",
      "Speed: 2.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2962977152_9d6958fdd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475778645_65b7343c47.jpg: 640x480 2 persons, 13.6ms\n",
      "Speed: 3.5ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 475778645_65b7343c47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2582390123_71120edb0c.jpg: 512x640 2 sheeps, 1 cow, 11.9ms\n",
      "Speed: 3.1ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2582390123_71120edb0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514990193_2d2422af2c.jpg: 480x640 2 persons, 1 couch, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 514990193_2d2422af2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2913818905_8e4d9aa82a.jpg: 384x640 3 dogs, 10.4ms\n",
      "Speed: 1.8ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2913818905_8e4d9aa82a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2999162229_80d17099b6.jpg: 448x640 3 persons, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2999162229_80d17099b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353328134_dd9ed0edab.jpg: 640x480 1 person, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3353328134_dd9ed0edab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458211052_bb73084398.jpg: 480x640 14 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3458211052_bb73084398.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/542648687_adf13c406b.jpg: 448x640 2 dogs, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 542648687_adf13c406b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/477204750_d04d111cd4.jpg: 448x640 1 dog, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 477204750_d04d111cd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836864045_9a093cfd65.jpg: 448x640 2 cars, 1 dog, 1 frisbee, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2836864045_9a093cfd65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/783353797_fdf91bdf4c.jpg: 448x640 1 person, 2 cars, 1 truck, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 783353797_fdf91bdf4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3100442775_6e2659b973.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3100442775_6e2659b973.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506343925_b30a235de6.jpg: 640x640 1 dog, 8.9ms\n",
      "Speed: 2.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 506343925_b30a235de6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2500567791_101d5ddee3.jpg: 512x640 1 sheep, 2 cows, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2500567791_101d5ddee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539705321_99406e5820.jpg: 448x640 5 persons, 1 sports ball, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 539705321_99406e5820.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516521516_9950340b96.jpg: 640x640 1 person, 1 sports ball, 9.0ms\n",
      "Speed: 2.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3516521516_9950340b96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2868776402_aef437e493.jpg: 480x640 9 persons, 5 umbrellas, 2 chairs, 1 potted plant, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2868776402_aef437e493.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635164923_2a774f7854.jpg: 448x640 1 person, 1 skateboard, 9.5ms\n",
      "Speed: 2.0ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2635164923_2a774f7854.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3545427060_c16a8b7dfd.jpg: 448x640 3 persons, 10.9ms\n",
      "Speed: 2.9ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3545427060_c16a8b7dfd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174453534_fcc927c647.jpg: 640x640 2 persons, 1 bottle, 1 donut, 10.5ms\n",
      "Speed: 3.3ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3174453534_fcc927c647.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014080715_f4f0dbb56e.jpg: 640x544 2 persons, 1 clock, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3014080715_f4f0dbb56e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724150944_fc62e8d5e0.jpg: 640x608 4 persons, 1 sports ball, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3724150944_fc62e8d5e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500829879_a643818d84.jpg: 448x640 1 motorcycle, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3500829879_a643818d84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218907190_5f43bf5e4d.jpg: 448x640 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2218907190_5f43bf5e4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752926645_801a198ff6.jpg: 448x640 10 persons, 1 handbag, 1 tie, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2752926645_801a198ff6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472066410_065b4f99d3.jpg: 640x448 2 persons, 2 cars, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3472066410_065b4f99d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2392625002_83a5a0978f.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2392625002_83a5a0978f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2646615552_3aeeb2473b.jpg: 640x640 2 persons, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2646615552_3aeeb2473b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/344078103_4b23931ce5.jpg: 448x640 2 dogs, 1 bear, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 344078103_4b23931ce5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/491600485_26c52c8816.jpg: 640x640 1 horse, 1 sports ball, 9.2ms\n",
      "Speed: 2.8ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 491600485_26c52c8816.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578477508_b7d839da16.jpg: 448x640 1 person, 2 skateboards, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3578477508_b7d839da16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2474092890_6c0781a8ed.jpg: 640x480 1 person, 1 bench, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2474092890_6c0781a8ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397451339_76a84bd310.jpg: 512x640 2 persons, 2 benchs, 1 frisbee, 2 chairs, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 397451339_76a84bd310.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3268443910_b36dbc1e5c.jpg: 640x512 1 person, 1 sports ball, 1 tennis racket, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3268443910_b36dbc1e5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1859726819_9a793b3b44.jpg: 640x512 3 persons, 1 tie, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1859726819_9a793b3b44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218334049_e649dbdb1a.jpg: 640x448 1 dog, 1 sports ball, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2218334049_e649dbdb1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143982558_9e2d44c155.jpg: 480x640 2 persons, 1 dog, 2 couchs, 1 remote, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143982558_9e2d44c155.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1859941832_7faf6e5fa9.jpg: 416x640 1 dog, 1 frisbee, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1859941832_7faf6e5fa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259222980_04fb62df97.jpg: 448x640 2 persons, 2 cars, 1 bottle, 1 laptop, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259222980_04fb62df97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/253762507_9c3356c2f6.jpg: 480x640 3 persons, 1 train, 1 refrigerator, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 253762507_9c3356c2f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3075068274_2a28a5263b.jpg: 640x640 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3075068274_2a28a5263b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3062173277_bfb5ef4c45.jpg: 448x640 7 persons, 1 surfboard, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3062173277_bfb5ef4c45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/449287870_f17fb825d7.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 449287870_f17fb825d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3148571800_c5515e6c3d.jpg: 640x640 4 persons, 2 horses, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3148571800_c5515e6c3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030223792_02b6f2be99.jpg: 448x640 2 persons, 2 beds, 1 remote, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3030223792_02b6f2be99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/519754987_51861fea85.jpg: 480x640 9 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 519754987_51861fea85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2467853482_17009933e8.jpg: 448x640 1 person, 1 cell phone, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2467853482_17009933e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245070961_8977fdd548.jpg: 512x640 1 person, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3245070961_8977fdd548.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427233064_6af01bfc5c.jpg: 448x640 1 person, 1 horse, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3427233064_6af01bfc5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2711075591_f3ee53cfaa.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2711075591_f3ee53cfaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3626689571_5817f99c0e.jpg: 448x640 1 person, 1 car, 1 frisbee, 1 sports ball, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3626689571_5817f99c0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/309049466_1d7e7d5fc2.jpg: 640x576 1 dog, 1 frisbee, 8.0ms\n",
      "Speed: 2.6ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 309049466_1d7e7d5fc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1732436777_950bcdc9b8.jpg: 448x640 4 persons, 1 handbag, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1732436777_950bcdc9b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182570190_48214e54c7.jpg: 640x448 7 persons, 2 bicycles, 1 backpack, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3182570190_48214e54c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1311132744_5ffd03f831.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1311132744_5ffd03f831.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2295216243_0712928988.jpg: 480x640 9 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2295216243_0712928988.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2220612655_030413b787.jpg: 448x640 1 dog, 1 frisbee, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2220612655_030413b787.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3112635165_2d614d7c1a.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3112635165_2d614d7c1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488549693_a1f51d8c4a.jpg: 448x640 2 persons, 1 frisbee, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 488549693_a1f51d8c4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222496967_45d468ee66.jpg: 640x448 4 persons, 3 handbags, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3222496967_45d468ee66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3142055158_01b9e4caa4.jpg: 608x640 9 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3142055158_01b9e4caa4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2301379282_5fbcf230d1.jpg: 640x480 2 persons, 1 remote, 2 books, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2301379282_5fbcf230d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/254475194_3d8f4dfd53.jpg: 480x640 1 person, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 254475194_3d8f4dfd53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2904129133_e6ae5a1ec6.jpg: 384x640 2 persons, 1 bench, 8.1ms\n",
      "Speed: 1.3ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2904129133_e6ae5a1ec6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1361420539_e9599c60ae.jpg: 608x640 1 bird, 1 bear, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1361420539_e9599c60ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085255128_61224cc47f.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2085255128_61224cc47f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671950830_b570bac1b9.jpg: 416x640 20 persons, 1 motorcycle, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3671950830_b570bac1b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2216568822_84c295c3b0.jpg: 640x544 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2216568822_84c295c3b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/138718600_f430ebca17.jpg: 480x640 1 person, 2 bicycles, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 138718600_f430ebca17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207264553_8cd4dcde53.jpg: 480x640 3 persons, 2 surfboards, 9.5ms\n",
      "Speed: 2.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3207264553_8cd4dcde53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2750832671_4b39f06acf.jpg: 448x640 2 dogs, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2750832671_4b39f06acf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3452341579_0147d2199b.jpg: 448x640 5 persons, 1 bicycle, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3452341579_0147d2199b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2086532897_b8714f2237.jpg: 448x640 2 persons, 1 surfboard, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2086532897_b8714f2237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1835511273_790eaae6e6.jpg: 640x384 8 persons, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 1835511273_790eaae6e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351360323_91bb341350.jpg: 480x640 2 persons, 13.8ms\n",
      "Speed: 3.7ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3351360323_91bb341350.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1827560917_c8d3c5627f.jpg: 512x640 1 dog, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1827560917_c8d3c5627f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/756004341_1a816df714.jpg: 480x640 2 persons, 10.1ms\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 756004341_1a816df714.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3347701468_bb0001b035.jpg: 448x640 6 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3347701468_bb0001b035.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/540721368_12ac732c6c.jpg: 640x448 2 persons, 1 umbrella, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 540721368_12ac732c6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3221815947_76c95b50b7.jpg: 448x640 1 horse, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3221815947_76c95b50b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632258003_6a0a69bf3a.jpg: 448x640 4 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3632258003_6a0a69bf3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1115679311_245eff2f4b.jpg: 480x640 (no detections), 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1115679311_245eff2f4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401039304_424ffc7dbf.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3401039304_424ffc7dbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347664_4a3e7e5be7.jpg: 640x448 4 persons, 2 sports balls, 1 chair, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347664_4a3e7e5be7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2844747252_64567cf14a.jpg: 256x640 1 dog, 3 cows, 10.2ms\n",
      "Speed: 1.3ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2844747252_64567cf14a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2970461648_fe14ba0359.jpg: 640x480 4 persons, 1 bench, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2970461648_fe14ba0359.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396036947_0af6c3aab7.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3396036947_0af6c3aab7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3767841911_6678052eb6.jpg: 480x640 1 person, 1 bench, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3767841911_6678052eb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1620397000_3883e3ecd3.jpg: 320x640 1 dog, 1 sheep, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 1620397000_3883e3ecd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535249787_0fcaa613a0.jpg: 640x480 5 persons, 1 bench, 1 sports ball, 1 baseball bat, 1 tennis racket, 2 chairs, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 535249787_0fcaa613a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/587604325_af5d6df679.jpg: 640x576 1 bear, 1 teddy bear, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 587604325_af5d6df679.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1160441615_fe6b3c5277.jpg: 480x640 3 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1160441615_fe6b3c5277.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502935765_a0ae1fa7be.jpg: 448x640 1 person, 1 surfboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2502935765_a0ae1fa7be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1423126855_6cd2a3956c.jpg: 480x640 1 person, 3 sheeps, 1 backpack, 1 sports ball, 1 baseball glove, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1423126855_6cd2a3956c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417102649_5c0b2f4b4d.jpg: 640x480 1 giraffe, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3417102649_5c0b2f4b4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1956678973_223cb1b847.jpg: 640x480 4 persons, 1 car, 1 truck, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1956678973_223cb1b847.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498404951_527adba7b8.jpg: 448x640 1 dog, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 498404951_527adba7b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2470588201_955132a946.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2470588201_955132a946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/321229104_3cbaf0f51c.jpg: 480x640 1 bird, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 321229104_3cbaf0f51c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2611651553_61f859837e.jpg: 640x640 3 persons, 7.4ms\n",
      "Speed: 2.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2611651553_61f859837e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/440737340_5af34ca9cf.jpg: 480x640 1 person, 1 dog, 1 suitcase, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 440737340_5af34ca9cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2338627102_6708a9b4fd.jpg: 448x640 1 person, 3 backpacks, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2338627102_6708a9b4fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2633201394_ee4a7666ed.jpg: 448x640 1 bear, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2633201394_ee4a7666ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2972929655_04233b5489.jpg: 576x640 2 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2972929655_04233b5489.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2994179598_a45c2732b5.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2994179598_a45c2732b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278002875_d011ae9dc5.jpg: 480x640 6 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278002875_d011ae9dc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3618504267_d7eaa495d0.jpg: 448x640 14 persons, 4 dogs, 1 handbag, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3618504267_d7eaa495d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2932740428_b15384f389.jpg: 448x640 4 persons, 1 motorcycle, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2932740428_b15384f389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/766061382_6c7ff514c4.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 766061382_6c7ff514c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3078844565_16e9cdcea2.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3078844565_16e9cdcea2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174713468_e22fa7779e.jpg: 640x640 1 person, 1 dog, 7.5ms\n",
      "Speed: 2.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3174713468_e22fa7779e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3270047169_2ed289a9af.jpg: 448x640 1 person, 2 benchs, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3270047169_2ed289a9af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187395715_f2940c2b72.jpg: 416x640 1 person, 1 skateboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3187395715_f2940c2b72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/10815824_2997e03d76.jpg: 448x640 2 persons, 3 horses, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 10815824_2997e03d76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3092756650_557c5f2d03.jpg: 448x640 10 persons, 1 car, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3092756650_557c5f2d03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3538686658_30afc75f02.jpg: 448x640 5 persons, 1 motorcycle, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3538686658_30afc75f02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109268897_d43797fc6a.jpg: 448x640 1 person, 1 tie, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3109268897_d43797fc6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3699318394_6193f2c8e0.jpg: 448x640 2 persons, 1 bottle, 1 cup, 1 chair, 1 tv, 1 laptop, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3699318394_6193f2c8e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2876848241_63290edfb4.jpg: 384x640 1 person, 1 motorcycle, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2876848241_63290edfb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1304100320_c8990a1539.jpg: 480x640 1 horse, 2 sheeps, 1 giraffe, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1304100320_c8990a1539.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1174525839_7c1e6cfa86.jpg: 448x640 1 person, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1174525839_7c1e6cfa86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/299178969_5ca1de8e40.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 299178969_5ca1de8e40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/872622575_ba1d3632cc.jpg: 640x480 2 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 872622575_ba1d3632cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639845565_be547c38ba.jpg: 480x640 9 persons, 7 cars, 1 truck, 2 benchs, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3639845565_be547c38ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315110972_1090d11728.jpg: 480x640 1 bird, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3315110972_1090d11728.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143978284_ac086be9a3.jpg: 480x640 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143978284_ac086be9a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2176874361_2b4149010b.jpg: 448x640 1 dog, 1 cow, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2176874361_2b4149010b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242088278_43eea5d17e.jpg: 416x640 3 birds, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3242088278_43eea5d17e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3132760860_3e743a935d.jpg: 448x640 1 person, 1 snowboard, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3132760860_3e743a935d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710176138_fbfe00bd35.jpg: 448x640 2 persons, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3710176138_fbfe00bd35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3009644534_992e9ea2a7.jpg: 512x640 1 dog, 1 frisbee, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3009644534_992e9ea2a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415589320_71a5bf64cf.jpg: 480x640 1 dog, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3415589320_71a5bf64cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258396041_69717247f7.jpg: 448x640 5 persons, 1 bicycle, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3258396041_69717247f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1465666502_de289b3b9c.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1465666502_de289b3b9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3125628091_25a31709df.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3125628091_25a31709df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2314732154_83bc7f7314.jpg: 416x640 1 dog, 1 zebra, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2314732154_83bc7f7314.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/527288854_f26127b770.jpg: 448x640 2 persons, 1 sports ball, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 527288854_f26127b770.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3167379087_927ff05a35.jpg: 480x640 3 persons, 3 bicycles, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3167379087_927ff05a35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1552065993_b4dcd2eadf.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1552065993_b4dcd2eadf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/824923476_d85edce294.jpg: 480x640 4 birds, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 824923476_d85edce294.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710674892_857b8056f7.jpg: 448x640 1 person, 1 apple, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3710674892_857b8056f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415578043_03d33e6efd.jpg: 448x640 1 bowl, 1 clock, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415578043_03d33e6efd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2782433864_5a0c311d87.jpg: 448x640 1 dog, 2 cows, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2782433864_5a0c311d87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2340919359_f56787d307.jpg: 448x640 1 person, 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2340919359_f56787d307.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2199793371_343809ff70.jpg: 448x640 2 dogs, 1 frisbee, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2199793371_343809ff70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470887785_e0b1241d94.jpg: 480x640 9 persons, 2 ties, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 470887785_e0b1241d94.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085400856_ae09df33a7.jpg: 640x512 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2085400856_ae09df33a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2452238877_2340609c6e.jpg: 416x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2452238877_2340609c6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866696346_4dcccbd3a5.jpg: 448x640 2 persons, 2 skateboards, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2866696346_4dcccbd3a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/944374205_fd3e69bfca.jpg: 480x640 10 persons, 1 sports ball, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 944374205_fd3e69bfca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211199368_ca78387f72.jpg: 480x640 4 persons, 1 car, 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3211199368_ca78387f72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3387542157_81bfd00072.jpg: 480x640 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3387542157_81bfd00072.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/99171998_7cc800ceef.jpg: 480x640 5 persons, 1 backpack, 1 skis, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 99171998_7cc800ceef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3219065971_702c4e8c34.jpg: 640x480 2 persons, 1 potted plant, 1 dining table, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3219065971_702c4e8c34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2294516804_11e255807a.jpg: 480x640 1 person, 1 bed, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2294516804_11e255807a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3620492762_7f6a9b4746.jpg: 448x640 1 person, 1 motorcycle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3620492762_7f6a9b4746.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3162289423_4ca8915d0c.jpg: 448x640 1 person, 1 snowboard, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3162289423_4ca8915d0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3079787482_0757e9d167.jpg: 480x640 1 person, 1 bed, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3079787482_0757e9d167.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2441815792_43565b1312.jpg: 512x640 17 persons, 1 car, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2441815792_43565b1312.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/549520317_af3d5c32eb.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 549520317_af3d5c32eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2846843520_b0e6211478.jpg: 640x640 1 dog, 1 orange, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2846843520_b0e6211478.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606909929_90a1a072b7.jpg: 384x640 1 car, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3606909929_90a1a072b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3469711377_bc29d48737.jpg: 448x640 1 person, 1 bowl, 2 chairs, 1 couch, 1 microwave, 1 oven, 2 refrigerators, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3469711377_bc29d48737.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364026240_645d533fda.jpg: 448x640 1 person, 1 skis, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3364026240_645d533fda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/765091078_a8a11c6f9e.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 765091078_a8a11c6f9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2504991916_dc61e59e49.jpg: 640x608 2 persons, 1 truck, 1 potted plant, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2504991916_dc61e59e49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/518610439_b64ab21c02.jpg: 448x640 1 person, 2 frisbees, 1 baseball glove, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 518610439_b64ab21c02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3309042087_ee96d94b8a.jpg: 640x448 1 surfboard, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3309042087_ee96d94b8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2424398046_1a55c71376.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2424398046_1a55c71376.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2345984157_724823b1e4.jpg: 640x640 1 dog, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2345984157_724823b1e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3715469645_6d1dc019b3.jpg: 608x640 2 persons, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3715469645_6d1dc019b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443326696_fe0549c5be.jpg: 448x640 5 persons, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3443326696_fe0549c5be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/412056525_191724b058.jpg: 480x640 2 persons, 2 cups, 2 bowls, 1 dining table, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 412056525_191724b058.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3445296377_1e5082b44b.jpg: 480x640 2 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3445296377_1e5082b44b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3692892751_f6574e2700.jpg: 448x640 14 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3692892751_f6574e2700.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2705947033_5999147842.jpg: 448x640 4 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2705947033_5999147842.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432586199_e50b0d6cb7.jpg: 640x576 2 persons, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3432586199_e50b0d6cb7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512634877_d7ad8c8329.jpg: 448x640 1 dog, 1 frisbee, 1 sports ball, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512634877_d7ad8c8329.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562903245_85071bb5f9.jpg: 544x640 2 persons, 2 bicycles, 1 frisbee, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3562903245_85071bb5f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3345025842_bc2082a509.jpg: 640x448 2 persons, 1 bicycle, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3345025842_bc2082a509.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/453756106_711c20471a.jpg: 416x640 2 dogs, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 453756106_711c20471a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3447876218_4ccf42d7a0.jpg: 384x640 1 person, 1 snowboard, 8.0ms\n",
      "Speed: 1.5ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3447876218_4ccf42d7a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3718076407_0b4588d7bc.jpg: 640x448 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3718076407_0b4588d7bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2125626631_a4b63af97e.jpg: 512x640 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2125626631_a4b63af97e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3073535022_4af81f360c.jpg: 512x640 1 person, 1 surfboard, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3073535022_4af81f360c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344531479_03c69750e9.jpg: 640x512 1 person, 1 tennis racket, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3344531479_03c69750e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485312202_784508f2a9.jpg: 448x640 2 persons, 1 bicycle, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 485312202_784508f2a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2187222896_c206d63396.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2187222896_c206d63396.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3558251719_3af5ae2d02.jpg: 640x448 1 person, 8.3ms\n",
      "Speed: 1.5ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3558251719_3af5ae2d02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1952896009_cee8147c90.jpg: 640x480 1 person, 1 stop sign, 3 handbags, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1952896009_cee8147c90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3289893683_d4cc3ce208.jpg: 640x480 4 persons, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3289893683_d4cc3ce208.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1576185717_f841ddc3da.jpg: 480x640 2 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1576185717_f841ddc3da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2074764331_90a9962b52.jpg: 544x640 1 dog, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2074764331_90a9962b52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2747714500_49476225c6.jpg: 480x640 9 persons, 1 bench, 2 backpacks, 1 chair, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2747714500_49476225c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/362316425_bda238b4de.jpg: 640x448 7 persons, 1 bird, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 362316425_bda238b4de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2064780645_8f28a1529f.jpg: 448x640 4 persons, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2064780645_8f28a1529f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2638369467_8fc251595b.jpg: 640x448 1 person, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2638369467_8fc251595b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/310213587_778fe8fb5b.jpg: 448x640 8 persons, 2 cars, 2 trucks, 3 benchs, 1 bird, 3 handbags, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 310213587_778fe8fb5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/289599470_cc665e2dfb.jpg: 448x640 2 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 289599470_cc665e2dfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543291644_64539956e9.jpg: 640x448 4 persons, 1 bottle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 543291644_64539956e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565237642_bdd46d7cef.jpg: 512x640 1 dog, 1 kite, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2565237642_bdd46d7cef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489773343_a8aecf7db3.jpg: 480x640 4 persons, 1 dog, 1 cow, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 489773343_a8aecf7db3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3343900764_2a4c0405f9.jpg: 640x448 5 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3343900764_2a4c0405f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346508_0b3907a95b.jpg: 448x640 7 persons, 1 sports ball, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346508_0b3907a95b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2888386138_578d21033a.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2888386138_578d21033a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2081141788_38fa84ce3c.jpg: 480x640 4 persons, 1 surfboard, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2081141788_38fa84ce3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2837799692_2f1c50722a.jpg: 640x512 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2837799692_2f1c50722a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424307754_1e2f44d265.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 424307754_1e2f44d265.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/930126921_1b94605bdc.jpg: 640x448 2 persons, 1 sports ball, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 930126921_1b94605bdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3260088697_af9b6d2393.jpg: 480x640 2 persons, 1 bird, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3260088697_af9b6d2393.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259694057_fae7484b0a.jpg: 640x448 2 persons, 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3259694057_fae7484b0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3138399980_d6ab8b2272.jpg: 640x448 6 persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3138399980_d6ab8b2272.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2396691909_6b8c2f7c44.jpg: 448x640 3 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2396691909_6b8c2f7c44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3416339125_0860d3d1eb.jpg: 384x640 10 persons, 4 bottles, 1 dining table, 1 cell phone, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3416339125_0860d3d1eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1387443857_602ab6f9bf.jpg: 640x480 4 persons, 3 cars, 1 bench, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1387443857_602ab6f9bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3029463004_c2d2c8f404.jpg: 448x640 1 person, 1 surfboard, 17.1ms\n",
      "Speed: 3.7ms preprocess, 17.1ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3029463004_c2d2c8f404.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537758332_8beb9cf522.jpg: 480x640 2 persons, 13.2ms\n",
      "Speed: 2.8ms preprocess, 13.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 537758332_8beb9cf522.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335885203_a3fe8e541f.jpg: 448x640 12 persons, 1 frisbee, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3335885203_a3fe8e541f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584196366_a4b43d6644.jpg: 448x640 7 birds, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584196366_a4b43d6644.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/979383193_0a542a059d.jpg: 448x640 16 persons, 9.6ms\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 979383193_0a542a059d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2837808847_5407af1986.jpg: 480x640 2 dogs, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2837808847_5407af1986.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2537583012_4a358a6a8a.jpg: 640x608 3 persons, 10.3ms\n",
      "Speed: 3.4ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2537583012_4a358a6a8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522076584_7c603d2ac5.jpg: 640x448 2 bicycles, 10.2ms\n",
      "Speed: 2.0ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3522076584_7c603d2ac5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2826574228_c63009e473.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2826574228_c63009e473.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/716597900_b72c58362c.jpg: 448x640 4 persons, 1 boat, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 716597900_b72c58362c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539676201_c8f1f04952.jpg: 480x640 1 person, 1 backpack, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 539676201_c8f1f04952.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2196316998_3b2d63f01f.jpg: 448x640 5 persons, 1 cell phone, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2196316998_3b2d63f01f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566397227_a469e9e415.jpg: 640x480 1 person, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 566397227_a469e9e415.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364796213_b8948913b5.jpg: 416x640 14 persons, 2 bicycles, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3364796213_b8948913b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384353160_f395e9a54b.jpg: 640x448 1 person, 1 tie, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2384353160_f395e9a54b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544669026_1b5c0e6316.jpg: 640x448 1 giraffe, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3544669026_1b5c0e6316.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/203114209_e4cd71a6b7.jpg: 480x640 1 person, 1 surfboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 203114209_e4cd71a6b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229821595_77ace81c6b.jpg: 256x640 14 persons, 11.0ms\n",
      "Speed: 1.2ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 3229821595_77ace81c6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3480126681_52cea26bda.jpg: 544x640 1 dog, 12.1ms\n",
      "Speed: 2.5ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3480126681_52cea26bda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839532455_36a7dc4758.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2839532455_36a7dc4758.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459362347_c412ef9901.jpg: 448x640 3 persons, 1 laptop, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3459362347_c412ef9901.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3674565156_14d3b41450.jpg: 416x640 2 persons, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3674565156_14d3b41450.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2689163361_4939875be5.jpg: 544x640 2 persons, 1 surfboard, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2689163361_4939875be5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258394043_a0b6a94dce.jpg: 480x640 9 persons, 2 backpacks, 1 handbag, 1 cup, 1 chair, 2 dining tables, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3258394043_a0b6a94dce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2903469015_a1e7d969c2.jpg: 448x640 (no detections), 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2903469015_a1e7d969c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726629271_7639634703.jpg: 480x640 4 dogs, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3726629271_7639634703.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214885227_2be09e7cfb.jpg: 384x640 2 persons, 9.6ms\n",
      "Speed: 1.4ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3214885227_2be09e7cfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/572618443_647483ca82.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 572618443_647483ca82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744690159_fe2c89e55b.jpg: 512x640 1 dog, 1 bear, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2744690159_fe2c89e55b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518687038_964c523958.jpg: 608x640 1 person, 1 baseball glove, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3518687038_964c523958.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3074842262_62b1b2168c.jpg: 576x640 1 person, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3074842262_62b1b2168c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3614582606_16bd88dab2.jpg: 448x640 3 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3614582606_16bd88dab2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1346051107_9cdc14e070.jpg: 640x448 1 person, 1 fire hydrant, 1 skateboard, 8.6ms\n",
      "Speed: 1.6ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1346051107_9cdc14e070.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217620013_8b17873273.jpg: 640x512 9 persons, 1 frisbee, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3217620013_8b17873273.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468141298_3154d717e1.jpg: 480x640 4 persons, 7 cars, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 468141298_3154d717e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2157003092_eaeb977789.jpg: 640x480 (no detections), 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2157003092_eaeb977789.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1463638541_c02cfa04dc.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1463638541_c02cfa04dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/974924582_10bed89b8d.jpg: 640x448 2 persons, 1 car, 1 truck, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 974924582_10bed89b8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2196107384_361d73a170.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2196107384_361d73a170.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/336551609_1385ab139e.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 336551609_1385ab139e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/422763475_0bc814dac6.jpg: 640x320 1 person, 1 dog, 1 suitcase, 9.7ms\n",
      "Speed: 1.3ms preprocess, 9.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 422763475_0bc814dac6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/523985664_c866af4850.jpg: 640x448 4 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 523985664_c866af4850.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3181322965_ce9da15271.jpg: 448x640 6 persons, 1 truck, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3181322965_ce9da15271.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3269087421_1d489abeae.jpg: 480x640 8 persons, 1 cell phone, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3269087421_1d489abeae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1626754053_81126b67b6.jpg: 448x640 1 dog, 1 frisbee, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1626754053_81126b67b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490736665_38710f4b91.jpg: 416x640 1 dog, 9 sports balls, 1 potted plant, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3490736665_38710f4b91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/140430106_2978fda105.jpg: 480x640 1 person, 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 140430106_2978fda105.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3608400551_d6f7965308.jpg: 640x480 1 person, 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3608400551_d6f7965308.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2348924378_47e556d81a.jpg: 448x640 3 persons, 3 cars, 1 frisbee, 2 sports balls, 1 baseball bat, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2348924378_47e556d81a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3416460533_d5819fbf69.jpg: 448x640 1 person, 2 skiss, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3416460533_d5819fbf69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2895966469_53e0b29295.jpg: 640x448 2 persons, 1 snowboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2895966469_53e0b29295.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244910130_e1f823a28a.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244910130_e1f823a28a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349308309_92cff519f3.jpg: 640x640 3 persons, 2 horses, 1 handbag, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3349308309_92cff519f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/258476074_f28f4a1ae6.jpg: 480x640 3 persons, 5 cars, 1 truck, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 258476074_f28f4a1ae6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/272156850_c4445a53f4.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 272156850_c4445a53f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185645793_49de805194.jpg: 480x640 2 persons, 2 bicycles, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3185645793_49de805194.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647170476_0fd71a4c9f.jpg: 640x544 14 persons, 1 bicycle, 1 chair, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3647170476_0fd71a4c9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2369248869_0266760c4a.jpg: 640x480 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2369248869_0266760c4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520936130_9e90872560.jpg: 448x640 5 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3520936130_9e90872560.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/411175971_0fffd3b8c6.jpg: 480x640 1 person, 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 411175971_0fffd3b8c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/720208977_f44c2bba5b.jpg: 448x640 9 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 720208977_f44c2bba5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3103231330_db98b14501.jpg: 640x448 8 persons, 1 handbag, 1 tie, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3103231330_db98b14501.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/257588281_39e1c9d929.jpg: 480x640 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 257588281_39e1c9d929.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/141755292_7a0b3364cf.jpg: 544x640 3 persons, 1 pizza, 12.6ms\n",
      "Speed: 4.0ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 141755292_7a0b3364cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587077732_0933f1677b.jpg: 640x288 1 person, 77.9ms\n",
      "Speed: 1.8ms preprocess, 77.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Cropped images saved for 3587077732_0933f1677b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341077091_7ca0833373.jpg: 640x448 3 persons, 3 skateboards, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3341077091_7ca0833373.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601803640_5f3cb05acf.jpg: 640x480 13 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3601803640_5f3cb05acf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2509824208_247aca3ea3.jpg: 480x640 1 dog, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2509824208_247aca3ea3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3195147187_a073d59fa8.jpg: 448x640 2 dogs, 1 chair, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3195147187_a073d59fa8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399618896_9ef60cd32c.jpg: 512x640 2 persons, 1 surfboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3399618896_9ef60cd32c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991993027_36ac04e9a0.jpg: 480x640 14 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2991993027_36ac04e9a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425853460_bfcd0b41f6.jpg: 640x512 2 persons, 1 sports ball, 1 tennis racket, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425853460_bfcd0b41f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621649810_cca783b777.jpg: 448x640 1 person, 1 boat, 9.7ms\n",
      "Speed: 2.0ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3621649810_cca783b777.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2797438951_88a3ed7541.jpg: 416x640 1 person, 1 bench, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2797438951_88a3ed7541.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/693450725_8ad72389e6.jpg: 640x448 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 693450725_8ad72389e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/101669240_b2d3e7f17b.jpg: 480x640 3 persons, 1 skis, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 101669240_b2d3e7f17b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2180480870_dcaf5ac0df.jpg: 448x640 1 person, 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2180480870_dcaf5ac0df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2311690895_0d6efe11c8.jpg: 512x640 1 dog, 1 couch, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2311690895_0d6efe11c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/709373049_15b8b6457a.jpg: 544x640 1 person, 1 bench, 9.2ms\n",
      "Speed: 3.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 709373049_15b8b6457a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227594168_3351722aae.jpg: 448x640 4 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3227594168_3351722aae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2764732789_1392e962d0.jpg: 448x640 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2764732789_1392e962d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2709275718_73fcf08c23.jpg: 480x640 9 persons, 1 cup, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2709275718_73fcf08c23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/412203580_2c7278909c.jpg: 512x640 1 person, 1 surfboard, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 412203580_2c7278909c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443351431_7b4061df5c.jpg: 448x640 1 bird, 1 vase, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3443351431_7b4061df5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2403376030_903521c371.jpg: 480x640 1 person, 1 dog, 1 clock, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2403376030_903521c371.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530687486_6e6be53602.jpg: 448x640 1 person, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3530687486_6e6be53602.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839890871_4b7c7dbd96.jpg: 640x576 8 persons, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2839890871_4b7c7dbd96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2325258180_6217dd17eb.jpg: 512x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2325258180_6217dd17eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487979741_5f244c0c4b.jpg: 640x448 1 person, 1 motorcycle, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3487979741_5f244c0c4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2095435987_1b7591d214.jpg: 448x640 1 cow, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2095435987_1b7591d214.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246094557_e174a5914f.jpg: 640x480 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 246094557_e174a5914f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987096101_a41896187a.jpg: 640x448 1 person, 1 handbag, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2987096101_a41896187a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498492764_fe276e505a.jpg: 480x640 9 persons, 1 bicycle, 1 bench, 1 tie, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 498492764_fe276e505a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325129757_7a1979ac11.jpg: 448x640 2 persons, 1 snowboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3325129757_7a1979ac11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3307978046_92fef4dfa9.jpg: 640x448 5 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3307978046_92fef4dfa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1224851143_33bcdd299c.jpg: 480x640 2 persons, 3 chairs, 1 toothbrush, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1224851143_33bcdd299c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399312265_9c74378692.jpg: 448x640 1 cat, 2 dogs, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3399312265_9c74378692.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3065468339_4955e90fd3.jpg: 640x576 2 dogs, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3065468339_4955e90fd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/337647771_3b819feaba.jpg: 640x512 1 dog, 1 suitcase, 3 books, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 337647771_3b819feaba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227499174_07feb26337.jpg: 640x544 3 persons, 1 tie, 1 potted plant, 1 dining table, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3227499174_07feb26337.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1348304997_afe60a61df.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1348304997_afe60a61df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3057862887_135c61816a.jpg: 480x640 2 persons, 1 banana, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3057862887_135c61816a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/505062117_a70b4e10ab.jpg: 512x640 1 bear, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 505062117_a70b4e10ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2549452277_873cb80d3e.jpg: 256x640 1 person, 8.5ms\n",
      "Speed: 1.0ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2549452277_873cb80d3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2938875913_0ed920a6be.jpg: 640x480 4 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2938875913_0ed920a6be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/96420612_feb18fc6c6.jpg: 448x640 1 person, 1 surfboard, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 96420612_feb18fc6c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1998255400_0cd086908f.jpg: 640x448 2 persons, 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1998255400_0cd086908f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/420355149_f2076770df.jpg: 480x640 1 person, 1 spoon, 1 couch, 1 book, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 420355149_f2076770df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1799188614_b5189728ba.jpg: 640x448 1 person, 1 bird, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1799188614_b5189728ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3191805046_77c334b506.jpg: 640x448 1 person, 1 snowboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3191805046_77c334b506.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/233242340_09963100a3.jpg: 448x640 1 bench, 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 233242340_09963100a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/762947607_2001ee4c72.jpg: 640x448 1 person, 1 baseball bat, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 762947607_2001ee4c72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260828892_7925d27865.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 260828892_7925d27865.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2960759328_2d31e4af9b.jpg: 448x640 3 persons, 1 horse, 1 cow, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2960759328_2d31e4af9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2356664078_4b1e6e465d.jpg: 480x640 3 dogs, 1 frisbee, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2356664078_4b1e6e465d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2813588204_69fe7deb14.jpg: 448x640 2 dogs, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2813588204_69fe7deb14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301438465_10121a2412.jpg: 448x640 4 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3301438465_10121a2412.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415178926_909db9400b.jpg: 448x640 1 person, 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415178926_909db9400b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3298233193_d2a550840d.jpg: 544x640 1 dog, 1 sports ball, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3298233193_d2a550840d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/508261758_78fb8ae067.jpg: 480x640 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 508261758_78fb8ae067.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/484896012_7787d04f41.jpg: 544x640 1 person, 1 sports ball, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 484896012_7787d04f41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/322103537_184367bf88.jpg: 640x480 5 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 322103537_184367bf88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3380643902_7e0670f80f.jpg: 320x640 1 person, 1 skateboard, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3380643902_7e0670f80f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2874876837_80d178ba9b.jpg: 640x480 1 person, 2 baseball bats, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2874876837_80d178ba9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3035118753_69287079dc.jpg: 480x640 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3035118753_69287079dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/548751378_c657401312.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 548751378_c657401312.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2743652730_d909c7ae82.jpg: 384x640 2 persons, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2743652730_d909c7ae82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591458156_f1a9a33918.jpg: 448x640 7 persons, 1 horse, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3591458156_f1a9a33918.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497255828_f27e009aac.jpg: 640x448 2 persons, 1 baseball bat, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3497255828_f27e009aac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2741051940_89fb6b2cee.jpg: 480x640 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2741051940_89fb6b2cee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506367606_7cca2bba9b.jpg: 480x640 4 persons, 2 potted plants, 1 vase, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 506367606_7cca2bba9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2584487952_f70e5aa9bf.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2584487952_f70e5aa9bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502856739_490db7a657.jpg: 384x640 4 dogs, 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2502856739_490db7a657.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3471463779_64084b686c.jpg: 640x640 1 person, 1 sports ball, 1 baseball glove, 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3471463779_64084b686c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/479807833_85eed6899c.jpg: 640x544 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 479807833_85eed6899c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2291511815_ac083fddbd.jpg: 448x640 3 persons, 3 motorcycles, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2291511815_ac083fddbd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2920516901_23d8571419.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2920516901_23d8571419.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2374247382_023a86b9ca.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2374247382_023a86b9ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/637342973_89f6fac1f7.jpg: 480x640 3 persons, 4 dogs, 1 horse, 1 donut, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 637342973_89f6fac1f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2119302248_72493d458c.jpg: 544x640 1 person, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2119302248_72493d458c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1499495021_d295ce577c.jpg: 640x608 2 persons, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1499495021_d295ce577c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/417966898_a04f9b5349.jpg: 480x640 1 person, 2 dogs, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 417966898_a04f9b5349.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345427_ece0d186c2.jpg: 448x640 11 persons, 2 sports balls, 10.8ms\n",
      "Speed: 2.9ms preprocess, 10.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345427_ece0d186c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240048764_acce8af2a5.jpg: 480x640 2 persons, 1 boat, 2 chairs, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3240048764_acce8af2a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629402527_6dfc5c504b.jpg: 448x640 1 bench, 1 dog, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2629402527_6dfc5c504b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3064383768_f6838f57da.jpg: 448x640 1 person, 2 surfboards, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3064383768_f6838f57da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/667626_18933d713e.jpg: 480x640 1 person, 2 dogs, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 667626_18933d713e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325497914_f9014d615b.jpg: 448x640 3 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3325497914_f9014d615b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597146852_3d000a5d5f.jpg: 448x640 1 person, 1 skateboard, 1 potted plant, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3597146852_3d000a5d5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2219959872_988e6d498e.jpg: 416x640 3 persons, 1 skateboard, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2219959872_988e6d498e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3424851862_0f51c42922.jpg: 640x448 3 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3424851862_0f51c42922.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3184031654_34b5c4ffe1.jpg: 448x640 4 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3184031654_34b5c4ffe1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2759211664_d21393b668.jpg: 640x640 3 persons, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2759211664_d21393b668.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301021288_95935b7a74.jpg: 480x640 1 person, 1 car, 1 truck, 1 skateboard, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3301021288_95935b7a74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3413019648_e787f0cb88.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3413019648_e787f0cb88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3449718979_e987c64e2d.jpg: 640x480 1 dog, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3449718979_e987c64e2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532098999_4e07a0a17e.jpg: 448x640 3 dogs, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532098999_4e07a0a17e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561751298_320eef38ec.jpg: 640x480 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2561751298_320eef38ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3707738261_777075e885.jpg: 448x640 2 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3707738261_777075e885.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3618115051_41b5a7706c.jpg: 640x608 3 persons, 9.1ms\n",
      "Speed: 2.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3618115051_41b5a7706c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/505929313_7668f021ab.jpg: 640x448 1 dog, 1 frisbee, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 505929313_7668f021ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2222186636_85e03e0b75.jpg: 608x640 8 persons, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2222186636_85e03e0b75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602258549_7401a3cdae.jpg: 448x640 8 persons, 2 bottles, 1 wine glass, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602258549_7401a3cdae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2324779494_5e72d29171.jpg: 640x640 1 person, 1 scissors, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2324779494_5e72d29171.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1263126002_881ebd7ac9.jpg: 448x640 1 person, 1 couch, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1263126002_881ebd7ac9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3279025792_23bfd21bcc.jpg: 640x480 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3279025792_23bfd21bcc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/958326692_6210150354.jpg: 640x448 1 person, 1 umbrella, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 958326692_6210150354.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512026551_ba63ddbd31.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512026551_ba63ddbd31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2196846255_2c1635359a.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2196846255_2c1635359a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089542487_b4c1ee7025.jpg: 480x640 1 person, 1 baseball glove, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2089542487_b4c1ee7025.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/467858872_f3431df682.jpg: 480x640 1 sheep, 1 cow, 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 467858872_f3431df682.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590753142_61993d39df.jpg: 640x608 3 persons, 2 handbags, 1 tennis racket, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3590753142_61993d39df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2930318834_8366811283.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2930318834_8366811283.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/750196276_c3258c6f1b.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 750196276_c3258c6f1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278002947_3fd22a2cb6.jpg: 480x640 5 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278002947_3fd22a2cb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3398276602_c7d106c34f.jpg: 512x640 2 persons, 1 bicycle, 1 dog, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3398276602_c7d106c34f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2541104331_a2d65cfa54.jpg: 448x640 1 dog, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2541104331_a2d65cfa54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259883609_6a1b46919e.jpg: 416x640 2 birds, 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3259883609_6a1b46919e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/210126070_0d43b300b9.jpg: 480x640 11 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 210126070_0d43b300b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/299612419_b55fe32fea.jpg: 480x640 6 persons, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 299612419_b55fe32fea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1234817607_924893f6e1.jpg: 448x640 1 person, 7 bottles, 3 cups, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1234817607_924893f6e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/439049388_3dcee2d30b.jpg: 416x640 2 persons, 3 boats, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 439049388_3dcee2d30b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259992638_0612a40288.jpg: 448x640 15 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259992638_0612a40288.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273105617_7c73d2d2d3.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2273105617_7c73d2d2d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/337793983_ac5b2e848e.jpg: 544x640 7 persons, 1 cup, 4 chairs, 2 laptops, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 337793983_ac5b2e848e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090997177_76d482b158.jpg: 416x640 1 horse, 1 cow, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2090997177_76d482b158.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523950181_414978964e.jpg: 480x640 4 persons, 1 baseball bat, 2 baseball gloves, 1 chair, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3523950181_414978964e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3343311201_eeb1a39def.jpg: 640x480 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3343311201_eeb1a39def.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030962048_f71948226c.jpg: 448x640 1 person, 1 cup, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3030962048_f71948226c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2990977776_1ec51c9281.jpg: 640x416 1 person, 4 cars, 2 skateboards, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2990977776_1ec51c9281.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632047678_f202609e50.jpg: 544x640 6 persons, 2 teddy bears, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3632047678_f202609e50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477681171_b1bb8b211d.jpg: 640x448 1 person, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3477681171_b1bb8b211d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/407569668_19b3f8eaf6.jpg: 448x640 4 persons, 1 traffic light, 2 chairs, 2 laptops, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 407569668_19b3f8eaf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2213113526_beeb4f9bdc.jpg: 640x448 2 persons, 1 bench, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2213113526_beeb4f9bdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2077079696_03380d218b.jpg: 640x448 1 person, 1 backpack, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2077079696_03380d218b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/211277478_7d43aaee09.jpg: 448x640 1 truck, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 211277478_7d43aaee09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2327088022_478dbd2c17.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2327088022_478dbd2c17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1335617803_4fbc03dab0.jpg: 480x640 2 persons, 1 potted plant, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1335617803_4fbc03dab0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1807169176_7f5226bf5a.jpg: 448x640 5 persons, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1807169176_7f5226bf5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/747921928_48eb02aab2.jpg: 640x512 1 person, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 747921928_48eb02aab2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3028404926_2bd27e3e83.jpg: 640x480 1 person, 1 bicycle, 1 bench, 2 potted plants, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3028404926_2bd27e3e83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3212625256_685bc4de99.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3212625256_685bc4de99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2173312932_269f9786fc.jpg: 448x640 2 persons, 3 dogs, 1 sports ball, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2173312932_269f9786fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2372572028_53b76104a9.jpg: 640x512 2 persons, 1 car, 1 motorcycle, 1 truck, 1 suitcase, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2372572028_53b76104a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1405221276_21634dcd58.jpg: 480x640 1 dog, 1 sheep, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1405221276_21634dcd58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2208067635_39a03834ca.jpg: 640x448 1 person, 1 frisbee, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2208067635_39a03834ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2664103423_c539f61016.jpg: 448x640 2 persons, 1 kite, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2664103423_c539f61016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429351964_531de1bf16.jpg: 640x480 2 persons, 3 cars, 1 truck, 1 banana, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3429351964_531de1bf16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2300229745_af7a20c959.jpg: 640x544 1 dog, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2300229745_af7a20c959.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666056567_661e25f54c.jpg: 640x640 4 persons, 1 handbag, 1 tie, 7.6ms\n",
      "Speed: 2.4ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3666056567_661e25f54c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/464251704_b0f0c4c87a.jpg: 480x640 1 person, 1 dog, 2 kites, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 464251704_b0f0c4c87a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3097171315_0ba7d283b1.jpg: 480x640 8 persons, 1 handbag, 1 couch, 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3097171315_0ba7d283b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2486364531_b482d7f521.jpg: 448x640 1 person, 1 frisbee, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2486364531_b482d7f521.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1417637704_572b4d6557.jpg: 480x640 2 persons, 1 car, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1417637704_572b4d6557.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1130369873_d80a1aa59c.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1130369873_d80a1aa59c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2382411771_a16145f345.jpg: 448x640 3 cows, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2382411771_a16145f345.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1285874746_486731a954.jpg: 448x640 9 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1285874746_486731a954.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2644920808_f5a214b744.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2644920808_f5a214b744.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3626998066_3ae11ee278.jpg: 448x640 2 dogs, 2 frisbees, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3626998066_3ae11ee278.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2922973230_5a769ef92a.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2922973230_5a769ef92a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033686219_452b172ab0.jpg: 416x640 2 dogs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3033686219_452b172ab0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3039209547_81cc93fbec.jpg: 640x224 2 persons, 93.6ms\n",
      "Speed: 1.0ms preprocess, 93.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 224)\n",
      "Cropped images saved for 3039209547_81cc93fbec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356700488_183566145b.jpg: 512x640 4 persons, 3 cars, 1 stop sign, 9.6ms\n",
      "Speed: 3.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3356700488_183566145b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3096163135_584901a5ae.jpg: 480x640 13 persons, 3 snowboards, 13.5ms\n",
      "Speed: 3.5ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3096163135_584901a5ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/536721406_884ab8fece.jpg: 480x640 1 dog, 1 horse, 1 sheep, 10.2ms\n",
      "Speed: 3.0ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 536721406_884ab8fece.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/274313927_404d0e94ab.jpg: 480x640 1 dog, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 274313927_404d0e94ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2819254573_9ecb5f4d5e.jpg: 480x640 4 persons, 1 chair, 10.0ms\n",
      "Speed: 2.5ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2819254573_9ecb5f4d5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339768802_8ab768558a.jpg: 416x640 2 persons, 3 sports balls, 10.4ms\n",
      "Speed: 2.0ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3339768802_8ab768558a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323952123_deb50b0629.jpg: 448x640 1 dog, 10.0ms\n",
      "Speed: 2.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3323952123_deb50b0629.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726590391_bc6e729bb6.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3726590391_bc6e729bb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2994205788_f8b3f2e840.jpg: 640x640 2 persons, 1 baseball bat, 9.7ms\n",
      "Speed: 2.8ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2994205788_f8b3f2e840.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2603334363_cfa32c4482.jpg: 640x480 1 person, 1 motorcycle, 2 frisbees, 1 sports ball, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2603334363_cfa32c4482.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2481367956_8577d2fa98.jpg: 640x480 2 persons, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2481367956_8577d2fa98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2709536455_2a6046e38a.jpg: 640x448 1 dog, 14.7ms\n",
      "Speed: 3.5ms preprocess, 14.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2709536455_2a6046e38a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2902661518_1513be3ea6.jpg: 448x640 1 person, 1 skateboard, 10.2ms\n",
      "Speed: 2.1ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2902661518_1513be3ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1597319381_1e80d9e39c.jpg: 448x640 7 persons, 1 sports ball, 1 baseball glove, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1597319381_1e80d9e39c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/102455176_5f8ead62d5.jpg: 640x448 1 person, 11.1ms\n",
      "Speed: 2.4ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 102455176_5f8ead62d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1012212859_01547e3f17.jpg: 448x640 1 dog, 1 sports ball, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1012212859_01547e3f17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3346040664_5b584e6133.jpg: 640x512 9 persons, 2 ties, 1 sports ball, 9.6ms\n",
      "Speed: 2.8ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3346040664_5b584e6133.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/827941668_2e4ac6cb39.jpg: 480x640 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 827941668_2e4ac6cb39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/689359034_4a64c24ca4.jpg: 480x640 4 persons, 1 car, 1 truck, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 689359034_4a64c24ca4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1965278563_8279e408de.jpg: 448x640 1 boat, 1 dog, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1965278563_8279e408de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2086534745_1e4ab80078.jpg: 448x640 1 surfboard, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2086534745_1e4ab80078.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420338549_bd78d35243.jpg: 448x640 6 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3420338549_bd78d35243.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635023078_6dae04758f.jpg: 448x640 2 persons, 1 clock, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2635023078_6dae04758f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2696394827_7342ced36f.jpg: 448x640 1 person, 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2696394827_7342ced36f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3130064588_6d1d3fa2dd.jpg: 448x640 2 persons, 1 surfboard, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3130064588_6d1d3fa2dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/141139674_246c0f90a1.jpg: 448x640 2 persons, 1 sports ball, 1 baseball bat, 12.5ms\n",
      "Speed: 3.6ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 141139674_246c0f90a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2775744946_1ab5d500a2.jpg: 480x640 1 person, 11.9ms\n",
      "Speed: 2.8ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2775744946_1ab5d500a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3706653103_e777a825e4.jpg: 448x640 1 person, 1 bicycle, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3706653103_e777a825e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2199200615_85e4c2a602.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2199200615_85e4c2a602.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3072114570_e1c0127529.jpg: 448x640 3 persons, 1 bicycle, 1 frisbee, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3072114570_e1c0127529.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597354819_0069aaf16e.jpg: 480x640 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3597354819_0069aaf16e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2759860913_f75b39d783.jpg: 480x640 1 person, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2759860913_f75b39d783.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609032038_005c789f64.jpg: 640x448 2 persons, 1 bicycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3609032038_005c789f64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163281186_e2f43dfb5f.jpg: 448x640 2 persons, 2 elephants, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3163281186_e2f43dfb5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2929506802_5432054d77.jpg: 640x448 2 persons, 1 handbag, 1 suitcase, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2929506802_5432054d77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/119534510_d52b3781a3.jpg: 448x640 1 person, 1 boat, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 119534510_d52b3781a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3234115903_f4dfc8fc75.jpg: 448x640 5 persons, 1 sports ball, 1 tennis racket, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3234115903_f4dfc8fc75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2045023435_181854c013.jpg: 416x640 1 dog, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2045023435_181854c013.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677302645_8cd3fac70d.jpg: 448x640 6 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677302645_8cd3fac70d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090545563_a4e66ec76b.jpg: 448x640 2 persons, 1 backpack, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2090545563_a4e66ec76b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126795109_73920ed5dc.jpg: 640x512 15 persons, 1 sports ball, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3126795109_73920ed5dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2667015110_1670324a33.jpg: 640x448 4 persons, 11.7ms\n",
      "Speed: 2.8ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2667015110_1670324a33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/327621377_0bc3b7fd26.jpg: 640x576 1 dog, 10.4ms\n",
      "Speed: 2.8ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 327621377_0bc3b7fd26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3195040792_a03954a19f.jpg: 448x640 1 bird, 3 dogs, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3195040792_a03954a19f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227148358_f152303584.jpg: 512x640 1 bird, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3227148358_f152303584.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2512682478_b67cc525c7.jpg: 640x576 3 persons, 9.1ms\n",
      "Speed: 2.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2512682478_b67cc525c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2276314067_7ee246f859.jpg: 544x640 1 dog, 1 sheep, 9.4ms\n",
      "Speed: 2.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2276314067_7ee246f859.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2775249812_f4db95e818.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2775249812_f4db95e818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/200771289_31902164a7.jpg: 480x640 4 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 200771289_31902164a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2306186887_0bd8ed3792.jpg: 640x640 2 dogs, 1 sports ball, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2306186887_0bd8ed3792.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421131122_2e4bde661e.jpg: 480x640 1 person, 1 surfboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3421131122_2e4bde661e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1813597483_3f09d2a020.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1813597483_3f09d2a020.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154693053_cfcd05c226.jpg: 640x448 5 persons, 1 sports ball, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3154693053_cfcd05c226.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583704941_611353857e.jpg: 640x448 1 person, 1 horse, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3583704941_611353857e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2403078014_4b1d6f8bde.jpg: 640x480 3 persons, 2 bicycles, 2 cars, 2 benchs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2403078014_4b1d6f8bde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2579899436_5086a33c7a.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2579899436_5086a33c7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/229978782_3c690f5a0e.jpg: 640x448 2 persons, 1 frisbee, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 229978782_3c690f5a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3280173193_98c2d6a223.jpg: 448x640 6 persons, 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3280173193_98c2d6a223.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159995270_17334ccb5b.jpg: 352x640 1 person, 1 skis, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3159995270_17334ccb5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/124881487_36e668145d.jpg: 640x480 1 person, 1 bicycle, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 124881487_36e668145d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2441354291_b32e00e5a6.jpg: 640x480 1 person, 12.8ms\n",
      "Speed: 3.6ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2441354291_b32e00e5a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2434074318_e35a567220.jpg: 448x640 1 person, 1 surfboard, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2434074318_e35a567220.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2590207488_ddd89037ba.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2590207488_ddd89037ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2570559405_dc93007f76.jpg: 608x640 1 sheep, 2 cows, 9.4ms\n",
      "Speed: 3.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2570559405_dc93007f76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3255620561_7644747791.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3255620561_7644747791.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275711232_e261143664.jpg: 480x640 1 person, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3275711232_e261143664.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3422458549_f3f3878dbf.jpg: 512x640 2 persons, 1 sports ball, 1 tennis racket, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3422458549_f3f3878dbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468871328_72990babd4.jpg: 480x640 3 dogs, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 468871328_72990babd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2968885599_0672a5f016.jpg: 640x512 3 persons, 7 dogs, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2968885599_0672a5f016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3067500667_0fce8f28d4.jpg: 448x640 9 persons, 8 chairs, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3067500667_0fce8f28d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2505465055_f1e6cf9b76.jpg: 448x640 1 dog, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2505465055_f1e6cf9b76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2552723989_7bc93e0f7b.jpg: 640x480 4 persons, 1 snowboard, 1 skateboard, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2552723989_7bc93e0f7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2133650765_fc6e5f295e.jpg: 448x640 3 persons, 1 car, 1 umbrella, 1 frisbee, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2133650765_fc6e5f295e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533451027_b078e4631b.jpg: 480x640 1 person, 1 frisbee, 1 sports ball, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3533451027_b078e4631b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423138514_950f79e432.jpg: 512x640 2 persons, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2423138514_950f79e432.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326273086_e09e845185.jpg: 512x640 1 bear, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3326273086_e09e845185.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502079538_10ef2e976b.jpg: 448x640 2 persons, 1 horse, 1 cow, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2502079538_10ef2e976b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405534893_2d0f3b0147.jpg: 640x448 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 405534893_2d0f3b0147.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397228832_8ce5b1c26f.jpg: 448x640 1 person, 2 surfboards, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3397228832_8ce5b1c26f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1094462889_f9966dafa6.jpg: 480x640 1 bench, 1 dog, 5 chairs, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1094462889_f9966dafa6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2592019072_a6c0090da4.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2592019072_a6c0090da4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490874218_babb404b39.jpg: 448x640 1 person, 2 bicycles, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3490874218_babb404b39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582742297_1daa29968e.jpg: 416x640 2 dogs, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3582742297_1daa29968e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2305437797_e6c3460190.jpg: 480x640 (no detections), 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2305437797_e6c3460190.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/440190907_bf8b7ba8ef.jpg: 448x640 11 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 440190907_bf8b7ba8ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425835357_204e620a66.jpg: 640x448 3 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3425835357_204e620a66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472540184_b0420b921a.jpg: 544x640 2 persons, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3472540184_b0420b921a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/214543992_ce6c0d9f9b.jpg: 640x640 2 dogs, 1 potted plant, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 214543992_ce6c0d9f9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/169490297_b6ff13632a.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 169490297_b6ff13632a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2684323357_c7a6d05d05.jpg: 480x640 1 person, 10 chairs, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2684323357_c7a6d05d05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514019869_7de4ece2a5.jpg: 448x640 2 persons, 1 dog, 1 surfboard, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514019869_7de4ece2a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2396100671_3a9d67f03d.jpg: 448x640 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2396100671_3a9d67f03d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3572346664_e1e6c77f11.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3572346664_e1e6c77f11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2094543127_46d2f1fedf.jpg: 416x640 1 person, 1 cat, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2094543127_46d2f1fedf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3453019315_cfd5c10dae.jpg: 640x512 2 persons, 1 bicycle, 1 motorcycle, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3453019315_cfd5c10dae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2094810449_f8df9dcdf7.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2094810449_f8df9dcdf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978271431_f6a7f19825.jpg: 448x640 2 cars, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2978271431_f6a7f19825.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3076928208_5763e9eb8c.jpg: 480x640 1 dog, 1 horse, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3076928208_5763e9eb8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484842724_ef1124c87a.jpg: 448x640 5 persons, 3 baseball gloves, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3484842724_ef1124c87a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330654550_3efe9a71af.jpg: 448x640 6 persons, 1 bicycle, 3 cars, 1 bus, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3330654550_3efe9a71af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2558911884_856dfc3951.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2558911884_856dfc3951.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421316045_ae7a1eb4bc.jpg: 480x640 3 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 421316045_ae7a1eb4bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661567396_cbe4c2e5be.jpg: 640x448 1 person, 1 chair, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2661567396_cbe4c2e5be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/423066487_07757b2b49.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 423066487_07757b2b49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2148991939_3b9fd6c439.jpg: 512x640 1 bench, 1 dog, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2148991939_3b9fd6c439.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3282121432_648dac8a29.jpg: 640x512 1 person, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3282121432_648dac8a29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911928620_06c3fa293e.jpg: 640x448 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2911928620_06c3fa293e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537201804_ce07aff237.jpg: 416x640 6 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3537201804_ce07aff237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319869052_08b000e4af.jpg: 448x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 319869052_08b000e4af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2490768374_45d94fc658.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2490768374_45d94fc658.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1573017288_4d481856e2.jpg: 640x576 4 persons, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 1573017288_4d481856e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208032657_27b9d6c4f3.jpg: 448x640 1 person, 1 bench, 4 chairs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3208032657_27b9d6c4f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3410902181_b2a3c6eec9.jpg: 640x480 1 person, 2 frisbees, 1 kite, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3410902181_b2a3c6eec9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534200447_b0f3ff02be.jpg: 480x640 2 persons, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 534200447_b0f3ff02be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2693539377_5442430f81.jpg: 512x640 3 persons, 1 car, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2693539377_5442430f81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2194495372_bdac7d9e71.jpg: 480x640 7 persons, 8 cars, 1 backpack, 1 sports ball, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2194495372_bdac7d9e71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631986552_944ea208fc.jpg: 448x640 1 person, 1 surfboard, 14.0ms\n",
      "Speed: 3.8ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3631986552_944ea208fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3283913180_7d4e43602d.jpg: 448x640 17 persons, 1 backpack, 9.9ms\n",
      "Speed: 2.6ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3283913180_7d4e43602d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661437618_ca7a15f3cb.jpg: 448x640 2 dogs, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2661437618_ca7a15f3cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2607099736_8681f601d9.jpg: 448x640 7 persons, 1 handbag, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2607099736_8681f601d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2739331794_4ae78f69a0.jpg: 448x640 1 person, 1 motorcycle, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2739331794_4ae78f69a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472250097_a3191a94b3.jpg: 416x640 1 person, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2472250097_a3191a94b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987775031_3f9ac69319.jpg: 480x640 6 persons, 1 bicycle, 1 bench, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2987775031_3f9ac69319.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1308617539_54e1a3dfbe.jpg: 640x448 2 persons, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1308617539_54e1a3dfbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109688427_d2e702456c.jpg: 448x640 3 persons, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3109688427_d2e702456c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187924573_203223e6c0.jpg: 448x640 3 birds, 10.8ms\n",
      "Speed: 2.9ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3187924573_203223e6c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/518144037_9a1754b2a6.jpg: 448x640 4 persons, 1 cell phone, 11.7ms\n",
      "Speed: 3.3ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 518144037_9a1754b2a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/507758961_e63ca126cc.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 507758961_e63ca126cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2100046085_69b59b6645.jpg: 480x640 1 bird, 2 dogs, 2 bears, 11.3ms\n",
      "Speed: 2.9ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2100046085_69b59b6645.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1584315962_5b0b45d02d.jpg: 480x640 1 person, 2 backpacks, 1 sports ball, 8.5ms\n",
      "Speed: 2.4ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1584315962_5b0b45d02d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671777903_6fbf643980.jpg: 448x640 4 persons, 6 kites, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3671777903_6fbf643980.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523972229_d44e9ff6d7.jpg: 640x448 1 person, 1 skateboard, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3523972229_d44e9ff6d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/745880539_cd3f948837.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 745880539_cd3f948837.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3068994801_b2bc079e67.jpg: 448x640 1 person, 1 bicycle, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3068994801_b2bc079e67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2768248810_06d543c080.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2768248810_06d543c080.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3756150099_50882fc029.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3756150099_50882fc029.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3463034205_e541313038.jpg: 640x448 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3463034205_e541313038.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2261346505_302c67951d.jpg: 448x640 8 persons, 3 cars, 8 umbrellas, 1 chair, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2261346505_302c67951d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3175434849_859f09fe07.jpg: 384x640 12 persons, 1 handbag, 6 teddy bears, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3175434849_859f09fe07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585495069_33cba06d0a.jpg: 384x640 4 persons, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3585495069_33cba06d0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3278777548_290b881018.jpg: 640x448 3 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3278777548_290b881018.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1007320043_627395c3d8.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1007320043_627395c3d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2734219983_fe86a60bf9.jpg: 640x448 3 persons, 3 cars, 1 umbrella, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2734219983_fe86a60bf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2553024095_735bc46267.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2553024095_735bc46267.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2467821766_0510c9a2d1.jpg: 640x480 4 persons, 1 backpack, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2467821766_0510c9a2d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3036971334_78187a9570.jpg: 640x448 4 persons, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3036971334_78187a9570.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405942945_f4af2934a6.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3405942945_f4af2934a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/375392855_54d46ed5c8.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 375392855_54d46ed5c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220200084_3ea129336e.jpg: 448x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3220200084_3ea129336e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2735979477_eef7c680f9.jpg: 448x640 2 dogs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2735979477_eef7c680f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417788829_cfdbc34d2c.jpg: 480x640 4 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3417788829_cfdbc34d2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3385246141_a263d1053e.jpg: 448x640 2 persons, 1 kite, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3385246141_a263d1053e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/314685044_da4390728e.jpg: 448x640 1 person, 4 cars, 1 truck, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 314685044_da4390728e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414710960_a4cde4af60.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2414710960_a4cde4af60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3241487502_f4f0cc4a8a.jpg: 448x640 1 dog, 1 sheep, 1 cow, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3241487502_f4f0cc4a8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421706363_a3e22a4908.jpg: 448x640 1 person, 1 bicycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421706363_a3e22a4908.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399616238_77acf4ee12.jpg: 640x512 3 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3399616238_77acf4ee12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/209605542_ca9cc52e7b.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 209605542_ca9cc52e7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/154871781_ae77696b77.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 154871781_ae77696b77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1991806812_065f747689.jpg: 448x640 12 persons, 1 skateboard, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1991806812_065f747689.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3625049113_554d82c2a1.jpg: 640x448 1 person, 3 cars, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3625049113_554d82c2a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331190056_09f4ca9fd2.jpg: 448x640 2 persons, 11.3ms\n",
      "Speed: 1.6ms preprocess, 11.3ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3331190056_09f4ca9fd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482879314_d3387e95b1.jpg: 640x448 9 persons, 1 bicycle, 14.4ms\n",
      "Speed: 3.1ms preprocess, 14.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3482879314_d3387e95b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3254640083_eb34b8edfe.jpg: 640x448 3 persons, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3254640083_eb34b8edfe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484832904_08619300d9.jpg: 640x448 5 persons, 1 sports ball, 1 baseball bat, 10.9ms\n",
      "Speed: 2.8ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3484832904_08619300d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3234401637_84e0d14414.jpg: 512x640 3 persons, 2 bicycles, 10.1ms\n",
      "Speed: 2.6ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3234401637_84e0d14414.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3017203816_5dc2a6b392.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3017203816_5dc2a6b392.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3124964754_2e8a98fb09.jpg: 640x640 1 person, 5 cars, 1 skateboard, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3124964754_2e8a98fb09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/112178718_87270d9b4d.jpg: 640x384 1 person, 1 surfboard, 10.3ms\n",
      "Speed: 1.8ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 112178718_87270d9b4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3526431764_056d2c61dc.jpg: 448x640 4 persons, 1 handbag, 2 cell phones, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3526431764_056d2c61dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/530950375_eea665583f.jpg: 448x640 2 persons, 1 bicycle, 4 cars, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 530950375_eea665583f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/480200554_6155e9dfeb.jpg: 640x512 2 persons, 2 donuts, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 480200554_6155e9dfeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1317292658_ba29330a0b.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1317292658_ba29330a0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3377117696_af91f13058.jpg: 448x640 1 person, 1 snowboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3377117696_af91f13058.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2060031241_a3ae7a06bb.jpg: 480x640 1 person, 1 tv, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2060031241_a3ae7a06bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/358875403_f357f03713.jpg: 448x640 1 bird, 1 horse, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 358875403_f357f03713.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/145721498_a27d2db576.jpg: 384x640 2 persons, 1 surfboard, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 145721498_a27d2db576.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3679707139_1cc1e71237.jpg: 640x448 1 dog, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3679707139_1cc1e71237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3435015880_eda46ff50f.jpg: 480x640 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3435015880_eda46ff50f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222749441_3bdfe088e3.jpg: 480x640 3 persons, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3222749441_3bdfe088e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347391_4fcd4639f4.jpg: 448x640 8 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347391_4fcd4639f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/500446858_125702b296.jpg: 448x640 2 persons, 2 dogs, 1 sports ball, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 500446858_125702b296.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/684375286_09cc1aa778.jpg: 480x640 3 dogs, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 684375286_09cc1aa778.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1383840121_c092110917.jpg: 512x640 1 dog, 2 beds, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1383840121_c092110917.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372214646_cc2ceb182f.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3372214646_cc2ceb182f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3412822878_5d961492e5.jpg: 448x640 1 person, 1 skateboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3412822878_5d961492e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3343116398_59a5341f7f.jpg: 640x512 8 persons, 1 tie, 1 tennis racket, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3343116398_59a5341f7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2456907314_49bc4591c4.jpg: 448x640 1 person, 1 boat, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2456907314_49bc4591c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596619849_7b635dd310.jpg: 640x480 4 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2596619849_7b635dd310.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3381038951_225bb163af.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3381038951_225bb163af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628017876_4ac27e687b.jpg: 640x512 1 person, 1 sports ball, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3628017876_4ac27e687b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/494792770_2c5f767ac0.jpg: 608x640 1 dog, 1 frisbee, 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 494792770_2c5f767ac0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2908466042_bf07cb52c7.jpg: 608x640 2 dogs, 1 tv, 6.5ms\n",
      "Speed: 2.3ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2908466042_bf07cb52c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2544182005_3aa1332bf9.jpg: 480x640 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2544182005_3aa1332bf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461049169_e068ae4f25.jpg: 448x640 3 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3461049169_e068ae4f25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3548191125_514f47e493.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3548191125_514f47e493.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3471117376_40585c3fd1.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3471117376_40585c3fd1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256272547_5ae6c66293.jpg: 480x640 7 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3256272547_5ae6c66293.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2604825598_593a825b5b.jpg: 448x640 2 dogs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2604825598_593a825b5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3526805681_38461c0d5d.jpg: 640x480 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3526805681_38461c0d5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3312096605_f458757418.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3312096605_f458757418.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3188044631_ca3a9cc737.jpg: 512x640 2 persons, 2 potted plants, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3188044631_ca3a9cc737.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287073593_f3d2a62455.jpg: 384x640 1 person, 6 dogs, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1287073593_f3d2a62455.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3104909823_0f41dd8be6.jpg: 448x640 5 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3104909823_0f41dd8be6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182996527_70d9c323d5.jpg: 640x448 1 person, 1 bicycle, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3182996527_70d9c323d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2407470303_6fd5e3600d.jpg: 480x640 3 persons, 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2407470303_6fd5e3600d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2520255786_b70a3ec032.jpg: 480x640 1 person, 2 frisbees, 1 surfboard, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2520255786_b70a3ec032.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3657016761_d553e514d9.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3657016761_d553e514d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3337046794_296bd2c7e0.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3337046794_296bd2c7e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3512791890_eb065b460a.jpg: 640x448 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3512791890_eb065b460a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1572286502_64e5c4b920.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1572286502_64e5c4b920.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/356929855_6bbf33d933.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 356929855_6bbf33d933.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3184112120_6ddcd98016.jpg: 480x640 4 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3184112120_6ddcd98016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561543598_3c1b572f9b.jpg: 448x640 11 persons, 2 ties, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3561543598_3c1b572f9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568605391_54ec367d88.jpg: 480x640 3 persons, 1 car, 1 motorcycle, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3568605391_54ec367d88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873070704_2141a7a86a.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2873070704_2141a7a86a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/160805827_5e6646b753.jpg: 384x640 1 person, 1 cup, 1 chair, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 160805827_5e6646b753.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2699733386_c346c87ea6.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2699733386_c346c87ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2439384468_58934deab6.jpg: 288x640 2 dogs, 8.5ms\n",
      "Speed: 1.0ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2439384468_58934deab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345522_c3c266a02a.jpg: 448x640 10 persons, 1 sports ball, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345522_c3c266a02a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/897406883_f09f673d94.jpg: 448x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 897406883_f09f673d94.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2457052334_b5a1d99048.jpg: 640x448 2 persons, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2457052334_b5a1d99048.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468102269_135938e209.jpg: 448x640 4 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 468102269_135938e209.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3403263046_3cc701a07a.jpg: 640x448 3 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3403263046_3cc701a07a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236730743_0d4fd8de5a.jpg: 640x448 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 236730743_0d4fd8de5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3243588540_b418ac7eda.jpg: 544x640 5 persons, 1 bottle, 1 chair, 1 cell phone, 1 book, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3243588540_b418ac7eda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215896272_e9b4b547a9.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3215896272_e9b4b547a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3553056438_4e611a7a2a.jpg: 640x416 2 persons, 7.0ms\n",
      "Speed: 1.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3553056438_4e611a7a2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/286660725_ffdbdf3481.jpg: 480x640 (no detections), 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 286660725_ffdbdf3481.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533775651_9d7e93dacf.jpg: 480x640 1 person, 4 cars, 1 handbag, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3533775651_9d7e93dacf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2933643390_1c6086684b.jpg: 512x640 8 persons, 1 sports ball, 1 baseball glove, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2933643390_1c6086684b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2907244809_07ab2c6b6c.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2907244809_07ab2c6b6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3592968286_b63c81bcd2.jpg: 480x640 3 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3592968286_b63c81bcd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3060594966_030658d318.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3060594966_030658d318.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2893515010_4a3d9dcc67.jpg: 640x512 4 persons, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2893515010_4a3d9dcc67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1084104085_3b06223afe.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1084104085_3b06223afe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1478268555_7e301fc510.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1478268555_7e301fc510.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2217728745_92b6779016.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2217728745_92b6779016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3361882891_6e610ffdbb.jpg: 448x640 2 persons, 1 bicycle, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3361882891_6e610ffdbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3525841965_7814484515.jpg: 512x640 4 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3525841965_7814484515.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/463786229_b54c9a3436.jpg: 640x480 1 potted plant, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 463786229_b54c9a3436.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1348947380_14f0fc1237.jpg: 608x640 1 person, 1 dog, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1348947380_14f0fc1237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426964258_67a0cee201.jpg: 448x640 7 persons, 2 umbrellas, 1 kite, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3426964258_67a0cee201.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/381239475_044cbffa2b.jpg: 448x640 1 bear, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 381239475_044cbffa2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1536774449_e16b1b6382.jpg: 480x640 10 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1536774449_e16b1b6382.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/133189853_811de6ab2a.jpg: 480x640 6 persons, 2 ties, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 133189853_811de6ab2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2858759108_6e697c5f3e.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2858759108_6e697c5f3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866529477_7e0c053ebc.jpg: 448x640 2 persons, 1 surfboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2866529477_7e0c053ebc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3551003620_0b02d76f65.jpg: 512x640 10 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3551003620_0b02d76f65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415646718_f9f4e23a66.jpg: 512x640 8 persons, 1 cow, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3415646718_f9f4e23a66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/403678611_73978faed7.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 403678611_73978faed7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1467533293_a2656cc000.jpg: 480x640 3 persons, 1 umbrella, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1467533293_a2656cc000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278608022_4175813019.jpg: 448x640 2 persons, 1 sports ball, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 278608022_4175813019.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2086678529_b3301c2d71.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2086678529_b3301c2d71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143688283_a96ded20f1.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 143688283_a96ded20f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1517340899_ee1c74a8f6.jpg: 640x480 1 person, 1 kite, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1517340899_ee1c74a8f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/887108308_2da97f15ef.jpg: 480x640 5 persons, 2 backpacks, 1 handbag, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 887108308_2da97f15ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3231596071_f0c522a639.jpg: 448x640 1 person, 1 chair, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3231596071_f0c522a639.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2114355355_9d7e2d8178.jpg: 640x448 1 boat, 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2114355355_9d7e2d8178.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1387785218_cee67735f5.jpg: 640x448 4 persons, 1 car, 1 dog, 4 sports balls, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1387785218_cee67735f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2999730677_0cfa1c146e.jpg: 448x640 3 dogs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2999730677_0cfa1c146e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1231229740_8dcbf80bfb.jpg: 640x480 2 persons, 2 couchs, 1 teddy bear, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1231229740_8dcbf80bfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/447722389_4b51b7e13d.jpg: 640x448 2 dogs, 1 horse, 1 cow, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 447722389_4b51b7e13d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3502563726_30d1ce29c8.jpg: 448x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3502563726_30d1ce29c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323076458_3ce72a1dae.jpg: 448x640 1 dog, 1 cow, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3323076458_3ce72a1dae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116011063_f4071ccce6.jpg: 448x640 1 person, 1 surfboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3116011063_f4071ccce6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/302983277_69a4e732e4.jpg: 480x640 1 person, 1 horse, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 302983277_69a4e732e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295452057_0c987f895f.jpg: 480x640 2 persons, 2 dogs, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3295452057_0c987f895f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/304408047_98bab3ea64.jpg: 512x640 2 dogs, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 304408047_98bab3ea64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/436608339_f1d1298770.jpg: 448x640 4 persons, 1 handbag, 1 tie, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 436608339_f1d1298770.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2806632713_edd6f6c893.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2806632713_edd6f6c893.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2801851082_8c3c480c0f.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2801851082_8c3c480c0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/909191414_1cf5d85821.jpg: 640x480 1 person, 2 tennis rackets, 2 potted plants, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 909191414_1cf5d85821.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2993167197_e5d0a73530.jpg: 640x512 7 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2993167197_e5d0a73530.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/251586160_a31b187a37.jpg: 640x640 1 person, 1 dog, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 251586160_a31b187a37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1415591512_a84644750c.jpg: 640x448 1 person, 1 car, 1 truck, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1415591512_a84644750c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2315867011_fc5fc9fa6d.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2315867011_fc5fc9fa6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/456299217_b2802efbc2.jpg: 448x640 4 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 456299217_b2802efbc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489551372_b19a6ad0ed.jpg: 448x640 1 person, 1 cow, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 489551372_b19a6ad0ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518334317_bc40bae18d.jpg: 640x448 4 persons, 1 bicycle, 1 motorcycle, 1 handbag, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3518334317_bc40bae18d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3465000218_c94e54e208.jpg: 448x640 10 persons, 1 cell phone, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3465000218_c94e54e208.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3248408149_41a8dd90d3.jpg: 448x640 1 dog, 1 cow, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3248408149_41a8dd90d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2359784186_36c9746d02.jpg: 448x640 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2359784186_36c9746d02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1244140539_da4804d828.jpg: 480x640 2 dogs, 1 chair, 1 bed, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1244140539_da4804d828.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192475933_d779bf42eb.jpg: 480x640 15 persons, 1 backpack, 1 umbrella, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2192475933_d779bf42eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2822290399_97c809d43b.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2822290399_97c809d43b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2300168895_a9b83e16fc.jpg: 416x640 1 dog, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2300168895_a9b83e16fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/900144365_03cd1899e3.jpg: 512x640 12 persons, 1 frisbee, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 900144365_03cd1899e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/364213568_7f83e7d144.jpg: 448x640 3 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 364213568_7f83e7d144.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3400385314_a5bc062e97.jpg: 640x448 3 persons, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3400385314_a5bc062e97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2193223202_4d908c0450.jpg: 448x640 3 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2193223202_4d908c0450.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/174466741_329a52b2fe.jpg: 640x512 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 174466741_329a52b2fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/464506846_1734302b58.jpg: 480x640 1 dog, 17.0ms\n",
      "Speed: 4.0ms preprocess, 17.0ms inference, 2.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 464506846_1734302b58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2288530008_4ebcee2174.jpg: 640x448 3 persons, 1 train, 2 handbags, 13.3ms\n",
      "Speed: 2.7ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2288530008_4ebcee2174.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224273695_0b517bd0eb.jpg: 448x640 2 persons, 13.2ms\n",
      "Speed: 2.2ms preprocess, 13.2ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 224273695_0b517bd0eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245912109_fdeef6b456.jpg: 448x640 21 persons, 1 backpack, 4 handbags, 11.0ms\n",
      "Speed: 2.9ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3245912109_fdeef6b456.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2157173498_2eea42ee38.jpg: 448x640 1 person, 1 bird, 1 snowboard, 9.6ms\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2157173498_2eea42ee38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3077166963_fe172c709d.jpg: 448x640 2 persons, 3 horses, 2 cows, 1 elephant, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3077166963_fe172c709d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1714316707_8bbaa2a2ba.jpg: 640x512 1 person, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1714316707_8bbaa2a2ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490044563_8eb551ef59.jpg: 480x640 4 persons, 1 umbrella, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3490044563_8eb551ef59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3471841031_a949645ba8.jpg: 480x640 6 persons, 1 bird, 1 bottle, 12.8ms\n",
      "Speed: 4.0ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3471841031_a949645ba8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3524519277_bd0c3e7382.jpg: 640x480 3 persons, 1 bottle, 11.5ms\n",
      "Speed: 2.8ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3524519277_bd0c3e7382.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537628742_146f2c24f8.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 537628742_146f2c24f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/330849796_c575c3108a.jpg: 448x640 2 persons, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 330849796_c575c3108a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1313961775_824b87d155.jpg: 416x640 1 person, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1313961775_824b87d155.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2642688531_8fb68b2147.jpg: 640x480 4 persons, 1 truck, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2642688531_8fb68b2147.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336831820_5c5df4b033.jpg: 480x640 8 persons, 1 tie, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3336831820_5c5df4b033.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3350614753_5624e181b3.jpg: 480x640 5 persons, 1 handbag, 8.5ms\n",
      "Speed: 2.3ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3350614753_5624e181b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2269795781_46a296aa21.jpg: 512x640 1 person, 1 sports ball, 1 tennis racket, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2269795781_46a296aa21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109704348_c6416244ce.jpg: 512x640 8 persons, 8.5ms\n",
      "Speed: 2.4ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3109704348_c6416244ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3571675421_7e07ac07c5.jpg: 480x640 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3571675421_7e07ac07c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/428485639_a82635d6ee.jpg: 480x640 1 person, 2 cars, 3 benchs, 8.4ms\n",
      "Speed: 2.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 428485639_a82635d6ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3358682439_be4b83544c.jpg: 448x640 3 persons, 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3358682439_be4b83544c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229519418_040f05ced1.jpg: 448x640 5 persons, 1 chair, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3229519418_040f05ced1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/427683329_95d510a087.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 427683329_95d510a087.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042484940_0975a5e486.jpg: 512x640 4 persons, 2 cell phones, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3042484940_0975a5e486.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506468593_7e41a6d9f1.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3506468593_7e41a6d9f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516761840_842dabc908.jpg: 416x640 7 persons, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 516761840_842dabc908.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949762776_52ece64d28.jpg: 448x640 14 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2949762776_52ece64d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921198890_6f70dfbf4c.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2921198890_6f70dfbf4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2918769188_565dd48060.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2918769188_565dd48060.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2081615901_13092cac56.jpg: 640x480 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2081615901_13092cac56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1402843760_d30f1dbf0f.jpg: 608x640 1 person, 1 sheep, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1402843760_d30f1dbf0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394922193_310166d6af.jpg: 480x640 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2394922193_310166d6af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262439544_e71cd26b24.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 262439544_e71cd26b24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3494105596_f05cb0d56f.jpg: 640x480 1 person, 2 sports balls, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3494105596_f05cb0d56f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3368865171_597d51cdd5.jpg: 640x576 13 persons, 1 bicycle, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3368865171_597d51cdd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2424976964_98f58a0618.jpg: 384x640 4 persons, 2 bicycles, 2 cars, 1 handbag, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2424976964_98f58a0618.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2344412916_9a5a9b1c82.jpg: 480x640 7 persons, 1 car, 1 skateboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2344412916_9a5a9b1c82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/860928274_744d14f198.jpg: 448x640 2 persons, 1 couch, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 860928274_744d14f198.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3577309234_c952c2af86.jpg: 480x640 1 bird, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3577309234_c952c2af86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356938707_d95ba97430.jpg: 480x640 6 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3356938707_d95ba97430.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1294578091_2ad02fea91.jpg: 448x640 5 persons, 3 handbags, 1 bottle, 5 chairs, 4 cell phones, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1294578091_2ad02fea91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461041826_0e24cdf597.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3461041826_0e24cdf597.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041170372_c4376cd497.jpg: 448x640 1 person, 1 bicycle, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3041170372_c4376cd497.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3054200086_657d4398e8.jpg: 480x640 2 persons, 1 bicycle, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3054200086_657d4398e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410399168_1462c422d4.jpg: 448x640 3 dogs, 1 sheep, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2410399168_1462c422d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215238223_29de2b35cb.jpg: 640x448 2 persons, 1 bicycle, 1 car, 1 truck, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3215238223_29de2b35cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2715035273_8fc8b1291c.jpg: 512x640 8 persons, 1 handbag, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2715035273_8fc8b1291c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1389264266_8170bc1c54.jpg: 640x512 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1389264266_8170bc1c54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3269380710_9161b0bd00.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3269380710_9161b0bd00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677329561_fa3e1fdcf9.jpg: 512x640 2 persons, 1 dog, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3677329561_fa3e1fdcf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2564663851_3a9832e4fc.jpg: 448x640 1 person, 1 bench, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2564663851_3a9832e4fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862931640_2501bd36c5.jpg: 448x640 18 persons, 1 bicycle, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2862931640_2501bd36c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/411863595_d77156687e.jpg: 448x640 1 person, 1 cow, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 411863595_d77156687e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474985008_0a827cd340.jpg: 640x448 1 person, 1 bicycle, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3474985008_0a827cd340.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582814058_564776f26c.jpg: 544x640 5 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3582814058_564776f26c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/281419391_522557ce27.jpg: 448x640 9 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 281419391_522557ce27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518608016_46453d8b18.jpg: 640x480 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3518608016_46453d8b18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2845845721_d0bc113ff7.jpg: 480x640 2 persons, 1 kite, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2845845721_d0bc113ff7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430782104_34da558eba.jpg: 640x384 3 persons, 2 horses, 7.9ms\n",
      "Speed: 1.4ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3430782104_34da558eba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516960094_87fb4889de.jpg: 640x288 7 persons, 8.3ms\n",
      "Speed: 1.1ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Cropped images saved for 3516960094_87fb4889de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1096097967_ac305887b4.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1096097967_ac305887b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2484190118_e89363c465.jpg: 640x416 1 person, 3 cars, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2484190118_e89363c465.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2212472643_80238475b5.jpg: 448x640 1 person, 2 chairs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2212472643_80238475b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1119418776_58e4b93eac.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1119418776_58e4b93eac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/339350939_6643bfb270.jpg: 480x640 1 person, 1 bird, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 339350939_6643bfb270.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621647714_fc67ab2617.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3621647714_fc67ab2617.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3629664676_36bcefe6b7.jpg: 640x576 1 dog, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3629664676_36bcefe6b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3097776588_312932e438.jpg: 640x480 2 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3097776588_312932e438.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2926595608_69b22be8d4.jpg: 640x416 10 persons, 4 chairs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2926595608_69b22be8d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506216254_04d119cac7.jpg: 448x640 5 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3506216254_04d119cac7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3650986674_3e101c606b.jpg: 480x640 1 person, 2 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3650986674_3e101c606b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301935788_2bb7bbc515.jpg: 448x640 6 persons, 1 snowboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3301935788_2bb7bbc515.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3491013009_572cf2c18a.jpg: 640x448 5 persons, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3491013009_572cf2c18a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3407357681_5aeaab5b59.jpg: 384x640 1 dog, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3407357681_5aeaab5b59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2095478050_736c4d2d28.jpg: 512x640 9 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2095478050_736c4d2d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488590040_35a3e96c89.jpg: 448x640 2 persons, 1 boat, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 488590040_35a3e96c89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3224227640_31865b3651.jpg: 416x640 2 dogs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3224227640_31865b3651.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241031670_e60f59b8e4.jpg: 448x640 10 persons, 6 umbrellas, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241031670_e60f59b8e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631474374_e40764d153.jpg: 448x640 5 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3631474374_e40764d153.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2451114871_8617ae2f16.jpg: 448x640 4 dogs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2451114871_8617ae2f16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/317641829_ab2607a6c0.jpg: 448x640 2 persons, 2 cars, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 317641829_ab2607a6c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638318149_b60450bfbe.jpg: 448x640 4 persons, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3638318149_b60450bfbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2112921744_92bf706805.jpg: 480x640 1 sheep, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2112921744_92bf706805.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3687222696_85bf6f78f7.jpg: 640x416 1 person, 1 frisbee, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3687222696_85bf6f78f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2795352290_9209b214f3.jpg: 480x640 1 person, 1 couch, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2795352290_9209b214f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2371809188_b805497cba.jpg: 640x480 2 persons, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2371809188_b805497cba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2366421102_2d60d53a0e.jpg: 640x640 1 person, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2366421102_2d60d53a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517127930_5dbddb45f6.jpg: 448x640 9 persons, 1 sports ball, 1 baseball bat, 1 baseball glove, 8.0ms\n",
      "Speed: 1.5ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3517127930_5dbddb45f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2479652566_8f9fac8af5.jpg: 640x480 1 person, 4 bowls, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2479652566_8f9fac8af5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472980433_210ec62874.jpg: 640x480 11 persons, 1 motorcycle, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2472980433_210ec62874.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3305895920_100a67d148.jpg: 640x448 7 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3305895920_100a67d148.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540203582_9a9ac92a5d.jpg: 640x448 1 person, 5 cars, 1 skateboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2540203582_9a9ac92a5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2781296531_f6f0f6c0f5.jpg: 448x640 1 person, 1 frisbee, 2 sports balls, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2781296531_f6f0f6c0f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1685463722_55843b6d3c.jpg: 640x448 1 person, 1 bicycle, 1 car, 1 dog, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1685463722_55843b6d3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/477768471_d7cd618fdb.jpg: 480x640 1 sheep, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 477768471_d7cd618fdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498417123_3eae6bbde6.jpg: 544x640 8 persons, 1 baseball glove, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3498417123_3eae6bbde6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2528552898_9e49a7033f.jpg: 448x640 16 persons, 1 car, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2528552898_9e49a7033f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271955077_0020b4ee0d.jpg: 640x384 1 person, 1 snowboard, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2271955077_0020b4ee0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3565021218_d2bc1aa644.jpg: 640x576 1 person, 1 bed, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3565021218_d2bc1aa644.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665179773_dd217416fc.jpg: 448x640 8 persons, 4 bicycles, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3665179773_dd217416fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3466353172_deb128bbb0.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3466353172_deb128bbb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2609836649_b55831ed41.jpg: 480x640 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2609836649_b55831ed41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2955985301_e4139bc772.jpg: 640x544 1 person, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2955985301_e4139bc772.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3201427741_3033f5b625.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3201427741_3033f5b625.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2883324329_24361e2d49.jpg: 640x448 3 persons, 1 potted plant, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2883324329_24361e2d49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/352382023_7605223d1c.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 352382023_7605223d1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/314904143_5a216a192b.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 314904143_5a216a192b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3172369593_eb4d787ffb.jpg: 448x640 3 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3172369593_eb4d787ffb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3529314899_b4e533bb76.jpg: 448x640 14 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3529314899_b4e533bb76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1352398363_9cc8ffcce9.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1352398363_9cc8ffcce9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2339106348_2df90aa6a9.jpg: 448x640 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2339106348_2df90aa6a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474912569_7165dc1d06.jpg: 640x480 7 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3474912569_7165dc1d06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2099323664_bb20457f26.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2099323664_bb20457f26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512991147_dc48e6839c.jpg: 480x640 5 persons, 1 backpack, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 512991147_dc48e6839c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549140234_e99b07c739.jpg: 640x448 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3549140234_e99b07c739.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2888658480_e922a3dec2.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2888658480_e922a3dec2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2289068031_fe26990183.jpg: 640x480 4 persons, 1 kite, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2289068031_fe26990183.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/382151094_c7376cf22b.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 382151094_c7376cf22b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3643087589_627a0a9e01.jpg: 416x640 2 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3643087589_627a0a9e01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319338707_892ae2a660.jpg: 448x640 9 persons, 4 benchs, 1 bottle, 2 chairs, 1 dining table, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3319338707_892ae2a660.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681651647_08eba60f89.jpg: 640x448 3 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3681651647_08eba60f89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2144846312_d4c738dc6c.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2144846312_d4c738dc6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119696225_b289efaec8.jpg: 448x640 1 person, 1 motorcycle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3119696225_b289efaec8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2555622234_3e531e4014.jpg: 480x640 1 dog, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2555622234_3e531e4014.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2287887341_663bfa15af.jpg: 480x640 1 dog, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2287887341_663bfa15af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877088081_7ca408cb25.jpg: 640x544 2 persons, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2877088081_7ca408cb25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/333973142_abcd151002.jpg: 480x640 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 333973142_abcd151002.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3300679815_2c6c2301cb.jpg: 480x640 4 persons, 1 sports ball, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3300679815_2c6c2301cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3118425885_f0cc035032.jpg: 480x640 7 persons, 1 cup, 1 bowl, 2 chairs, 1 dining table, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3118425885_f0cc035032.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2091171488_c8512fec76.jpg: 640x544 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2091171488_c8512fec76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1057210460_09c6f4c6c1.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1057210460_09c6f4c6c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2325816912_b3bb41cdbb.jpg: 448x640 1 dog, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2325816912_b3bb41cdbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/852469220_bc0fee3623.jpg: 448x640 (no detections), 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 852469220_bc0fee3623.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616561200_ea079f285a.jpg: 576x640 1 cat, 1 sports ball, 12.2ms\n",
      "Speed: 4.1ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2616561200_ea079f285a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2655196158_5c878a4af0.jpg: 384x640 1 person, 1 dog, 1 bed, 1 teddy bear, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2655196158_5c878a4af0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1449370354_380c4123c9.jpg: 480x640 3 persons, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1449370354_380c4123c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623247254_3bfc795121.jpg: 448x640 2 persons, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2623247254_3bfc795121.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3759230208_1c2a492b12.jpg: 640x448 2 persons, 14.1ms\n",
      "Speed: 3.5ms preprocess, 14.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3759230208_1c2a492b12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2439813616_c9ac54cc9f.jpg: 640x448 1 person, 1 skateboard, 10.9ms\n",
      "Speed: 2.6ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2439813616_c9ac54cc9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3679341667_936769fd0c.jpg: 448x640 5 persons, 10.3ms\n",
      "Speed: 2.2ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3679341667_936769fd0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220161734_77f42734b9.jpg: 640x448 2 persons, 1 sports ball, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3220161734_77f42734b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1235580648_7eebaed9bc.jpg: 480x640 2 persons, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1235580648_7eebaed9bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2364394224_c17b09e035.jpg: 544x640 1 person, 1 dog, 11.4ms\n",
      "Speed: 3.1ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2364394224_c17b09e035.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2061927950_dafba5b8a3.jpg: 384x640 2 persons, 1 baseball glove, 10.8ms\n",
      "Speed: 1.8ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2061927950_dafba5b8a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2656688132_d93be870e0.jpg: 640x640 1 person, 1 dog, 1 handbag, 9.9ms\n",
      "Speed: 2.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2656688132_d93be870e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1389323170_d1c81d6b51.jpg: 608x640 10 persons, 1 handbag, 1 surfboard, 2 chairs, 1 cell phone, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1389323170_d1c81d6b51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2374570771_c395fc224a.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2374570771_c395fc224a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3741462565_cc35966b7a.jpg: 448x640 3 persons, 1 wine glass, 1 cup, 1 chair, 1 dining table, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3741462565_cc35966b7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2500354186_0836309cc9.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2500354186_0836309cc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3504275465_604ce2ef34.jpg: 640x480 1 dog, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3504275465_604ce2ef34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485245061_5a5de43e20.jpg: 480x640 1 person, 1 backpack, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 485245061_5a5de43e20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2562347802_c049a2ba88.jpg: 480x640 1 dog, 1 cow, 1 bear, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2562347802_c049a2ba88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3226541300_6c81711e8e.jpg: 640x448 2 persons, 1 baseball bat, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3226541300_6c81711e8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/899810584_61e1578d3f.jpg: 640x448 1 elephant, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 899810584_61e1578d3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2554081584_233bdf289a.jpg: 640x448 1 person, 1 horse, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2554081584_233bdf289a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3164347907_2813f8ff0b.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3164347907_2813f8ff0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3565655045_8eb00b7423.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3565655045_8eb00b7423.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2659183350_730951f740.jpg: 448x640 13 persons, 1 backpack, 2 sports balls, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2659183350_730951f740.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1456630952_dd4778a48f.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1456630952_dd4778a48f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/557721978_dfde31bc02.jpg: 640x544 1 person, 1 car, 9.1ms\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 557721978_dfde31bc02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2992658871_ac786d37a6.jpg: 448x640 2 persons, 1 bench, 1 skateboard, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2992658871_ac786d37a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/113678030_87a6a6e42e.jpg: 512x640 4 persons, 1 kite, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 113678030_87a6a6e42e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2453891449_fedb277908.jpg: 384x640 2 dogs, 1 frisbee, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2453891449_fedb277908.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3146232740_df3da0163b.jpg: 448x640 1 surfboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3146232740_df3da0163b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2655647656_ee450446ed.jpg: 448x640 7 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2655647656_ee450446ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019917636_4e0bb0acc4.jpg: 640x448 1 person, 1 bicycle, 2 cars, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3019917636_4e0bb0acc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223809913_ae15d14d9a.jpg: 640x448 4 persons, 3 skateboards, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3223809913_ae15d14d9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/515755283_8f890b3207.jpg: 480x640 5 persons, 1 motorcycle, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 515755283_8f890b3207.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3361210233_962d630ec5.jpg: 640x544 1 person, 1 dog, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3361210233_962d630ec5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/441817653_fbdf83060b.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 441817653_fbdf83060b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460134050_06de9f5c4a.jpg: 608x640 1 cow, 8.3ms\n",
      "Speed: 2.4ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2460134050_06de9f5c4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/261737543_b8fdc24671.jpg: 480x640 1 person, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 261737543_b8fdc24671.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185409663_95f6b958d8.jpg: 640x448 1 person, 12.7ms\n",
      "Speed: 3.2ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3185409663_95f6b958d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3505657604_8899161734.jpg: 640x384 2 persons, 1 chair, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3505657604_8899161734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/299676757_571ee47280.jpg: 512x640 14 persons, 2 umbrellas, 1 handbag, 9.7ms\n",
      "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 299676757_571ee47280.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1459032057_97e73ed6ab.jpg: 480x640 7 persons, 1 bottle, 1 sandwich, 1 donut, 1 chair, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1459032057_97e73ed6ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2926786902_815a99a154.jpg: 448x640 1 person, 3 skiss, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2926786902_815a99a154.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3536561454_e75993d903.jpg: 640x448 3 persons, 1 skateboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3536561454_e75993d903.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174196837_800689a2f3.jpg: 448x640 3 dogs, 1 bear, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3174196837_800689a2f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3552796830_2dd2aa9c2c.jpg: 416x640 2 persons, 3 cars, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3552796830_2dd2aa9c2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1045521051_108ebc19be.jpg: 480x640 1 person, 1 tv, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1045521051_108ebc19be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/501320769_31eea7b7ea.jpg: 480x640 1 person, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 501320769_31eea7b7ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2049051050_20359a434a.jpg: 640x480 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2049051050_20359a434a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3448855727_f16dea7b03.jpg: 640x480 3 persons, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3448855727_f16dea7b03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2710563762_06d48329d7.jpg: 640x480 9 persons, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2710563762_06d48329d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2363419943_717e6b119d.jpg: 640x448 5 persons, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2363419943_717e6b119d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208188198_2b271d2a2e.jpg: 448x640 1 person, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3208188198_2b271d2a2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2833431496_09d999db4d.jpg: 640x448 1 person, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2833431496_09d999db4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2432709509_2a4d0c833f.jpg: 640x448 1 person, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2432709509_2a4d0c833f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2557972410_6925fe695c.jpg: 448x640 3 persons, 2 baseball gloves, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2557972410_6925fe695c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/554774472_b5d165ff69.jpg: 448x640 2 persons, 2 sports balls, 2 surfboards, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 554774472_b5d165ff69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2322593776_e6aaf69e80.jpg: 480x640 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2322593776_e6aaf69e80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1419286010_b59af3962a.jpg: 640x512 2 persons, 1 motorcycle, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1419286010_b59af3962a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635905544_dbc65d0622.jpg: 640x640 1 dog, 1 frisbee, 1 sports ball, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2635905544_dbc65d0622.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/551403320_dfdcf9fc3b.jpg: 480x640 1 person, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 551403320_dfdcf9fc3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099965396_2a0018cb9e.jpg: 640x480 10 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3099965396_2a0018cb9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143991972_7193381aeb.jpg: 480x640 3 persons, 3 suitcases, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143991972_7193381aeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3205754736_32c29b5208.jpg: 640x512 12 persons, 1 bicycle, 1 backpack, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3205754736_32c29b5208.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2431723485_bc6b8e6418.jpg: 448x640 1 person, 2 bicycles, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2431723485_bc6b8e6418.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544483327_830349e7bc.jpg: 512x640 5 persons, 1 bicycle, 1 traffic light, 12.6ms\n",
      "Speed: 3.5ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3544483327_830349e7bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349194268_0ee555c9a2.jpg: 480x640 2 persons, 1 broccoli, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3349194268_0ee555c9a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2206594874_5e0087c6b7.jpg: 480x640 7 persons, 2 cups, 2 chairs, 1 dining table, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2206594874_5e0087c6b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/502884177_25939ac000.jpg: 640x480 3 persons, 1 truck, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 502884177_25939ac000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/267325341_1a96ef436e.jpg: 640x608 1 dog, 2 chairs, 9.1ms\n",
      "Speed: 2.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 267325341_1a96ef436e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3151492269_28d8edaa68.jpg: 352x640 1 bird, 2 dogs, 1 horse, 9.9ms\n",
      "Speed: 1.7ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3151492269_28d8edaa68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/673806038_0a3682a83f.jpg: 480x640 9 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 673806038_0a3682a83f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2675685200_0913d84d9b.jpg: 448x640 1 person, 2 cups, 3 chairs, 1 dining table, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2675685200_0913d84d9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/370713359_7560808550.jpg: 448x640 10 persons, 2 bottles, 2 cups, 1 bowl, 3 chairs, 1 potted plant, 3 dining tables, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 370713359_7560808550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192131110_8a40e7c028.jpg: 640x448 1 person, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2192131110_8a40e7c028.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/843616798_5ab0ecb525.jpg: 512x640 1 dog, 7 sheeps, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 843616798_5ab0ecb525.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690159129_93ba49ea18.jpg: 640x448 8 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3690159129_93ba49ea18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3307077951_dd31f1971c.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3307077951_dd31f1971c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/459778335_857d8ffebf.jpg: 448x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 459778335_857d8ffebf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3334537556_a2cf4e9b9a.jpg: 480x640 5 persons, 2 dogs, 1 skis, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3334537556_a2cf4e9b9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1579798212_d30844b4c5.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1579798212_d30844b4c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2247192427_5e106f24a9.jpg: 640x448 11 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2247192427_5e106f24a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2261257940_449b6e6c91.jpg: 448x640 2 dogs, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2261257940_449b6e6c91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613027188_1645ca1976.jpg: 480x640 6 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3613027188_1645ca1976.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561314880_ea9a7e245f.jpg: 640x480 1 person, 1 suitcase, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3561314880_ea9a7e245f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208987435_780ae35ef0.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3208987435_780ae35ef0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3268175963_113d90d178.jpg: 448x640 2 persons, 2 tennis rackets, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3268175963_113d90d178.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2944362789_aebbc22db4.jpg: 448x640 2 persons, 1 bench, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2944362789_aebbc22db4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1439046601_cf110a75a7.jpg: 480x640 9 persons, 1 umbrella, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1439046601_cf110a75a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477778668_81ff0a68e0.jpg: 480x640 16 persons, 5 ties, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3477778668_81ff0a68e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3270691950_88583c3524.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3270691950_88583c3524.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3123463486_f5b36a3624.jpg: 448x640 1 dog, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3123463486_f5b36a3624.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088922727_4bdf2ccc94.jpg: 544x640 4 persons, 1 handbag, 1 chair, 1 dining table, 1 tv, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3088922727_4bdf2ccc94.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2312731013_1a3a8e25c6.jpg: 640x512 5 persons, 2 dogs, 1 cup, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2312731013_1a3a8e25c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537559285_29be110134.jpg: 480x640 2 persons, 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 537559285_29be110134.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427540832_c882fded1d.jpg: 640x640 8 persons, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3427540832_c882fded1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1131804997_177c3c0640.jpg: 448x640 2 persons, 2 backpacks, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1131804997_177c3c0640.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/404216567_75b50b5a36.jpg: 448x640 1 person, 1 traffic light, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 404216567_75b50b5a36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583903436_028b06c489.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3583903436_028b06c489.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3165826902_6bf9c4bdb2.jpg: 640x448 4 persons, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3165826902_6bf9c4bdb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3411393875_a9ff73c67a.jpg: 640x544 2 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3411393875_a9ff73c67a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/78984436_ad96eaa802.jpg: 480x640 2 dogs, 1 frisbee, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 78984436_ad96eaa802.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3659686168_49c3abcee1.jpg: 640x448 3 persons, 1 handbag, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3659686168_49c3abcee1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1925434818_2949a8f6d8.jpg: 448x640 2 persons, 2 apples, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1925434818_2949a8f6d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3053743109_a2d780c0d2.jpg: 640x448 7 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3053743109_a2d780c0d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2149968397_a7411729d1.jpg: 640x480 7 persons, 1 boat, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2149968397_a7411729d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3305767464_d64a336f60.jpg: 640x448 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3305767464_d64a336f60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2896483502_6f807bae9e.jpg: 480x640 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2896483502_6f807bae9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155390408_8e1a81efb2.jpg: 640x480 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3155390408_8e1a81efb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345864_138471c9ea.jpg: 640x448 11 persons, 1 backpack, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241345864_138471c9ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/210686241_b8e069fff3.jpg: 640x480 3 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 210686241_b8e069fff3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159641529_c2c74f3eaf.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3159641529_c2c74f3eaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/747242766_afdc9cb2ba.jpg: 608x640 1 dog, 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 747242766_afdc9cb2ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070011270_390e597783.jpg: 448x640 1 dog, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3070011270_390e597783.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2929006980_9f9f8f3d21.jpg: 608x640 1 person, 1 sports ball, 1 skateboard, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2929006980_9f9f8f3d21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2607462776_78e639d891.jpg: 480x640 2 benchs, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2607462776_78e639d891.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2266142543_b2de18c081.jpg: 640x384 1 person, 8.4ms\n",
      "Speed: 1.5ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2266142543_b2de18c081.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2641288004_30ce961211.jpg: 608x640 1 person, 2 dogs, 1 bed, 2 books, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2641288004_30ce961211.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2432038587_5e4148e277.jpg: 480x640 2 birds, 1 cat, 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2432038587_5e4148e277.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3614542901_29877fc342.jpg: 608x640 5 persons, 1 sports ball, 1 cell phone, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3614542901_29877fc342.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1980882959_9a161f3469.jpg: 480x640 5 persons, 3 bicycles, 3 cars, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1980882959_9a161f3469.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1468429623_f001988691.jpg: 480x640 1 person, 1 elephant, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1468429623_f001988691.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143688895_e837c3bc76.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 143688895_e837c3bc76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3368569524_a9df2fc312.jpg: 640x448 8 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3368569524_a9df2fc312.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401902253_cd27e6d0fe.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3401902253_cd27e6d0fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046190891_05c6ecd9b6.jpg: 448x640 1 person, 1 surfboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3046190891_05c6ecd9b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2450299735_62c095f40e.jpg: 640x480 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2450299735_62c095f40e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3661239105_973f8216c4.jpg: 640x448 1 person, 2 baseball bats, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3661239105_973f8216c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171035252_dba286ae5c.jpg: 640x480 1 kite, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3171035252_dba286ae5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3593220756_5c416c3ceb.jpg: 544x640 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3593220756_5c416c3ceb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070031806_3d587c2a66.jpg: 640x448 1 person, 5 dogs, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3070031806_3d587c2a66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2973269132_252bfd0160.jpg: 448x640 1 dog, 1 cow, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2973269132_252bfd0160.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482907079_22085ada04.jpg: 448x640 3 persons, 1 cow, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 482907079_22085ada04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3574627719_790325430e.jpg: 640x480 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3574627719_790325430e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3593556797_46b49a02a8.jpg: 640x480 6 persons, 1 bicycle, 3 cars, 1 truck, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3593556797_46b49a02a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2321466753_5606a10721.jpg: 544x640 2 persons, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2321466753_5606a10721.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421177332_a05741cfa4.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421177332_a05741cfa4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1809796012_a2dac6c26b.jpg: 640x640 1 cat, 1 dog, 1 couch, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1809796012_a2dac6c26b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934022873_3fdd69aee4.jpg: 448x640 7 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2934022873_3fdd69aee4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/527946505_a51ade1578.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 527946505_a51ade1578.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2579268572_d78f8436cb.jpg: 448x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2579268572_d78f8436cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431860810_44277cd360.jpg: 512x640 1 dog, 1 sheep, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3431860810_44277cd360.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892989340_bb7e0e5548.jpg: 640x448 1 person, 2 surfboards, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2892989340_bb7e0e5548.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3314900697_c5c5ae9af6.jpg: 640x448 6 persons, 1 cow, 1 elephant, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3314900697_c5c5ae9af6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107368071_724613fc4f.jpg: 416x640 17 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3107368071_724613fc4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3709030554_02301229ea.jpg: 512x640 1 person, 1 chair, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3709030554_02301229ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1252396628_eb81d3905b.jpg: 640x448 2 persons, 1 motorcycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1252396628_eb81d3905b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3416091866_a96003d652.jpg: 416x640 5 persons, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3416091866_a96003d652.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1472230829_803818a383.jpg: 480x640 4 persons, 2 surfboards, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1472230829_803818a383.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2769605231_dae8b30201.jpg: 448x640 1 bird, 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2769605231_dae8b30201.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380590140_25b9889772.jpg: 448x640 3 persons, 2 bicycles, 13 cars, 1 bus, 1 traffic light, 1 handbag, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 380590140_25b9889772.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2830869109_c4e403eae6.jpg: 608x640 2 persons, 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2830869109_c4e403eae6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3382679230_baef3d1eaa.jpg: 480x640 4 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3382679230_baef3d1eaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2976155358_b4dd4407cf.jpg: 640x576 1 person, 1 sports ball, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2976155358_b4dd4407cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/252802010_3d47bee500.jpg: 448x640 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 252802010_3d47bee500.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443030942_f409586258.jpg: 384x640 1 person, 1 skis, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3443030942_f409586258.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444881000_bba92e585c.jpg: 448x640 11 persons, 1 handbag, 1 tie, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 444881000_bba92e585c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523471597_87e0bf3b21.jpg: 448x640 2 persons, 1 car, 2 buss, 1 backpack, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3523471597_87e0bf3b21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601569729_bf4bf82768.jpg: 448x640 4 persons, 4 horses, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3601569729_bf4bf82768.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541962817_78bcd3835b.jpg: 640x608 1 person, 1 horse, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3541962817_78bcd3835b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/279230262_e541f9b670.jpg: 640x448 2 persons, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 279230262_e541f9b670.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2504764590_cf017c2a6e.jpg: 480x640 1 person, 1 bird, 1 dog, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2504764590_cf017c2a6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589895574_ee08207d26.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3589895574_ee08207d26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2264316030_600e55748d.jpg: 640x480 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2264316030_600e55748d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3097196395_ec06075389.jpg: 448x640 2 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3097196395_ec06075389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1387461595_2fe6925f73.jpg: 352x640 3 persons, 8 cars, 2 backpacks, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 1387461595_2fe6925f73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/664341930_bd5c8b60ea.jpg: 448x640 1 person, 1 couch, 1 bed, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 664341930_bd5c8b60ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323498985_fd9d2803fd.jpg: 640x512 1 person, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3323498985_fd9d2803fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3079786914_fe598b0e54.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3079786914_fe598b0e54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/760180310_3c6bd4fd1f.jpg: 512x640 1 person, 1 dog, 1 bed, 8.5ms\n",
      "Speed: 3.2ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 760180310_3c6bd4fd1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/541046380_513e295384.jpg: 640x512 5 persons, 1 apple, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 541046380_513e295384.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855667597_bf6ceaef8e.jpg: 448x640 4 dogs, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2855667597_bf6ceaef8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2531531628_b4a5041680.jpg: 640x448 1 person, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2531531628_b4a5041680.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259757648_71edb4347b.jpg: 480x640 1 dog, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3259757648_71edb4347b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638688673_176f99d7fd.jpg: 640x480 1 person, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3638688673_176f99d7fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2252299132_14ca6e584b.jpg: 512x640 6 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2252299132_14ca6e584b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2706766641_a9df81969d.jpg: 640x448 1 person, 1 chair, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2706766641_a9df81969d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831723637_30d4456665.jpg: 640x576 3 dogs, 8.8ms\n",
      "Speed: 2.9ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2831723637_30d4456665.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3186527735_6e9fe2cf88.jpg: 640x640 5 persons, 1 car, 1 traffic light, 1 suitcase, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3186527735_6e9fe2cf88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540750172_070250ece5.jpg: 480x640 9 persons, 2 boats, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2540750172_070250ece5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/433810429_a4da0eac50.jpg: 640x448 3 persons, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 433810429_a4da0eac50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1812525037_528465037c.jpg: 448x640 4 persons, 1 teddy bear, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1812525037_528465037c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/318070878_92ead85868.jpg: 640x640 2 persons, 1 tie, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 318070878_92ead85868.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1107471216_4336c9b328.jpg: 480x640 1 person, 1 cell phone, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1107471216_4336c9b328.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/290982269_79fc9f36dc.jpg: 480x640 1 person, 1 backpack, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 290982269_79fc9f36dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1262454669_f1caafec2d.jpg: 448x640 6 persons, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1262454669_f1caafec2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/393987665_91d28f0ed0.jpg: 576x640 4 persons, 1 sink, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 393987665_91d28f0ed0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3261493263_381a4c5cc7.jpg: 640x608 10 persons, 3 dogs, 1 cow, 1 baseball glove, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3261493263_381a4c5cc7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/215876547_fa584c5ec3.jpg: 640x448 1 person, 1 boat, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 215876547_fa584c5ec3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3445428367_25bafffe75.jpg: 448x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3445428367_25bafffe75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349309109_4024a09a17.jpg: 480x640 3 persons, 1 horse, 1 cow, 1 elephant, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3349309109_4024a09a17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/531152619_6db02a7ed9.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 531152619_6db02a7ed9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/862187579_9faf4a51e0.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 862187579_9faf4a51e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2073756099_7e02c0110c.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2073756099_7e02c0110c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326024473_4c16e4fbfc.jpg: 448x640 1 car, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3326024473_4c16e4fbfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3337332770_5eda5cceb7.jpg: 448x640 3 dogs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3337332770_5eda5cceb7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2082005167_a0d6a70020.jpg: 480x640 1 dog, 13.8ms\n",
      "Speed: 4.0ms preprocess, 13.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2082005167_a0d6a70020.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/412082368_371df946b3.jpg: 448x640 6 persons, 1 car, 2 kites, 1 baseball bat, 10.8ms\n",
      "Speed: 2.7ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 412082368_371df946b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3177298173_78cea31d64.jpg: 640x480 1 person, 1 sports ball, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3177298173_78cea31d64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2914737181_0c8e052da8.jpg: 256x640 2 persons, 2 surfboards, 12.6ms\n",
      "Speed: 1.6ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2914737181_0c8e052da8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1303727066_23d0f6ed43.jpg: 448x640 9 persons, 1 umbrella, 1 handbag, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1303727066_23d0f6ed43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163068926_d28ed3ef53.jpg: 480x640 1 person, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3163068926_d28ed3ef53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247617754_4b1137de8c.jpg: 448x640 3 persons, 1 bench, 1 backpack, 1 chair, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 247617754_4b1137de8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2609847254_0ec40c1cce.jpg: 480x640 15 persons, 1 horse, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2609847254_0ec40c1cce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636126441_5617c89aaa.jpg: 640x448 1 person, 1 bench, 1 skateboard, 13.7ms\n",
      "Speed: 3.7ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3636126441_5617c89aaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1131932671_c8d17751b3.jpg: 640x448 1 person, 1 bed, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1131932671_c8d17751b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2771424045_1fdf9617eb.jpg: 448x640 2 persons, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2771424045_1fdf9617eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3572144280_ea42bbd927.jpg: 640x448 1 person, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3572144280_ea42bbd927.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2567812221_30fb64f5e9.jpg: 448x640 5 persons, 3 dogs, 1 chair, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2567812221_30fb64f5e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1370773415_967b1ffde1.jpg: 448x640 2 persons, 2 cars, 1 clock, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1370773415_967b1ffde1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/963730324_0638534227.jpg: 480x640 2 persons, 2 backpacks, 1 suitcase, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 963730324_0638534227.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014986976_0e7b858970.jpg: 448x640 15 persons, 4 cars, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3014986976_0e7b858970.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2780087302_6a77658cbf.jpg: 480x640 3 persons, 1 motorcycle, 1 teddy bear, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2780087302_6a77658cbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/115684808_cb01227802.jpg: 480x640 2 persons, 4 backpacks, 1 snowboard, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 115684808_cb01227802.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3002448718_a478c64fb4.jpg: 480x640 7 persons, 2 handbags, 1 remote, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3002448718_a478c64fb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934573544_7ffe92a2c9.jpg: 480x640 1 bench, 1 dog, 1 horse, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2934573544_7ffe92a2c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421789737_f625dd17ed.jpg: 448x640 3 persons, 1 skateboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421789737_f625dd17ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182161610_4d349b257f.jpg: 480x640 2 persons, 3 chairs, 1 couch, 12.9ms\n",
      "Speed: 3.5ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3182161610_4d349b257f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3687996569_99163a41c3.jpg: 448x640 9 persons, 1 umbrella, 1 handbag, 2 teddy bears, 10.7ms\n",
      "Speed: 2.7ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3687996569_99163a41c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3333826465_9c84c1b3c6.jpg: 544x640 4 persons, 1 sports ball, 10.0ms\n",
      "Speed: 2.7ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3333826465_9c84c1b3c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2302747917_aa0300eb68.jpg: 480x640 3 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2302747917_aa0300eb68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860667542_95abec3380.jpg: 544x640 1 bird, 9.1ms\n",
      "Speed: 2.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2860667542_95abec3380.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3260214530_7179346407.jpg: 512x640 3 persons, 1 bench, 14.7ms\n",
      "Speed: 2.3ms preprocess, 14.7ms inference, 2.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3260214530_7179346407.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3079917032_3cfacb2fd7.jpg: 480x640 3 persons, 1 car, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3079917032_3cfacb2fd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/357191373_a1cb5696e8.jpg: 352x640 5 persons, 5 horses, 12.5ms\n",
      "Speed: 2.0ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 357191373_a1cb5696e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2417745327_a2c2705043.jpg: 448x640 1 dog, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2417745327_a2c2705043.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3350671534_2a5d45a961.jpg: 448x640 1 person, 1 skateboard, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3350671534_2a5d45a961.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2501968935_02f2cd8079.jpg: 640x480 4 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2501968935_02f2cd8079.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2398605966_1d0c9e6a20.jpg: 448x640 2 dogs, 13.9ms\n",
      "Speed: 3.5ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2398605966_1d0c9e6a20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397547349_1fd14b95af.jpg: 640x608 4 persons, 2 cars, 2 dogs, 11.8ms\n",
      "Speed: 3.5ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 397547349_1fd14b95af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2936590102_25036069a6.jpg: 448x640 1 person, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2936590102_25036069a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752043092_f48ebfeaa2.jpg: 480x640 1 person, 1 couch, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2752043092_f48ebfeaa2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2079554580_f18d5c181b.jpg: 480x640 2 birds, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2079554580_f18d5c181b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1204996216_71d7519d9a.jpg: 640x448 1 person, 4 benchs, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1204996216_71d7519d9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3452411712_5b42d2a1b5.jpg: 640x480 2 persons, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3452411712_5b42d2a1b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3550459890_161f436c8d.jpg: 640x448 1 person, 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3550459890_161f436c8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2673148534_8daf0de833.jpg: 448x640 1 cow, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2673148534_8daf0de833.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2688902319_52ceaf4a2a.jpg: 480x640 1 person, 3 cars, 1 handbag, 3 potted plants, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2688902319_52ceaf4a2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236474697_0c73dd5d8b.jpg: 448x640 8 persons, 2 bottles, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 236474697_0c73dd5d8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1432179046_8e3d75cf81.jpg: 480x640 2 persons, 2 cars, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1432179046_8e3d75cf81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393971707_bce01ae754.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2393971707_bce01ae754.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/518251319_40e031e818.jpg: 640x448 3 persons, 1 handbag, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 518251319_40e031e818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3692836015_d11180727b.jpg: 352x640 7 persons, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3692836015_d11180727b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2638981862_6b23833f37.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2638981862_6b23833f37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/640409060_6af18fdd54.jpg: 448x640 1 person, 4 cars, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 640409060_6af18fdd54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374176648_ba4b88c221.jpg: 640x480 2 zebras, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 374176648_ba4b88c221.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3288173388_03bc2a844d.jpg: 448x640 2 persons, 1 tv, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3288173388_03bc2a844d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2299427360_422a3fb8b0.jpg: 480x640 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2299427360_422a3fb8b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259579174_30a8a27058.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3259579174_30a8a27058.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2538477523_1da77eb11c.jpg: 640x448 1 person, 2 dogs, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2538477523_1da77eb11c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/197142902_f05ff198c2.jpg: 480x640 2 persons, 1 bottle, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 197142902_f05ff198c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512101751_05a6d93e19.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512101751_05a6d93e19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1402640441_81978e32a9.jpg: 448x640 1 person, 1 sports ball, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1402640441_81978e32a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/36422830_55c844bc2d.jpg: 480x640 1 person, 1 truck, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 36422830_55c844bc2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2975073156_7543ed326f.jpg: 448x640 5 persons, 2 cars, 13.6ms\n",
      "Speed: 3.8ms preprocess, 13.6ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2975073156_7543ed326f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3721082512_8277087f3f.jpg: 640x448 1 person, 1 bicycle, 10.9ms\n",
      "Speed: 2.5ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3721082512_8277087f3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441399292_60c83bd5db.jpg: 448x640 2 persons, 3 dogs, 10.1ms\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3441399292_60c83bd5db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1473250020_dc829a090f.jpg: 480x640 1 person, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1473250020_dc829a090f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134385454_4f1d55333f.jpg: 448x640 2 persons, 1 surfboard, 14.3ms\n",
      "Speed: 3.7ms preprocess, 14.3ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3134385454_4f1d55333f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/479807115_3a484fb18b.jpg: 640x448 1 person, 1 bicycle, 12.6ms\n",
      "Speed: 3.0ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 479807115_3a484fb18b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3409326324_a704565e8f.jpg: 448x640 13 birds, 10.8ms\n",
      "Speed: 2.4ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3409326324_a704565e8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2068960566_21e85ae0dc.jpg: 480x640 2 persons, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2068960566_21e85ae0dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3700346840_bb80d622f7.jpg: 448x640 1 bird, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3700346840_bb80d622f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116769029_f5a76f04ba.jpg: 448x640 1 person, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3116769029_f5a76f04ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724759125_2dc0e1f4a3.jpg: 448x640 1 person, 1 chair, 10.3ms\n",
      "Speed: 2.5ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3724759125_2dc0e1f4a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1142847777_2a0c1c2551.jpg: 640x448 1 person, 1 bicycle, 4 cars, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1142847777_2a0c1c2551.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109823397_e35154645f.jpg: 448x640 1 person, 1 motorcycle, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 109823397_e35154645f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2865564810_5c63328cd4.jpg: 416x640 7 persons, 10.3ms\n",
      "Speed: 2.0ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2865564810_5c63328cd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590653633_495de5f288.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3590653633_495de5f288.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429641260_2f035c1813.jpg: 448x640 2 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3429641260_2f035c1813.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/820169182_f5e78d7d19.jpg: 448x640 3 persons, 1 sports ball, 1 bed, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 820169182_f5e78d7d19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2508918369_2659db1cb6.jpg: 448x640 2 persons, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2508918369_2659db1cb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2447284966_d6bbdb4b6e.jpg: 448x640 7 persons, 1 backpack, 17 chairs, 11.7ms\n",
      "Speed: 3.3ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2447284966_d6bbdb4b6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/561179884_8b6b925ef9.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 561179884_8b6b925ef9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3543294190_0037c59607.jpg: 640x448 4 persons, 4 bicycles, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3543294190_0037c59607.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3494394662_3edfd4a34c.jpg: 448x640 3 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3494394662_3edfd4a34c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2312746782_4528a5b818.jpg: 480x640 2 persons, 2 benchs, 1 baseball bat, 2 chairs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2312746782_4528a5b818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2385744837_8780f6731a.jpg: 640x480 1 person, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2385744837_8780f6731a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628698119_5566769777.jpg: 448x640 1 dog, 2 potted plants, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3628698119_5566769777.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/522063319_33827f1627.jpg: 448x640 2 persons, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 522063319_33827f1627.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256043809_47258e0b3e.jpg: 448x640 1 dog, 10.1ms\n",
      "Speed: 2.6ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3256043809_47258e0b3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457045393_2bbbb4e941.jpg: 512x640 2 persons, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3457045393_2bbbb4e941.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3394654132_9a8659605c.jpg: 448x640 1 person, 2 cars, 1 parking meter, 2 dogs, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3394654132_9a8659605c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/398662202_97e5819b79.jpg: 480x640 4 persons, 1 sports ball, 1 baseball bat, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 398662202_97e5819b79.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2651915425_7a58e862e9.jpg: 640x480 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2651915425_7a58e862e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/47871819_db55ac4699.jpg: 416x640 4 persons, 1 sports ball, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 47871819_db55ac4699.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2959500257_3621429a37.jpg: 448x640 1 person, 1 horse, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2959500257_3621429a37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2268109835_d6edbe1c2b.jpg: 480x640 1 kite, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2268109835_d6edbe1c2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3607489370_92683861f7.jpg: 512x640 1 person, 1 bicycle, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3607489370_92683861f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1778020185_1d44c04dae.jpg: 512x640 3 dogs, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1778020185_1d44c04dae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472449219_eb927f05b8.jpg: 640x448 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3472449219_eb927f05b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2992999413_018f48aabc.jpg: 480x640 10 persons, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2992999413_018f48aabc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3209966887_5b744bd050.jpg: 640x480 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3209966887_5b744bd050.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2698666984_13e17236ae.jpg: 640x448 1 person, 1 car, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2698666984_13e17236ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2636876892_9353521a1c.jpg: 448x640 1 dog, 1 sheep, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2636876892_9353521a1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1579198375_84b18e003a.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1579198375_84b18e003a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3009383694_e045c6169e.jpg: 640x640 1 person, 2 airplanes, 1 traffic light, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3009383694_e045c6169e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229913073_e7857a5966.jpg: 480x640 1 person, 1 baseball glove, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3229913073_e7857a5966.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477977145_4df89d69a1.jpg: 448x640 1 person, 1 dog, 1 horse, 1 sports ball, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3477977145_4df89d69a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2704362232_7d84503433.jpg: 480x640 1 person, 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2704362232_7d84503433.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345905_5826a72da1.jpg: 448x640 7 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345905_5826a72da1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271252073_0a1b9525fc.jpg: 480x640 3 persons, 1 tie, 2 wine glasss, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3271252073_0a1b9525fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265209567_b3b9c8e0fe.jpg: 448x640 1 dog, 1 cow, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3265209567_b3b9c8e0fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1247181182_35cabd76f3.jpg: 480x640 1 person, 1 backpack, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1247181182_35cabd76f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1655781989_b15ab4cbff.jpg: 480x640 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1655781989_b15ab4cbff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/487074671_66db20bf47.jpg: 640x448 1 person, 1 suitcase, 13.3ms\n",
      "Speed: 3.3ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 487074671_66db20bf47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2804851816_9aae9071ca.jpg: 480x640 1 dog, 10.9ms\n",
      "Speed: 2.9ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2804851816_9aae9071ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470373679_98dceb19e7.jpg: 480x640 6 persons, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 470373679_98dceb19e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1298866571_b4c496b71c.jpg: 640x512 2 persons, 1 tie, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1298866571_b4c496b71c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2816259113_461f8dedb0.jpg: 448x640 2 persons, 2 dogs, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2816259113_461f8dedb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/251056963_c8b67f0107.jpg: 640x448 1 bench, 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 251056963_c8b67f0107.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666324102_18ecdf8253.jpg: 640x640 2 persons, 1 handbag, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3666324102_18ecdf8253.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/848180689_d67a1361ce.jpg: 480x640 2 persons, 1 car, 1 chair, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 848180689_d67a1361ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2355763034_9fb61a8165.jpg: 640x512 8 persons, 11.6ms\n",
      "Speed: 3.1ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2355763034_9fb61a8165.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1164131282_b30926f332.jpg: 544x640 (no detections), 11.1ms\n",
      "Speed: 3.1ms preprocess, 11.1ms inference, 0.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1164131282_b30926f332.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2650485780_29d89268d7.jpg: 448x640 6 persons, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2650485780_29d89268d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503624011_733d745d5a.jpg: 480x640 3 persons, 1 umbrella, 1 bottle, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3503624011_733d745d5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3383388869_a14552e551.jpg: 448x640 1 bear, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3383388869_a14552e551.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3178300150_d4605ff02c.jpg: 640x448 13 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3178300150_d4605ff02c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2630806789_6835bbae95.jpg: 480x640 1 person, 4 boats, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2630806789_6835bbae95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405759441_fb31c80240.jpg: 640x480 2 persons, 1 horse, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3405759441_fb31c80240.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1713248047_d03721456d.jpg: 416x640 2 dogs, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1713248047_d03721456d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/973827791_467d83986e.jpg: 640x448 1 person, 2 cars, 1 boat, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 973827791_467d83986e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3007214949_a4b027f8a3.jpg: 480x640 2 persons, 1 skateboard, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3007214949_a4b027f8a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1510669311_75330b4781.jpg: 448x640 10 persons, 1 bottle, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1510669311_75330b4781.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3550276904_19de3561c5.jpg: 576x640 2 persons, 1 couch, 8.9ms\n",
      "Speed: 2.6ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3550276904_19de3561c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3195701071_81879257f5.jpg: 640x448 1 dog, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3195701071_81879257f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2187904131_96ea83b9b5.jpg: 416x640 6 persons, 1 skis, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2187904131_96ea83b9b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2894008505_a445ccaaff.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2894008505_a445ccaaff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143684568_3c59299bae.jpg: 640x480 1 person, 1 cow, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 143684568_3c59299bae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293751640_d81a6f3a0c.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3293751640_d81a6f3a0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420323191_d66e003264.jpg: 544x640 1 person, 1 boat, 1 surfboard, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3420323191_d66e003264.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/346253487_687150ab04.jpg: 640x480 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 346253487_687150ab04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1384292980_4022a7520c.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1384292980_4022a7520c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3308018795_68a97a425c.jpg: 448x640 13 persons, 9 bicycles, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3308018795_68a97a425c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3141440149_00becbbb93.jpg: 480x640 2 persons, 1 chair, 2 potted plants, 1 sink, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3141440149_00becbbb93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3717531382_e1e05e22c5.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3717531382_e1e05e22c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2564888404_b57f89d3c7.jpg: 448x640 2 persons, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2564888404_b57f89d3c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516285214_59823b341e.jpg: 352x640 1 person, 1 surfboard, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3516285214_59823b341e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247097023_e656d5854d.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 247097023_e656d5854d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421129418_088af794f7.jpg: 416x640 1 person, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3421129418_088af794f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2765029348_667111fc30.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2765029348_667111fc30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223302125_f8154417f4.jpg: 448x640 5 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3223302125_f8154417f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437107047_715c60e9c8.jpg: 512x640 5 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3437107047_715c60e9c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501083764_cf592292a6.jpg: 448x640 8 persons, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3501083764_cf592292a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1229756013_94663527d7.jpg: 480x640 1 dog, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1229756013_94663527d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2068403258_2669cf9763.jpg: 448x640 7 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2068403258_2669cf9763.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831656774_36982aafdb.jpg: 640x544 1 person, 1 bench, 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2831656774_36982aafdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924483864_cfdb900a13.jpg: 448x640 1 kite, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2924483864_cfdb900a13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2600170955_bf30c5d5c0.jpg: 640x480 2 persons, 5 cars, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2600170955_bf30c5d5c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2965604928_435dc93bf7.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2965604928_435dc93bf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/440184957_267f3f3a2b.jpg: 480x640 2 persons, 1 bird, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 440184957_267f3f3a2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2172526745_649f420569.jpg: 448x640 3 persons, 1 boat, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2172526745_649f420569.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2220185725_45d4fa68d9.jpg: 608x640 1 person, 1 dog, 1 horse, 1 elephant, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2220185725_45d4fa68d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2714220101_c31db50b10.jpg: 416x640 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2714220101_c31db50b10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1425919702_ddb761aeec.jpg: 448x640 8 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1425919702_ddb761aeec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681172959_6674c118d2.jpg: 640x448 6 persons, 1 tie, 1 bottle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3681172959_6674c118d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219730733_6a55382dd2.jpg: 640x512 9 persons, 1 traffic light, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 219730733_6a55382dd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/239807547_4923efc821.jpg: 480x640 (no detections), 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 239807547_4923efc821.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2046778775_0dd7cac6ab.jpg: 448x640 18 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2046778775_0dd7cac6ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2864634088_d087494dff.jpg: 416x640 9 persons, 16 bicycles, 1 motorcycle, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2864634088_d087494dff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3580375310_46ec3e476c.jpg: 448x640 1 person, 1 sports ball, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3580375310_46ec3e476c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264678536_46601d25f0.jpg: 320x640 2 dogs, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3264678536_46601d25f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1511807116_41c3645e8c.jpg: 640x512 4 bears, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1511807116_41c3645e8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1801063894_60bce29e19.jpg: 640x448 1 person, 1 bird, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1801063894_60bce29e19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/406248253_27b5eba25a.jpg: 640x480 2 persons, 1 surfboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 406248253_27b5eba25a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287982439_6578006e22.jpg: 480x640 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1287982439_6578006e22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533470072_87a5b595ba.jpg: 576x640 2 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3533470072_87a5b595ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351418768_596ef7fd6f.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3351418768_596ef7fd6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3243233886_235a80e8c7.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3243233886_235a80e8c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271890493_da441718ba.jpg: 448x640 2 dogs, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2271890493_da441718ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3624076529_9793655a21.jpg: 448x640 1 bird, 1 dog, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3624076529_9793655a21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264937930_9623496b64.jpg: 640x448 6 persons, 2 umbrellas, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3264937930_9623496b64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2654943319_d17fee7800.jpg: 416x640 2 persons, 1 skateboard, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2654943319_d17fee7800.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2804374083_311f98f5f2.jpg: 640x640 1 person, 3 surfboards, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2804374083_311f98f5f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3152317129_177b4678b7.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3152317129_177b4678b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597715122_45878432ec.jpg: 448x640 4 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3597715122_45878432ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631136463_53ff624b82.jpg: 448x640 1 dog, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3631136463_53ff624b82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/50030244_02cd4de372.jpg: 480x640 1 cat, 2 sheeps, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 50030244_02cd4de372.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432730942_4dc4685277.jpg: 448x640 3 persons, 2 motorcycles, 1 backpack, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3432730942_4dc4685277.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244867897_d00369a779.jpg: 480x640 2 dogs, 1 chair, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244867897_d00369a779.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3248752274_96740ed073.jpg: 448x640 5 persons, 1 boat, 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3248752274_96740ed073.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3045613316_4e88862836.jpg: 480x640 3 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3045613316_4e88862836.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539744890_85e63f5854.jpg: 640x256 2 persons, 1 bicycle, 54.4ms\n",
      "Speed: 1.0ms preprocess, 54.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 256)\n",
      "Cropped images saved for 539744890_85e63f5854.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2116444946_1f5d1fe5d1.jpg: 512x640 1 person, 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2116444946_1f5d1fe5d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/135235570_5698072cd4.jpg: 480x640 11 persons, 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 135235570_5698072cd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3464871350_3f2d624a9c.jpg: 448x640 1 airplane, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3464871350_3f2d624a9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/453473508_682c0a7189.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 453473508_682c0a7189.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2325386353_1f1a05e1ce.jpg: 448x640 5 persons, 1 handbag, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2325386353_1f1a05e1ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300765528_8c8f709dda.jpg: 640x448 11 persons, 1 bicycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 300765528_8c8f709dda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211210739_3dea005fde.jpg: 640x448 10 persons, 1 bus, 1 skateboard, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3211210739_3dea005fde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1525153022_06c48dbe52.jpg: 448x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1525153022_06c48dbe52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2373234213_4ebe9c4ee5.jpg: 640x544 2 persons, 7.1ms\n",
      "Speed: 2.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2373234213_4ebe9c4ee5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3149919755_f9272b10b3.jpg: 480x640 1 dog, 1 horse, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3149919755_f9272b10b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/189740668_0b045f1ff2.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 189740668_0b045f1ff2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564312955_716e86c48b.jpg: 384x640 1 person, 1 bird, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3564312955_716e86c48b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1682079482_9a72fa57fa.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1682079482_9a72fa57fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2528489543_546c1ca81f.jpg: 448x640 2 persons, 1 airplane, 1 dog, 1 umbrella, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2528489543_546c1ca81f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319938879_daf0857f91.jpg: 640x448 1 person, 1 skateboard, 2 chairs, 1 potted plant, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 319938879_daf0857f91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2863027424_9c75105660.jpg: 448x640 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2863027424_9c75105660.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590647207_2d4ec3f52c.jpg: 448x640 12 persons, 5 horses, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3590647207_2d4ec3f52c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345981_1ef4f8109c.jpg: 448x640 6 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345981_1ef4f8109c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106857210_07a92577fc.jpg: 416x640 2 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3106857210_07a92577fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1528205014_1323aa9dfd.jpg: 640x480 12 persons, 1 traffic light, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1528205014_1323aa9dfd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458215674_2aa5e64643.jpg: 480x640 5 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3458215674_2aa5e64643.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949353587_64c54e9589.jpg: 448x640 1 dog, 1 sheep, 1 teddy bear, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2949353587_64c54e9589.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2238019823_79318d1f11.jpg: 640x512 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2238019823_79318d1f11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/343218198_1ca90e0734.jpg: 640x576 1 dog, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 343218198_1ca90e0734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346260_f50d57b517.jpg: 448x640 7 persons, 1 baseball glove, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346260_f50d57b517.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3655773435_c234e94820.jpg: 640x448 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3655773435_c234e94820.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351596152_bf283f03d1.jpg: 448x640 8 persons, 3 skateboards, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3351596152_bf283f03d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3465396606_5ba1574128.jpg: 512x640 2 persons, 1 dog, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3465396606_5ba1574128.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3406802138_ef77bbddd0.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3406802138_ef77bbddd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2618538137_44fffe10c4.jpg: 512x640 2 persons, 1 dog, 1 skateboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2618538137_44fffe10c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/751737218_b89839a311.jpg: 544x640 2 persons, 1 frisbee, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 751737218_b89839a311.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/446138054_d40c66d5f0.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 446138054_d40c66d5f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139837262_fe5ee7ccd9.jpg: 640x448 1 person, 1 snowboard, 1 baseball glove, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3139837262_fe5ee7ccd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/315880837_90db309bab.jpg: 448x640 3 dogs, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 315880837_90db309bab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/191003285_edd8d0cf58.jpg: 480x640 6 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 191003285_edd8d0cf58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591457224_88281dd04f.jpg: 448x640 9 persons, 3 cars, 1 tie, 2 chairs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3591457224_88281dd04f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/425518464_a18b87c563.jpg: 512x640 1 person, 1 bench, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 425518464_a18b87c563.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429351222_17ae744daf.jpg: 480x640 11 persons, 1 umbrella, 2 handbags, 1 chair, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3429351222_17ae744daf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460477085_088e25f857.jpg: 448x640 5 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2460477085_088e25f857.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2844641033_dab3715a99.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2844641033_dab3715a99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2103568100_5d018c495b.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2103568100_5d018c495b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3352697012_751b079bbb.jpg: 480x640 20 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3352697012_751b079bbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2268596214_ca532f5c63.jpg: 448x640 2 persons, 1 motorcycle, 1 cell phone, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2268596214_ca532f5c63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2800934095_b84a5009dd.jpg: 640x448 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2800934095_b84a5009dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/466956675_a2fb6bf901.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 466956675_a2fb6bf901.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336211088_4c294a870b.jpg: 448x640 9 persons, 2 handbags, 1 chair, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3336211088_4c294a870b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/824123145_59243e504e.jpg: 448x640 9 persons, 3 cars, 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 824123145_59243e504e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2832487464_2d30634e1e.jpg: 640x544 1 person, 1 bench, 1 dog, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2832487464_2d30634e1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/457631171_12b1aee828.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 457631171_12b1aee828.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1969573381_5ecfae4c80.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1969573381_5ecfae4c80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1580172290_e19067e0dd.jpg: 512x640 2 persons, 1 book, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1580172290_e19067e0dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1056249424_ef2a2e041c.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1056249424_ef2a2e041c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540241710_a4f49cde52.jpg: 640x448 3 persons, 1 skateboard, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3540241710_a4f49cde52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/696663662_232edd58af.jpg: 480x640 1 person, 1 bed, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 696663662_232edd58af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695085448_a11833df95.jpg: 448x640 1 person, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2695085448_a11833df95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3381788544_2c50e139dd.jpg: 448x640 6 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3381788544_2c50e139dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1394927474_0afdd82fc4.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1394927474_0afdd82fc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2402793046_3385554e81.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2402793046_3385554e81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/478209058_21e2c37c73.jpg: 448x640 1 person, 1 snowboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 478209058_21e2c37c73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/269650644_059a84ece5.jpg: 448x640 15 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 269650644_059a84ece5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1313693129_71d0b21c63.jpg: 448x640 2 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1313693129_71d0b21c63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461583471_2b8b6b4d73.jpg: 640x448 1 person, 1 snowboard, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3461583471_2b8b6b4d73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2095007523_591f255708.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2095007523_591f255708.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2964438493_413667c04a.jpg: 640x448 3 dogs, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2964438493_413667c04a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474985382_26e1560338.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3474985382_26e1560338.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2929405404_1dff5ab847.jpg: 480x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2929405404_1dff5ab847.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/381976882_0063d16d88.jpg: 640x640 1 bird, 4 dogs, 1 horse, 7.3ms\n",
      "Speed: 2.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 381976882_0063d16d88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2578289278_01516d23a0.jpg: 480x640 2 persons, 2 horses, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2578289278_01516d23a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/121800200_bef08fae5f.jpg: 480x640 (no detections), 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 121800200_bef08fae5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2330536645_2d36b516e1.jpg: 384x640 7 persons, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2330536645_2d36b516e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3573202338_f43dd22d28.jpg: 448x640 2 persons, 1 frisbee, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3573202338_f43dd22d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2975118353_5af5a5df54.jpg: 640x448 1 person, 1 snowboard, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2975118353_5af5a5df54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2074146683_7c83167aa1.jpg: 640x448 6 persons, 1 handbag, 1 cup, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2074146683_7c83167aa1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2324979199_4193ef7537.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2324979199_4193ef7537.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1355833561_9c43073eda.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1355833561_9c43073eda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349955993_a04aea97d8.jpg: 448x640 17 persons, 1 sports ball, 1 bottle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3349955993_a04aea97d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/624058168_f1c5e87e59.jpg: 576x640 1 giraffe, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 624058168_f1c5e87e59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/613900608_2e49415772.jpg: 640x608 18 persons, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 613900608_2e49415772.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1096165011_cc5eb16aa6.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1096165011_cc5eb16aa6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1338523142_57fce8229b.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1338523142_57fce8229b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3508522093_51f6d77f45.jpg: 448x640 1 person, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3508522093_51f6d77f45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287920676_d21a0b289b.jpg: 384x640 1 cat, 1 dog, 8.1ms\n",
      "Speed: 1.3ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1287920676_d21a0b289b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459156091_c1879ebe28.jpg: 448x640 4 persons, 1 dog, 1 elephant, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3459156091_c1879ebe28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/400562847_e15aba0aac.jpg: 480x640 1 dog, 1 cow, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 400562847_e15aba0aac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752175795_c9def67895.jpg: 480x640 4 persons, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2752175795_c9def67895.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3721812313_6000566803.jpg: 480x640 5 persons, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3721812313_6000566803.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3603870481_1ebc696d91.jpg: 640x448 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3603870481_1ebc696d91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/339658315_fbb178c252.jpg: 640x416 2 persons, 2 chairs, 7.4ms\n",
      "Speed: 1.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 339658315_fbb178c252.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3061481868_d1e00b1f2e.jpg: 448x640 2 persons, 1 motorcycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3061481868_d1e00b1f2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143953179_1c08c023a5.jpg: 480x640 15 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143953179_1c08c023a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3285298241_9b1ed98d19.jpg: 640x448 4 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3285298241_9b1ed98d19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3303797949_339bb969ba.jpg: 640x448 1 person, 1 sports ball, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3303797949_339bb969ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/503717911_fc43cb3cf9.jpg: 448x640 1 dog, 1 frisbee, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 503717911_fc43cb3cf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518443604_6da641f07d.jpg: 448x640 1 person, 1 cell phone, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3518443604_6da641f07d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602279427_191773c9e2.jpg: 448x640 6 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602279427_191773c9e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613264553_97b687f172.jpg: 416x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3613264553_97b687f172.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2587846523_b177c9a3e3.jpg: 640x448 1 person, 1 horse, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2587846523_b177c9a3e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/310728631_155c3bbeea.jpg: 640x544 2 persons, 9 birds, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 310728631_155c3bbeea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2841449931_84a05850ec.jpg: 640x544 1 person, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2841449931_84a05850ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3511062827_cd87871c67.jpg: 448x640 1 person, 1 bear, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3511062827_cd87871c67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2241768909_3d96d48417.jpg: 640x512 2 persons, 1 handbag, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2241768909_3d96d48417.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591462960_86045906bd.jpg: 448x640 8 persons, 1 horse, 1 cow, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3591462960_86045906bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/394563330_68b566368c.jpg: 512x640 5 persons, 1 surfboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 394563330_68b566368c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/408748500_e8dc8c0c4f.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 408748500_e8dc8c0c4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3695064885_a6922f06b2.jpg: 544x640 8 persons, 1 dog, 2 umbrellas, 1 frisbee, 1 sports ball, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3695064885_a6922f06b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/186348874_75b2cf1ec5.jpg: 640x448 7 persons, 3 bicycles, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 186348874_75b2cf1ec5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3629492654_619d7b67ee.jpg: 640x608 2 persons, 1 skateboard, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3629492654_619d7b67ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1057089366_ca83da0877.jpg: 544x640 3 persons, 1 bench, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1057089366_ca83da0877.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2994104606_bc2df6c1f4.jpg: 608x640 1 dog, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2994104606_bc2df6c1f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376014640_ff5b00769f.jpg: 448x640 2 persons, 1 surfboard, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376014640_ff5b00769f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708743823_3e3e0554d1.jpg: 448x640 3 persons, 1 skis, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3708743823_3e3e0554d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/954987350_a0c608b467.jpg: 640x448 3 persons, 1 cell phone, 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 954987350_a0c608b467.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/525887861_4cc7a1beca.jpg: 640x480 1 person, 1 skateboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 525887861_4cc7a1beca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429142249_d09a32e291.jpg: 480x640 1 person, 1 cow, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3429142249_d09a32e291.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404552106_f516df0f5b.jpg: 480x640 9 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3404552106_f516df0f5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/772212710_f5fc22ed35.jpg: 608x640 3 persons, 1 frisbee, 5 teddy bears, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 772212710_f5fc22ed35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286017638_c688c83e3d.jpg: 448x640 4 persons, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286017638_c688c83e3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/634891010_9fa189effb.jpg: 384x640 2 dogs, 1 horse, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 634891010_9fa189effb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259231890_16fe167b31.jpg: 448x640 2 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259231890_16fe167b31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375014075_157388f8a9.jpg: 640x608 1 person, 1 dog, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3375014075_157388f8a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2857372127_d86639002c.jpg: 480x640 4 persons, 1 baseball bat, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2857372127_d86639002c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106026005_473a7b1c8c.jpg: 416x640 3 persons, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3106026005_473a7b1c8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3012513414_86180c44cb.jpg: 640x480 1 person, 2 skiss, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3012513414_86180c44cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2528547068_7d37479b9b.jpg: 448x640 6 persons, 1 car, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2528547068_7d37479b9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2469620360_6c620c6f35.jpg: 512x640 2 persons, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2469620360_6c620c6f35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639363462_bcdb21de29.jpg: 480x640 3 persons, 1 clock, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3639363462_bcdb21de29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744600462_5804577296.jpg: 448x640 8 persons, 16 bicycles, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2744600462_5804577296.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3024022266_3528c16ed8.jpg: 448x640 4 persons, 1 surfboard, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3024022266_3528c16ed8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3316046339_8e504be038.jpg: 640x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3316046339_8e504be038.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019857541_3de3e24f54.jpg: 448x640 1 cat, 1 dog, 1 bowl, 1 potted plant, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3019857541_3de3e24f54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/146577645_91b570c0d0.jpg: 512x640 2 persons, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 146577645_91b570c0d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3003691049_f4363c2d5c.jpg: 480x640 2 persons, 2 cars, 1 truck, 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3003691049_f4363c2d5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3123526484_02952e40fc.jpg: 448x640 7 persons, 1 traffic light, 3 handbags, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3123526484_02952e40fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/254295381_d98fa049f4.jpg: 640x640 1 dog, 6.9ms\n",
      "Speed: 2.2ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 254295381_d98fa049f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3157744152_31ace8c9ed.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3157744152_31ace8c9ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/348380010_33bb0599ef.jpg: 480x640 1 dog, 1 sports ball, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 348380010_33bb0599ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/305199420_89f6ddd778.jpg: 448x640 3 persons, 1 backpack, 2 sports balls, 1 teddy bear, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 305199420_89f6ddd778.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3607405494_0df89110a6.jpg: 448x640 3 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3607405494_0df89110a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3524612244_64f00afec5.jpg: 448x640 7 persons, 2 backpacks, 3 handbags, 3 chairs, 1 tv, 6 refrigerators, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3524612244_64f00afec5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2985679744_75a7102aab.jpg: 448x640 1 person, 1 tie, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2985679744_75a7102aab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489134459_1b3f46fc03.jpg: 480x640 2 persons, 1 handbag, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 489134459_1b3f46fc03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3492180255_0bd48a18f8.jpg: 640x448 5 persons, 1 chair, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3492180255_0bd48a18f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2792195540_af5fb95caa.jpg: 448x640 4 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2792195540_af5fb95caa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3200120942_59cfbb3437.jpg: 480x640 1 person, 1 skis, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3200120942_59cfbb3437.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399028417_50a621274c.jpg: 480x640 13 persons, 1 traffic light, 1 handbag, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3399028417_50a621274c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346402_5c070a0c6d.jpg: 448x640 4 persons, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346402_5c070a0c6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263741906_6e4508d1c8.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3263741906_6e4508d1c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208074567_ac44aeb3f3.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3208074567_ac44aeb3f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724718895_bd03f4a4dc.jpg: 512x640 1 dog, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3724718895_bd03f4a4dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345596_91e0e2daf5.jpg: 448x640 14 persons, 1 baseball glove, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345596_91e0e2daf5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2980445969_a86f4e6a0e.jpg: 448x640 4 persons, 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2980445969_a86f4e6a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2930622766_fa8f84deb1.jpg: 448x640 9 persons, 3 backpacks, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2930622766_fa8f84deb1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2206960564_325ed0c7ae.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2206960564_325ed0c7ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1517721825_10176d0683.jpg: 448x640 3 dogs, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1517721825_10176d0683.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623939135_0cd02ffa5d.jpg: 608x640 2 persons, 1 bottle, 3 cups, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2623939135_0cd02ffa5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373946160_1c82d54442.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3373946160_1c82d54442.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415003392_139c0f3586.jpg: 640x448 1 person, 1 bicycle, 1 motorcycle, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3415003392_139c0f3586.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3555231025_73fa54fa29.jpg: 640x448 4 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3555231025_73fa54fa29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3146937399_8c046b7b1a.jpg: 512x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3146937399_8c046b7b1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3368819708_0bfa0808f8.jpg: 448x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3368819708_0bfa0808f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1398613231_18de248606.jpg: 544x640 2 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1398613231_18de248606.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2735558076_0d7bbc18fc.jpg: 640x544 7 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2735558076_0d7bbc18fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3309082580_7228067ee0.jpg: 640x448 2 persons, 1 motorcycle, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3309082580_7228067ee0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537222436_ab406234ce.jpg: 640x448 3 persons, 1 backpack, 3 handbags, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 537222436_ab406234ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3479423813_517e93a43a.jpg: 640x448 3 persons, 1 motorcycle, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3479423813_517e93a43a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2208310655_a3d83080c5.jpg: 480x640 3 persons, 1 chair, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2208310655_a3d83080c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2169067981_cc19268f23.jpg: 512x640 4 persons, 1 chair, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2169067981_cc19268f23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286822339_5535af6b93.jpg: 480x640 11 persons, 1 bicycle, 1 backpack, 2 umbrellas, 1 handbag, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3286822339_5535af6b93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1764955991_5e53a28c87.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1764955991_5e53a28c87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3192069971_83c5a90b4c.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3192069971_83c5a90b4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677318686_b018862bb7.jpg: 448x640 1 person, 1 sports ball, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677318686_b018862bb7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2518094853_dfce24ce8c.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2518094853_dfce24ce8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2970067128_8842ab3603.jpg: 448x640 1 person, 1 bicycle, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2970067128_8842ab3603.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3288174272_2daa06d360.jpg: 448x640 3 persons, 1 handbag, 1 suitcase, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3288174272_2daa06d360.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/967719295_3257695095.jpg: 448x640 2 boats, 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 967719295_3257695095.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/275516348_cbccebc125.jpg: 480x640 1 bird, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 275516348_cbccebc125.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595412126_4020d4643b.jpg: 448x640 5 persons, 1 bird, 1 bottle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3595412126_4020d4643b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1020651753_06077ec457.jpg: 416x640 1 dog, 1 sports ball, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1020651753_06077ec457.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1222322358_225067636e.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1222322358_225067636e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/448658518_eec0b648a6.jpg: 608x640 1 person, 1 traffic light, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 448658518_eec0b648a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443703471_14845d8850.jpg: 480x640 1 dog, 1 sheep, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3443703471_14845d8850.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3450874870_c4dcf58fb3.jpg: 384x640 1 person, 1 skis, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3450874870_c4dcf58fb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/288177922_b889f2e1fe.jpg: 448x640 4 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 288177922_b889f2e1fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2257294002_0073263c54.jpg: 384x640 4 persons, 1 bench, 4 chairs, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2257294002_0073263c54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143574389_8a4048fbe2.jpg: 480x640 17 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143574389_8a4048fbe2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1151466868_3bc4d9580b.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1151466868_3bc4d9580b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2308108566_2cba6bca53.jpg: 448x640 1 person, 1 bicycle, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2308108566_2cba6bca53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102581664_5ea50f85c6.jpg: 352x640 1 dog, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2102581664_5ea50f85c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836808985_b26e4ca09e.jpg: 512x640 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2836808985_b26e4ca09e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2938120171_970564e3d8.jpg: 448x640 2 dogs, 1 potted plant, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2938120171_970564e3d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3616808182_fb4eaec778.jpg: 448x640 1 person, 1 bicycle, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3616808182_fb4eaec778.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454315016_f1e30d4676.jpg: 512x640 1 bird, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3454315016_f1e30d4676.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/990890291_afc72be141.jpg: 480x640 4 persons, 1 bicycle, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 990890291_afc72be141.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2789688929_9424fceed1.jpg: 480x640 3 persons, 1 handbag, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2789688929_9424fceed1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2113530024_5bc6a90e42.jpg: 640x640 6 persons, 1 sports ball, 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2113530024_5bc6a90e42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/385186343_464f5fc186.jpg: 640x448 (no detections), 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 385186343_464f5fc186.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084010872_cbc3ea8239.jpg: 640x480 2 persons, 1 cup, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3084010872_cbc3ea8239.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/185057637_e8ada37343.jpg: 544x640 5 persons, 2 frisbees, 1 bottle, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 185057637_e8ada37343.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2204777844_1bcf26bf84.jpg: 480x640 4 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2204777844_1bcf26bf84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2115620856_4fa5025ac6.jpg: 448x640 1 person, 1 bicycle, 9 cars, 3 trucks, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2115620856_4fa5025ac6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262475923_f1f77fcd9f.jpg: 640x640 2 persons, 3 cars, 1 frisbee, 2 baseball gloves, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3262475923_f1f77fcd9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437781040_82b06facb3.jpg: 480x640 4 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3437781040_82b06facb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2405978603_6221b0c2e7.jpg: 448x640 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2405978603_6221b0c2e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2066271441_1f1f056c01.jpg: 640x544 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2066271441_1f1f056c01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3111482098_11c0f4f309.jpg: 448x640 (no detections), 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3111482098_11c0f4f309.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421153376_d1d325568f.jpg: 448x640 1 bear, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 421153376_d1d325568f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533660418_f3a73a257c.jpg: 448x640 3 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3533660418_f3a73a257c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1234293791_6566284bcd.jpg: 640x480 1 person, 1 horse, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1234293791_6566284bcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3360823754_90967276ec.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3360823754_90967276ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752341621_54490b9b09.jpg: 256x640 5 persons, 8.0ms\n",
      "Speed: 0.9ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2752341621_54490b9b09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3246804978_ea2c9e56f2.jpg: 288x640 5 dogs, 2 sheeps, 8.5ms\n",
      "Speed: 1.0ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3246804978_ea2c9e56f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517124784_4b4eb62a7a.jpg: 448x640 5 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3517124784_4b4eb62a7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3408274796_0dc62225e9.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3408274796_0dc62225e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347356_8a515555fd.jpg: 448x640 14 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347356_8a515555fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514431934_9cf78f05a9.jpg: 448x640 2 persons, 1 truck, 1 traffic light, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 514431934_9cf78f05a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1420060118_aed262d606.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1420060118_aed262d606.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3721881082_afe9fc734e.jpg: 640x448 4 persons, 1 potted plant, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3721881082_afe9fc734e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1303548017_47de590273.jpg: 544x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1303548017_47de590273.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3111502208_71e2a414f5.jpg: 576x640 5 persons, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3111502208_71e2a414f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3104690333_4314d979de.jpg: 640x480 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3104690333_4314d979de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2909811789_ed8f3fd972.jpg: 448x640 1 person, 1 bicycle, 2 cars, 1 bus, 1 stop sign, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2909811789_ed8f3fd972.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601978895_9fec23ce0c.jpg: 640x480 2 persons, 1 traffic light, 1 sports ball, 1 baseball bat, 1 toilet, 1 clock, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3601978895_9fec23ce0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025495499_b15797b452.jpg: 640x512 1 person, 1 sports ball, 1 tennis racket, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3025495499_b15797b452.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485566887_57eac33bd1.jpg: 448x640 3 dogs, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 485566887_57eac33bd1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2436160351_108924a65b.jpg: 640x640 2 persons, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2436160351_108924a65b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839789830_89668775a4.jpg: 448x640 1 person, 1 dog, 2 frisbees, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2839789830_89668775a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3689355450_fd559b816d.jpg: 480x640 1 person, 2 benchs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3689355450_fd559b816d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2592711202_55f8c64495.jpg: 448x640 3 persons, 1 frisbee, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2592711202_55f8c64495.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353962769_ba48691bc6.jpg: 640x448 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3353962769_ba48691bc6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3468694409_a51571d621.jpg: 480x640 13 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3468694409_a51571d621.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/315125146_d9a8e60061.jpg: 640x448 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 315125146_d9a8e60061.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/632251903_b36701a5e9.jpg: 640x448 2 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 632251903_b36701a5e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/279550225_d64d56158a.jpg: 480x640 2 persons, 2 bottles, 1 cup, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 279550225_d64d56158a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540360421_f7c2401da8.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2540360421_f7c2401da8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3288839246_fdb00395ae.jpg: 640x512 1 person, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3288839246_fdb00395ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1093716555_801aacef79.jpg: 480x640 2 persons, 2 backpacks, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1093716555_801aacef79.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300500054_56653bf217.jpg: 416x640 4 persons, 7.3ms\n",
      "Speed: 1.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 300500054_56653bf217.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3462512074_2b4db1ffd6.jpg: 640x448 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3462512074_2b4db1ffd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/472535997_0dbf42b9f3.jpg: 640x480 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 472535997_0dbf42b9f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2986716822_e220754d32.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2986716822_e220754d32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563871276_c8b2a00df5.jpg: 448x640 1 boat, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3563871276_c8b2a00df5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632225464_612d7b4c0f.jpg: 640x448 1 airplane, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3632225464_612d7b4c0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/486917990_72bd4069af.jpg: 640x416 1 person, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 486917990_72bd4069af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474265683_43b1033d94.jpg: 544x640 10 persons, 1 tie, 1 cell phone, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3474265683_43b1033d94.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1460500597_866fa0c6f3.jpg: 640x480 3 persons, 1 chair, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1460500597_866fa0c6f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2522297487_57edf117f7.jpg: 448x640 4 persons, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2522297487_57edf117f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/771048251_602e5e8f45.jpg: 448x640 1 person, 1 cat, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 771048251_602e5e8f45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489372715_ce52da796a.jpg: 640x608 1 dog, 9.0ms\n",
      "Speed: 3.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 489372715_ce52da796a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2127209046_94711c747b.jpg: 640x480 5 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2127209046_94711c747b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215108916_0473007b47.jpg: 640x640 1 boat, 8.9ms\n",
      "Speed: 2.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3215108916_0473007b47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3395173129_f0ac0a1ed4.jpg: 448x640 3 persons, 1 chair, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3395173129_f0ac0a1ed4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2255332561_3375897ff0.jpg: 448x640 (no detections), 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2255332561_3375897ff0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336361161_c06cdd160e.jpg: 576x640 1 dog, 1 bottle, 9.5ms\n",
      "Speed: 2.9ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3336361161_c06cdd160e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2045109977_b00ec93491.jpg: 640x448 1 person, 1 chair, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2045109977_b00ec93491.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171066023_ec60ba30f3.jpg: 448x640 1 person, 3 skiss, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3171066023_ec60ba30f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3566111626_9a35a7b2c0.jpg: 640x448 1 person, 1 bicycle, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3566111626_9a35a7b2c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287931016_fb015e2e10.jpg: 384x640 2 dogs, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1287931016_fb015e2e10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2174206711_11cb712a8d.jpg: 416x640 5 persons, 9.1ms\n",
      "Speed: 1.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2174206711_11cb712a8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/871290666_4877e128c0.jpg: 480x640 1 person, 1 backpack, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 871290666_4877e128c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/324355356_859988a710.jpg: 512x640 2 dogs, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 324355356_859988a710.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3272847211_9e8a4f8308.jpg: 640x448 7 persons, 1 sports ball, 2 baseball bats, 1 baseball glove, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3272847211_9e8a4f8308.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163477256_073605e06e.jpg: 640x448 8 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3163477256_073605e06e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3620343911_64a862904e.jpg: 640x448 3 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3620343911_64a862904e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240558825_cd859d6230.jpg: 448x640 9 persons, 2 boats, 1 umbrella, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3240558825_cd859d6230.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639105305_bd9cb2d1db.jpg: 480x640 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3639105305_bd9cb2d1db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/393284934_d38e1cd6fe.jpg: 480x640 (no detections), 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 393284934_d38e1cd6fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/146100443_906d87faa2.jpg: 480x640 7 persons, 2 cell phones, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 146100443_906d87faa2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998185688_8d33e4ce38.jpg: 480x640 6 persons, 4 horses, 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2998185688_8d33e4ce38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425918361_3b890d9575.jpg: 448x640 1 person, 2 bicycles, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3425918361_3b890d9575.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3078229723_2aa52600de.jpg: 448x640 3 persons, 1 sports ball, 1 baseball glove, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3078229723_2aa52600de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/452419961_6d42ab7000.jpg: 480x640 4 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 452419961_6d42ab7000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2274992140_bb9e868bb8.jpg: 512x640 2 persons, 3 couchs, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2274992140_bb9e868bb8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2894850774_2d530040a1.jpg: 480x640 2 persons, 2 handbags, 1 suitcase, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2894850774_2d530040a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3559781965_d4ec00e506.jpg: 480x640 1 horse, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3559781965_d4ec00e506.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/531055369_936fd76a63.jpg: 480x640 4 persons, 1 handbag, 1 chair, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 531055369_936fd76a63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498957941_f0eda42787.jpg: 640x512 1 person, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 498957941_f0eda42787.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278007543_99f5a91a3e.jpg: 640x640 1 dog, 13.5ms\n",
      "Speed: 4.8ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 278007543_99f5a91a3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428797297_7fc3c862db.jpg: 448x640 2 dogs, 1 carrot, 10.6ms\n",
      "Speed: 2.4ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2428797297_7fc3c862db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2075041394_0b3ea1822d.jpg: 448x640 1 dog, 2 sheeps, 1 bear, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2075041394_0b3ea1822d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498997518_c2b16f0a0e.jpg: 640x640 4 persons, 13.5ms\n",
      "Speed: 4.5ms preprocess, 13.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3498997518_c2b16f0a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2298661279_016d87ba2f.jpg: 512x640 2 persons, 1 surfboard, 11.1ms\n",
      "Speed: 2.7ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2298661279_016d87ba2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3069282021_e05e1829f3.jpg: 448x640 1 dog, 5 horses, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3069282021_e05e1829f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3688005475_d200165cf7.jpg: 480x640 (no detections), 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3688005475_d200165cf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256292144_b53aadae27.jpg: 448x640 2 persons, 2 dogs, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 256292144_b53aadae27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2642474867_4e6346f809.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2642474867_4e6346f809.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2680990587_eee6bd04fb.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2680990587_eee6bd04fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2483792149_a9b4ffecec.jpg: 384x640 1 person, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2483792149_a9b4ffecec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/58368365_03ed3e5bdf.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 58368365_03ed3e5bdf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3658733605_fbcf570843.jpg: 640x448 1 person, 1 bicycle, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3658733605_fbcf570843.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1663751778_90501966f0.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1663751778_90501966f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3519942322_b37d088aae.jpg: 384x640 2 dogs, 1 cow, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3519942322_b37d088aae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1449625950_fc9a8d02d9.jpg: 384x640 2 dogs, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1449625950_fc9a8d02d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/687513087_413d4a3a3b.jpg: 480x640 3 dogs, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 687513087_413d4a3a3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3653764864_225958c9c1.jpg: 480x640 5 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3653764864_225958c9c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2422302286_385725e3cf.jpg: 640x480 1 person, 1 truck, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2422302286_385725e3cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354414391_a3908bd4ff.jpg: 640x512 2 cars, 3 dogs, 2 frisbees, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3354414391_a3908bd4ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860041212_797afd6ccf.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2860041212_797afd6ccf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139118874_599b30b116.jpg: 640x640 2 persons, 13.2ms\n",
      "Speed: 4.8ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3139118874_599b30b116.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444803340_fdcaab86f9.jpg: 640x448 2 persons, 1 bench, 1 handbag, 10.9ms\n",
      "Speed: 2.5ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 444803340_fdcaab86f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3280052365_c4644bf0a5.jpg: 448x640 10 persons, 1 dining table, 1 cell phone, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3280052365_c4644bf0a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3386893620_5f0bb4e794.jpg: 640x640 10 persons, 1 book, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3386893620_5f0bb4e794.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/476740978_45b65ebe0c.jpg: 448x640 14 persons, 1 traffic light, 3 umbrellas, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 476740978_45b65ebe0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1386964743_9e80d96b05.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1386964743_9e80d96b05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2310233145_910cb5b4c8.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2310233145_910cb5b4c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3113769557_9edbb8275c.jpg: 640x512 3 persons, 1 cell phone, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3113769557_9edbb8275c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3179336562_c3d0c0a3bd.jpg: 640x640 3 persons, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3179336562_c3d0c0a3bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272750492_91e8f67328.jpg: 640x576 1 person, 1 snowboard, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2272750492_91e8f67328.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862509442_4f5dc96dca.jpg: 480x640 4 persons, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2862509442_4f5dc96dca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375549004_beee810e60.jpg: 640x480 7 persons, 1 cup, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3375549004_beee810e60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3136688093_568b053bdf.jpg: 448x640 2 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3136688093_568b053bdf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/684255145_db3f8e3e46.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 684255145_db3f8e3e46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2097407245_c798e0dcaf.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2097407245_c798e0dcaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421928157_69a325366f.jpg: 448x640 10 persons, 1 boat, 1 dog, 1 snowboard, 3 surfboards, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421928157_69a325366f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3234719720_5bb2fc5ffa.jpg: 640x416 2 persons, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3234719720_5bb2fc5ffa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3282925526_535ff9f2b2.jpg: 480x640 2 persons, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3282925526_535ff9f2b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/805682444_90ed9e1ef3.jpg: 640x640 1 bird, 1 dog, 8.7ms\n",
      "Speed: 2.7ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 805682444_90ed9e1ef3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2176364472_31fcd37531.jpg: 416x640 1 person, 1 car, 1 dog, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2176364472_31fcd37531.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2583001715_1ce6f58942.jpg: 640x448 1 dog, 1 sports ball, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2583001715_1ce6f58942.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187096035_65dc416291.jpg: 480x640 4 persons, 1 chair, 1 couch, 1 remote, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3187096035_65dc416291.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2746072388_b127f8259b.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2746072388_b127f8259b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613242966_a1c63a0174.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3613242966_a1c63a0174.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330935489_9cb67ca36b.jpg: 416x640 1 dog, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3330935489_9cb67ca36b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3162095736_cc41dd41ff.jpg: 448x640 6 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3162095736_cc41dd41ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319020762_d429d56a69.jpg: 480x640 2 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3319020762_d429d56a69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497236690_a48bf7ac42.jpg: 640x448 5 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3497236690_a48bf7ac42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/199809190_e3f6bbe2bc.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 199809190_e3f6bbe2bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3105929913_94a6882e25.jpg: 448x640 6 persons, 1 tie, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3105929913_94a6882e25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3712742641_641282803e.jpg: 480x640 1 bear, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3712742641_641282803e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3410215754_5d5caeffaf.jpg: 480x640 5 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3410215754_5d5caeffaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2229179070_dc8ea8582e.jpg: 640x576 8 persons, 1 dog, 1 handbag, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2229179070_dc8ea8582e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/313326614_b2adbe59e0.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 313326614_b2adbe59e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/799431781_65dc312afc.jpg: 448x640 3 persons, 1 baseball glove, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 799431781_65dc312afc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3705976184_53ae07e898.jpg: 480x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3705976184_53ae07e898.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/242559369_9ae90ed0b4.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 242559369_9ae90ed0b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534669139_1a4f8ab9d5.jpg: 512x640 2 persons, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 534669139_1a4f8ab9d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2285570521_05015cbf4b.jpg: 640x480 1 person, 1 backpack, 1 skis, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2285570521_05015cbf4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3641022607_e7a5455d6c.jpg: 448x640 1 person, 1 surfboard, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3641022607_e7a5455d6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3543600125_223747ef4c.jpg: 480x640 3 persons, 2 cups, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3543600125_223747ef4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319388517_5609ae9805.jpg: 384x640 3 persons, 3 handbags, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3319388517_5609ae9805.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1515883224_14e36a53c7.jpg: 480x640 7 persons, 1 tennis racket, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1515883224_14e36a53c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3224578187_749882c17f.jpg: 448x640 1 boat, 1 dog, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3224578187_749882c17f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2278110011_ba846e7795.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2278110011_ba846e7795.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2276120079_4f235470bc.jpg: 448x640 6 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2276120079_4f235470bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3502897880_8392d0e4de.jpg: 448x640 4 persons, 3 skateboards, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3502897880_8392d0e4de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/463978865_c87c6ca84c.jpg: 416x640 4 persons, 8 cars, 1 traffic light, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 463978865_c87c6ca84c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/112243673_fd68255217.jpg: 448x640 3 persons, 4 motorcycles, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 112243673_fd68255217.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/636503038_17ca82b50f.jpg: 480x640 3 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 636503038_17ca82b50f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2720985888_8f5920e8cf.jpg: 640x448 2 persons, 1 banana, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2720985888_8f5920e8cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262386960_14f5d857db.jpg: 448x640 8 persons, 1 sports ball, 1 baseball glove, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3262386960_14f5d857db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/533483374_86c5d4c13e.jpg: 480x640 1 dog, 1 sports ball, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 533483374_86c5d4c13e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3282634762_2650d0088a.jpg: 384x640 1 bird, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3282634762_2650d0088a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2120383553_5825333a3f.jpg: 640x480 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2120383553_5825333a3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3658427967_6e2e57458d.jpg: 640x480 1 person, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3658427967_6e2e57458d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3409740108_1505489537.jpg: 480x640 2 persons, 1 dog, 1 horse, 3 cows, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3409740108_1505489537.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3495349745_1b29a63571.jpg: 640x416 2 persons, 2 bicycles, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3495349745_1b29a63571.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3322443827_a04a94bb91.jpg: 640x352 1 person, 1 bench, 1 cup, 55.8ms\n",
      "Speed: 1.2ms preprocess, 55.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3322443827_a04a94bb91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891240104_6755281868.jpg: 480x640 4 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2891240104_6755281868.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590654365_fd4819f48b.jpg: 448x640 5 persons, 1 cow, 1 elephant, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3590654365_fd4819f48b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3254817653_632e840423.jpg: 512x640 5 persons, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3254817653_632e840423.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084149186_4bc08b0752.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3084149186_4bc08b0752.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224026428_0165164ceb.jpg: 640x448 2 persons, 13.0ms\n",
      "Speed: 3.5ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 224026428_0165164ceb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2967549094_d32422eb01.jpg: 640x640 1 person, 1 train, 1 cell phone, 11.1ms\n",
      "Speed: 4.0ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2967549094_d32422eb01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/322050103_145f7233c6.jpg: 416x640 4 persons, 4 traffic lights, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 322050103_145f7233c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924884815_63826aa60d.jpg: 512x640 14 persons, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2924884815_63826aa60d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2852982055_8112d0964f.jpg: 480x640 3 persons, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2852982055_8112d0964f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2178064851_bb39652d28.jpg: 640x480 2 persons, 13.0ms\n",
      "Speed: 2.2ms preprocess, 13.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2178064851_bb39652d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294202771_e8ee78a439.jpg: 416x640 1 person, 12.2ms\n",
      "Speed: 3.0ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3294202771_e8ee78a439.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1489286545_8df476fa26.jpg: 448x640 1 person, 1 bench, 1 dog, 10.9ms\n",
      "Speed: 2.5ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1489286545_8df476fa26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3380364224_2626d9d354.jpg: 384x640 3 persons, 1 snowboard, 10.7ms\n",
      "Speed: 1.9ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3380364224_2626d9d354.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/217838128_1f0a84ddc1.jpg: 640x448 1 person, 1 boat, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 217838128_1f0a84ddc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218519240_cac5aab53c.jpg: 640x640 1 person, 9.4ms\n",
      "Speed: 3.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2218519240_cac5aab53c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091338773_9cf10467b4.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3091338773_9cf10467b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/170100272_d820db2199.jpg: 640x640 1 person, 1 fire hydrant, 1 bench, 9.3ms\n",
      "Speed: 3.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 170100272_d820db2199.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/223299142_521aedf9e7.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 223299142_521aedf9e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/255330891_86d65dfdbf.jpg: 640x608 2 persons, 1 sheep, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 255330891_86d65dfdbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272489996_95b0a62d15.jpg: 448x640 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2272489996_95b0a62d15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595992258_6f192e6ae7.jpg: 640x544 1 dog, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3595992258_6f192e6ae7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/540338917_57069687be.jpg: 480x640 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 540338917_57069687be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3559429170_3183c404b9.jpg: 640x448 2 persons, 1 bicycle, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3559429170_3183c404b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2601008162_f00eeb5c14.jpg: 640x448 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2601008162_f00eeb5c14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2469351714_d72becd21e.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2469351714_d72becd21e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2081446176_f97dc76951.jpg: 608x640 1 person, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2081446176_f97dc76951.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560771491_2a18b6241e.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3560771491_2a18b6241e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710050559_f6f12760fe.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3710050559_f6f12760fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/519167484_ee03e2a91e.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 519167484_ee03e2a91e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3691670743_0ed111bcf3.jpg: 416x640 8 persons, 1 skateboard, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3691670743_0ed111bcf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2366643786_9c9a830db8.jpg: 480x640 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2366643786_9c9a830db8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2182488373_df73c7cc09.jpg: 448x640 10 persons, 3 cars, 1 handbag, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2182488373_df73c7cc09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3383545083_1d7c95b003.jpg: 384x640 1 dog, 1 sports ball, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3383545083_1d7c95b003.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/584484388_0eeb36d03d.jpg: 480x640 2 dogs, 1 frisbee, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 584484388_0eeb36d03d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3029043380_b28644ea5d.jpg: 640x512 1 person, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3029043380_b28644ea5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3475005101_6f6e437459.jpg: 640x640 3 persons, 1 sports ball, 2 baseball gloves, 8.9ms\n",
      "Speed: 3.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3475005101_6f6e437459.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3630332976_fdba22c50b.jpg: 448x640 3 persons, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3630332976_fdba22c50b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3310067561_b92017acab.jpg: 480x640 3 persons, 1 cat, 1 dog, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3310067561_b92017acab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560125106_691c292893.jpg: 480x640 1 truck, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3560125106_691c292893.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3082934678_58534e9d2c.jpg: 480x640 2 dogs, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3082934678_58534e9d2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/576920249_df1bdc2068.jpg: 480x640 18 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 576920249_df1bdc2068.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540515072_8c951b738b.jpg: 448x640 1 bird, 2 dogs, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3540515072_8c951b738b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/396179143_e1511336e1.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 396179143_e1511336e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3648160673_0c783236a6.jpg: 448x640 1 dog, 1 surfboard, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3648160673_0c783236a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236095031_5cb17dc54a.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 236095031_5cb17dc54a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1262077938_8b9516c273.jpg: 544x640 1 person, 1 car, 1 bench, 1 handbag, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1262077938_8b9516c273.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2594336381_a93772823b.jpg: 640x576 1 person, 7.8ms\n",
      "Speed: 2.2ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2594336381_a93772823b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3338590946_f25b76cbc7.jpg: 480x640 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3338590946_f25b76cbc7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2199250692_a16b0c2ae1.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2199250692_a16b0c2ae1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1124448967_2221af8dc5.jpg: 480x640 1 person, 1 bicycle, 1 chair, 1 potted plant, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1124448967_2221af8dc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1255504166_f2437febcb.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1255504166_f2437febcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070485870_eab1a75c6f.jpg: 480x640 1 person, 1 snowboard, 1 surfboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3070485870_eab1a75c6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/285306009_f6ddabe687.jpg: 512x640 3 persons, 3 benchs, 2 chairs, 1 cell phone, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 285306009_f6ddabe687.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2660008870_b672a4c76a.jpg: 640x448 1 person, 1 clock, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2660008870_b672a4c76a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2203449950_e51d0f9065.jpg: 640x448 1 person, 1 bird, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2203449950_e51d0f9065.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1322323208_c7ecb742c6.jpg: 448x640 3 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1322323208_c7ecb742c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3178371973_60c6b8f110.jpg: 448x640 4 persons, 1 bus, 1 train, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3178371973_60c6b8f110.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3509611207_7645b1d28d.jpg: 448x640 16 persons, 1 handbag, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3509611207_7645b1d28d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225025519_c089c14559.jpg: 640x448 3 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3225025519_c089c14559.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2229509318_be3fef006b.jpg: 448x640 1 person, 1 dog, 1 cow, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2229509318_be3fef006b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2589308405_e208b5e745.jpg: 544x640 2 persons, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2589308405_e208b5e745.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016759846_062663f8ab.jpg: 640x448 2 persons, 1 bird, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3016759846_062663f8ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3066429707_842e50b8f7.jpg: 512x640 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3066429707_842e50b8f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2813033949_e19fa08805.jpg: 640x448 1 car, 1 dog, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2813033949_e19fa08805.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3718892835_a3e74a3417.jpg: 448x640 6 persons, 1 bicycle, 2 sports balls, 2 skateboards, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3718892835_a3e74a3417.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2610447973_89227ff978.jpg: 448x640 3 persons, 1 motorcycle, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2610447973_89227ff978.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911658792_6a6ef07e3a.jpg: 640x448 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2911658792_6a6ef07e3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2156726763_034ecd2e39.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2156726763_034ecd2e39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635400219_2e1a984fd3.jpg: 448x640 2 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2635400219_2e1a984fd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537532165_e4b7c0e61a.jpg: 640x480 3 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 537532165_e4b7c0e61a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1508269285_6c5723f67d.jpg: 640x448 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1508269285_6c5723f67d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/172097782_f0844ec317.jpg: 640x448 2 persons, 1 car, 2 surfboards, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 172097782_f0844ec317.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025546819_ce031d2fc3.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3025546819_ce031d2fc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467510271_0f57e52768.jpg: 448x640 3 persons, 1 airplane, 1 surfboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3467510271_0f57e52768.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3558683579_8fb36b55a6.jpg: 448x640 13 persons, 1 frisbee, 1 sports ball, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3558683579_8fb36b55a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/270809922_043e3bef06.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 270809922_043e3bef06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/359173181_a75c950aeb.jpg: 640x448 1 horse, 1 cow, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 359173181_a75c950aeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/222878446_32c6fc4bc9.jpg: 480x640 1 person, 1 bench, 1 umbrella, 1 sports ball, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 222878446_32c6fc4bc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/927420680_6cba7c040a.jpg: 416x640 1 person, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 927420680_6cba7c040a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/443885436_6e927e6c58.jpg: 480x640 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 443885436_6e927e6c58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/445861800_75fc6a8c16.jpg: 448x640 4 persons, 7 cars, 1 truck, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 445861800_75fc6a8c16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2301525531_edde12d673.jpg: 448x640 1 bench, 1 dog, 1 frisbee, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2301525531_edde12d673.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3645080830_1d9ee2f50a.jpg: 640x448 2 persons, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3645080830_1d9ee2f50a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1801663973_5ad393caeb.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1801663973_5ad393caeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/429205889_ff5a006311.jpg: 640x416 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 429205889_ff5a006311.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3198962089_e647d1b0cd.jpg: 448x640 1 person, 1 snowboard, 1 surfboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3198962089_e647d1b0cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884252132_5d8e776893.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2884252132_5d8e776893.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3250589803_3f440ba781.jpg: 640x448 4 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3250589803_3f440ba781.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3115174046_9e96b9ce47.jpg: 448x640 1 person, 1 skateboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3115174046_9e96b9ce47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609999845_faf5d2fe74.jpg: 448x640 10 persons, 1 frisbee, 1 sports ball, 1 tennis racket, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3609999845_faf5d2fe74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2337919839_df83827fa0.jpg: 544x640 2 persons, 1 bird, 11.8ms\n",
      "Speed: 3.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2337919839_df83827fa0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1479679558_d0a01bc62b.jpg: 480x640 (no detections), 10.5ms\n",
      "Speed: 2.7ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1479679558_d0a01bc62b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1213336750_2269b51397.jpg: 320x640 2 persons, 12 cars, 1 truck, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 1213336750_2269b51397.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2161799386_27aa938421.jpg: 640x448 1 person, 1 remote, 10.6ms\n",
      "Speed: 2.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2161799386_27aa938421.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2276499757_b44dc6f8ce.jpg: 448x640 3 dogs, 1 cow, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2276499757_b44dc6f8ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3700004668_99c3e3f55b.jpg: 448x640 3 persons, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3700004668_99c3e3f55b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/42637986_135a9786a6.jpg: 576x640 2 persons, 1 boat, 1 bench, 10.0ms\n",
      "Speed: 3.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 42637986_135a9786a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251648670_9339943ba2.jpg: 640x448 6 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3251648670_9339943ba2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1470132731_fa416b7504.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1470132731_fa416b7504.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3279988814_d3693dcb6c.jpg: 480x640 3 persons, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3279988814_d3693dcb6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2576878141_87f25a10f0.jpg: 640x480 1 person, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2576878141_87f25a10f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116379964_86986750af.jpg: 480x640 3 persons, 1 dog, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3116379964_86986750af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2573141440_28a762d537.jpg: 448x640 1 person, 1 chair, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2573141440_28a762d537.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2971211296_2587c3924d.jpg: 640x448 1 person, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2971211296_2587c3924d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2377460540_8cfb62463a.jpg: 640x512 1 dog, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2377460540_8cfb62463a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1479857177_9d4a6f38fd.jpg: 640x480 2 suitcases, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1479857177_9d4a6f38fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354489242_dd529ffa1f.jpg: 448x640 4 persons, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3354489242_dd529ffa1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3368207495_1e2dbd6d3f.jpg: 512x640 1 dog, 1 horse, 1 sports ball, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3368207495_1e2dbd6d3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2205336881_d9ee4179d3.jpg: 480x640 2 persons, 1 cup, 2 couchs, 2 remotes, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2205336881_d9ee4179d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1713248099_d860df4e10.jpg: 416x640 2 dogs, 1 cow, 1 bear, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1713248099_d860df4e10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2990563425_2f7246f458.jpg: 448x640 1 person, 1 motorcycle, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2990563425_2f7246f458.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2672445419_251ce9419a.jpg: 640x448 1 person, 1 frisbee, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2672445419_251ce9419a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3705430840_e108de78bf.jpg: 640x448 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3705430840_e108de78bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1453366750_6e8cf601bf.jpg: 448x640 3 persons, 1 bed, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1453366750_6e8cf601bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/811663364_4b350a62ce.jpg: 640x448 1 person, 1 baseball bat, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 811663364_4b350a62ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2451285022_59255e7fd9.jpg: 640x640 2 persons, 7.9ms\n",
      "Speed: 2.6ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2451285022_59255e7fd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482859574_3908de3427.jpg: 512x640 8 persons, 1 bicycle, 2 backpacks, 2 handbags, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3482859574_3908de3427.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359563671_35b67898e7.jpg: 448x640 1 person, 1 snowboard, 1 sports ball, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3359563671_35b67898e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3313620862_0c65c645f5.jpg: 640x480 7 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3313620862_0c65c645f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1100214449_d10861e633.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1100214449_d10861e633.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405970010_8cebaa77d3.jpg: 480x640 1 person, 1 backpack, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 405970010_8cebaa77d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/320093980_5388cb3733.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 320093980_5388cb3733.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638783842_af08dbb518.jpg: 640x448 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3638783842_af08dbb518.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3451085951_e66f7f5d5c.jpg: 448x640 2 dogs, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3451085951_e66f7f5d5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397725001_e51f7c391c.jpg: 640x480 9 persons, 1 bus, 10.7ms\n",
      "Speed: 2.7ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 397725001_e51f7c391c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2890731828_8a7032503a.jpg: 640x448 1 person, 1 motorcycle, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2890731828_8a7032503a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346471_c756a8f139.jpg: 448x640 16 persons, 1 sports ball, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346471_c756a8f139.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2939371251_f923569a72.jpg: 448x640 7 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2939371251_f923569a72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891924845_92f69b0f18.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2891924845_92f69b0f18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2628331789_c7f7d90e5d.jpg: 448x640 8 persons, 1 bottle, 2 chairs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2628331789_c7f7d90e5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2768021570_46bc6325e3.jpg: 256x640 13 persons, 1 dog, 4 sheeps, 12.3ms\n",
      "Speed: 1.3ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2768021570_46bc6325e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397601572_9587a39291.jpg: 448x640 4 persons, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 397601572_9587a39291.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374567836_3ae12ecffb.jpg: 480x640 1 dog, 1 bear, 1 frisbee, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 374567836_3ae12ecffb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273489163_8209545810.jpg: 640x448 12 persons, 1 train, 3 handbags, 1 suitcase, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3273489163_8209545810.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/54723805_bcf7af3f16.jpg: 448x640 5 persons, 1 sports ball, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 54723805_bcf7af3f16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2252264255_03fefc25af.jpg: 448x640 2 persons, 5 cars, 1 sports ball, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2252264255_03fefc25af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/469969326_4b84073286.jpg: 480x640 1 person, 1 handbag, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 469969326_4b84073286.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240014971_ee2ea8406f.jpg: 480x640 7 persons, 1 motorcycle, 2 traffic lights, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3240014971_ee2ea8406f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371567346_b6522efdb8.jpg: 640x544 2 persons, 1 handbag, 9.9ms\n",
      "Speed: 2.6ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3371567346_b6522efdb8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/148512773_bae6901fd6.jpg: 480x640 2 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 148512773_bae6901fd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3252985078_c4ee2aca4e.jpg: 512x640 13 persons, 1 sports ball, 1 baseball glove, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3252985078_c4ee2aca4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2492258999_5764124bba.jpg: 448x640 4 persons, 1 bed, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2492258999_5764124bba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2445283938_ff477c7952.jpg: 448x640 10 persons, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2445283938_ff477c7952.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3279228339_71deaa3d9b.jpg: 448x640 2 persons, 1 bottle, 1 refrigerator, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3279228339_71deaa3d9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2655183854_5852790214.jpg: 608x640 7 persons, 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2655183854_5852790214.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3360994630_d4616c1b14.jpg: 480x640 1 bird, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3360994630_d4616c1b14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1403414927_5f80281505.jpg: 640x640 2 sheeps, 8.1ms\n",
      "Speed: 2.4ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1403414927_5f80281505.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666537170_c4ecda4be8.jpg: 480x640 6 persons, 1 surfboard, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3666537170_c4ecda4be8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3689975998_72f50b6d91.jpg: 640x480 1 person, 12.2ms\n",
      "Speed: 3.3ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3689975998_72f50b6d91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506096155_13632955e8.jpg: 480x640 4 persons, 3 motorcycles, 10.1ms\n",
      "Speed: 2.6ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3506096155_13632955e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3712923460_1b20ebb131.jpg: 480x640 9 persons, 11.8ms\n",
      "Speed: 3.3ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3712923460_1b20ebb131.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3081330705_7a1732e12c.jpg: 448x640 1 person, 10.8ms\n",
      "Speed: 2.4ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3081330705_7a1732e12c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394857899_76bfdf720b.jpg: 640x608 1 person, 10.6ms\n",
      "Speed: 3.0ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2394857899_76bfdf720b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2875528143_94d9480fdd.jpg: 608x640 1 person, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2875528143_94d9480fdd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375920709_ef7370fa09.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3375920709_ef7370fa09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2425262733_afe0718276.jpg: 640x448 1 person, 1 skateboard, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2425262733_afe0718276.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/760138567_762d9022d4.jpg: 512x640 2 persons, 1 surfboard, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 760138567_762d9022d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3246991821_750a3097e2.jpg: 448x640 5 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3246991821_750a3097e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2335634931_7e9e8c2959.jpg: 640x576 1 person, 1 dog, 1 skateboard, 1 potted plant, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2335634931_7e9e8c2959.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/697582336_601462e052.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 697582336_601462e052.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356642567_f1d92cb81b.jpg: 544x640 2 persons, 9.9ms\n",
      "Speed: 2.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3356642567_f1d92cb81b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681056426_fbd6c0c92c.jpg: 640x448 3 horses, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3681056426_fbd6c0c92c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2853407781_c9fea8eef4.jpg: 416x640 1 person, 2 cups, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2853407781_c9fea8eef4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563059800_c073081ce3.jpg: 640x448 2 persons, 3 bottles, 1 cup, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3563059800_c073081ce3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3668518431_43abb169eb.jpg: 448x640 10 persons, 4 boats, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3668518431_43abb169eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635938723_11b85e6763.jpg: 640x416 1 person, 2 ties, 11.7ms\n",
      "Speed: 1.9ms preprocess, 11.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2635938723_11b85e6763.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1089181217_ee1167f7af.jpg: 640x640 1 dog, 10.8ms\n",
      "Speed: 3.3ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1089181217_ee1167f7af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2664102751_d5a737a566.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2664102751_d5a737a566.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2275253272_00f941366e.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2275253272_00f941366e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/652542470_60e858da64.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 652542470_60e858da64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2506892928_7e79bec613.jpg: 448x640 4 persons, 1 teddy bear, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2506892928_7e79bec613.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561433412_3985208d53.jpg: 480x640 1 dog, 1 sports ball, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3561433412_3985208d53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397982550_cf9f5cdb74.jpg: 640x448 5 persons, 1 car, 1 handbag, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 397982550_cf9f5cdb74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/708945669_08e7ffb9a7.jpg: 480x640 1 person, 1 cell phone, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 708945669_08e7ffb9a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/268704620_8a8cef4cb3.jpg: 480x640 2 dogs, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 268704620_8a8cef4cb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480327661_fb69829f57.jpg: 640x448 5 persons, 3 chairs, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2480327661_fb69829f57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636247381_65ccf8f106.jpg: 480x640 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3636247381_65ccf8f106.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/161669933_3e7d8c7e2c.jpg: 448x640 1 person, 1 motorcycle, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 161669933_3e7d8c7e2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2942798367_022df04b49.jpg: 448x640 2 persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2942798367_022df04b49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344798356_5cc41f7939.jpg: 640x480 2 persons, 1 backpack, 1 skis, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3344798356_5cc41f7939.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3333039854_461329aac2.jpg: 480x640 4 persons, 1 chair, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3333039854_461329aac2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3374384485_751f719be4.jpg: 640x448 1 horse, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3374384485_751f719be4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/486720042_b785e7f88c.jpg: 640x480 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 486720042_b785e7f88c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1622619190_d0b51aff28.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1622619190_d0b51aff28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2156131463_5b53636cf0.jpg: 640x480 6 persons, 1 umbrella, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2156131463_5b53636cf0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532761259_14026c1e96.jpg: 512x640 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3532761259_14026c1e96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/693785581_68bec8312a.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 693785581_68bec8312a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2114739371_83aa8bdb0e.jpg: 352x640 1 dog, 8.9ms\n",
      "Speed: 1.2ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2114739371_83aa8bdb0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1159574340_99ba8c3c59.jpg: 640x480 2 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1159574340_99ba8c3c59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3018467501_a03d404413.jpg: 448x640 2 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3018467501_a03d404413.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347823_6b25c3e58e.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347823_6b25c3e58e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/350588129_6aef7b7fe2.jpg: 640x512 1 person, 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 350588129_6aef7b7fe2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632197966_0c5061025f.jpg: 480x640 5 persons, 1 bicycle, 2 motorcycles, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3632197966_0c5061025f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/111766423_4522d36e56.jpg: 448x640 1 person, 1 bird, 1 chair, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 111766423_4522d36e56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475313618_bdb2f72be5.jpg: 640x480 3 persons, 1 backpack, 1 handbag, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 475313618_bdb2f72be5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3246773992_89bf86937b.jpg: 640x512 5 persons, 1 umbrella, 1 handbag, 12.1ms\n",
      "Speed: 3.7ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3246773992_89bf86937b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2785108434_cd4a1c949c.jpg: 640x448 4 persons, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2785108434_cd4a1c949c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323661814_1e8e1ae88c.jpg: 640x448 3 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3323661814_1e8e1ae88c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2340206885_58754a799a.jpg: 384x640 5 dogs, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2340206885_58754a799a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2621415349_ef1a7e73be.jpg: 640x448 1 person, 1 donut, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2621415349_ef1a7e73be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639684919_cb6fbf5638.jpg: 640x448 2 persons, 2 ties, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3639684919_cb6fbf5638.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374124237_51f62b6937.jpg: 480x640 4 persons, 1 handbag, 1 bowl, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 374124237_51f62b6937.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215870337_c92a1a1b2f.jpg: 480x640 14 persons, 1 traffic light, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3215870337_c92a1a1b2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3352531708_a65dd694b1.jpg: 480x640 2 persons, 1 horse, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3352531708_a65dd694b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359089834_263e529c71.jpg: 448x640 2 persons, 3 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3359089834_263e529c71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2613993276_3c365cca12.jpg: 480x640 1 dog, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2613993276_3c365cca12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1805990081_da9cefe3a5.jpg: 448x640 1 person, 4 cars, 1 truck, 1 dog, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 4.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1805990081_da9cefe3a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3662909101_21b9e59a3e.jpg: 448x640 1 dog, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3662909101_21b9e59a3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752919987_8bfca604ab.jpg: 448x640 13 persons, 1 handbag, 1 cell phone, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2752919987_8bfca604ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2240539658_dea8db6e55.jpg: 480x640 2 dogs, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2240539658_dea8db6e55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225478803_f7a9a41a1d.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3225478803_f7a9a41a1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831313661_1a328acb70.jpg: 640x512 1 person, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2831313661_1a328acb70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2455286250_fb6a66175a.jpg: 480x640 5 persons, 2 sports balls, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2455286250_fb6a66175a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2461631708_decc5b8c87.jpg: 480x640 5 persons, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2461631708_decc5b8c87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/502671104_b2114246c7.jpg: 480x640 1 person, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 502671104_b2114246c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535768334_a3924bcccd.jpg: 640x448 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3535768334_a3924bcccd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2510020918_b2ca0fb2aa.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2510020918_b2ca0fb2aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/842960985_91daf0d6ec.jpg: 288x640 1 person, 1 parking meter, 2 umbrellas, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 842960985_91daf0d6ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1390268323_2c8204e91c.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1390268323_2c8204e91c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2298283771_fb21a4217e.jpg: 448x640 4 persons, 1 chair, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2298283771_fb21a4217e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2851931813_eaf8ed7be3.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2851931813_eaf8ed7be3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2652155912_8ba5426790.jpg: 480x640 (no detections), 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2652155912_8ba5426790.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2970183443_accd597e0a.jpg: 448x640 3 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2970183443_accd597e0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3473320907_3884a7203b.jpg: 480x640 5 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3473320907_3884a7203b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867026654_38be983b44.jpg: 480x640 1 dog, 1 frisbee, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2867026654_38be983b44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892995070_39f3c9a56e.jpg: 448x640 2 dogs, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2892995070_39f3c9a56e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/211295363_49010ca38d.jpg: 480x640 3 boats, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 211295363_49010ca38d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1269470943_ba7fc49b4d.jpg: 480x640 1 person, 1 sports ball, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1269470943_ba7fc49b4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2833820456_143ea6ce47.jpg: 480x640 2 persons, 3 cups, 1 dining table, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2833820456_143ea6ce47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225037367_a71fa86319.jpg: 416x640 23 persons, 1 truck, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3225037367_a71fa86319.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671262694_29fbeb9d95.jpg: 448x640 14 persons, 1 skateboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3671262694_29fbeb9d95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421322723_3470543368.jpg: 640x448 3 persons, 2 cell phones, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 421322723_3470543368.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3566225740_375fc15dde.jpg: 448x640 3 persons, 1 car, 4 trucks, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3566225740_375fc15dde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294209955_a1f1e2cc19.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3294209955_a1f1e2cc19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/382701159_f98c1988cd.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 382701159_f98c1988cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415113018_7b95ddcd11.jpg: 448x640 10 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415113018_7b95ddcd11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084001782_41a848df4e.jpg: 480x640 3 persons, 1 cup, 1 chair, 1 dining table, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3084001782_41a848df4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534655560_dc1c335b3f.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 534655560_dc1c335b3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613800013_5a54968ab0.jpg: 640x448 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3613800013_5a54968ab0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2256231539_05c27179f1.jpg: 640x480 4 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2256231539_05c27179f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2228022180_9597b2a458.jpg: 448x640 24 persons, 1 backpack, 1 handbag, 1 bottle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2228022180_9597b2a458.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2594459477_8ca0121a9a.jpg: 448x640 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2594459477_8ca0121a9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484070900_3e76d7fd30.jpg: 448x640 1 person, 1 snowboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3484070900_3e76d7fd30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3056569684_c264c88d00.jpg: 480x640 2 trucks, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3056569684_c264c88d00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635908229_b9fc90d3fb.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2635908229_b9fc90d3fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/367964525_b1528ac6e4.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 367964525_b1528ac6e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/103205630_682ca7285b.jpg: 288x640 4 persons, 1 truck, 8.0ms\n",
      "Speed: 1.0ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 103205630_682ca7285b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2980118787_2099de53ec.jpg: 640x448 1 person, 1 motorcycle, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2980118787_2099de53ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535529555_583d89b7f2.jpg: 448x640 2 dogs, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 535529555_583d89b7f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978409165_acc4f29a40.jpg: 640x544 1 person, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2978409165_acc4f29a40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/432167214_c17fcc1a2d.jpg: 512x640 8 persons, 1 laptop, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 432167214_c17fcc1a2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3108378861_d2214d971e.jpg: 640x480 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3108378861_d2214d971e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2696951725_e0ae54f6da.jpg: 640x448 2 persons, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2696951725_e0ae54f6da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470903027_489cc507de.jpg: 416x640 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 470903027_489cc507de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582920844_2742804f3d.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3582920844_2742804f3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417037373_67f7db2dd2.jpg: 480x640 7 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3417037373_67f7db2dd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1332208215_fa824f6659.jpg: 512x640 6 persons, 1 bench, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1332208215_fa824f6659.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2636514498_01fcc5f501.jpg: 640x480 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2636514498_01fcc5f501.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2222559267_6fd31e3941.jpg: 480x640 2 persons, 6 cars, 1 traffic light, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2222559267_6fd31e3941.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/345284642_77dded0907.jpg: 448x640 1 cow, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 345284642_77dded0907.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251646144_d9f4ccca3f.jpg: 544x640 1 person, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3251646144_d9f4ccca3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2751567262_e089b33ed9.jpg: 480x640 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2751567262_e089b33ed9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458379941_657182bb09.jpg: 640x640 7 persons, 5 umbrellas, 1 handbag, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3458379941_657182bb09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399843227_3b9d2a8dbf.jpg: 640x448 5 persons, 1 bowl, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3399843227_3b9d2a8dbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/316577571_27a0e0253e.jpg: 352x640 2 dogs, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 316577571_27a0e0253e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480820830_bdec1f5b76.jpg: 448x640 1 person, 1 dog, 1 cow, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2480820830_bdec1f5b76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286620180_4b00e93e8e.jpg: 448x640 16 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286620180_4b00e93e8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3679405397_bb130ea3c2.jpg: 640x448 8 persons, 1 umbrella, 12.4ms\n",
      "Speed: 3.0ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3679405397_bb130ea3c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3433567526_00b5a70319.jpg: 480x640 1 dog, 10.2ms\n",
      "Speed: 2.8ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3433567526_00b5a70319.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/429851331_b248ca01cd.jpg: 384x640 1 dog, 1 frisbee, 9.8ms\n",
      "Speed: 1.9ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 429851331_b248ca01cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/519228867_2fd25e38d4.jpg: 640x448 1 person, 1 bed, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 519228867_2fd25e38d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1716445442_9cf3528342.jpg: 480x640 1 person, 1 car, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1716445442_9cf3528342.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/426065353_e9a604a01f.jpg: 480x640 6 persons, 2 bottles, 1 cup, 1 cell phone, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 426065353_e9a604a01f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517040752_debec03376.jpg: 448x640 8 persons, 1 bottle, 1 chair, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3517040752_debec03376.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436418401_b00ceb27c0.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3436418401_b00ceb27c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/17273391_55cfc7d3d4.jpg: 416x640 1 person, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 17273391_55cfc7d3d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522989916_f20319cc59.jpg: 448x640 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3522989916_f20319cc59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2508313118_524e93d48c.jpg: 448x640 1 dog, 1 frisbee, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2508313118_524e93d48c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/375384566_254c2362d4.jpg: 448x640 3 persons, 2 chairs, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 375384566_254c2362d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1472882567_33dc14c8b6.jpg: 480x640 4 persons, 6 backpacks, 1 skis, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1472882567_33dc14c8b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/661757041_61e131e913.jpg: 480x640 1 dog, 1 sports ball, 11.5ms\n",
      "Speed: 3.3ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 661757041_61e131e913.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2485467050_1d5e2696d4.jpg: 320x640 4 persons, 1 car, 1 frisbee, 12.4ms\n",
      "Speed: 2.0ms preprocess, 12.4ms inference, 1.6ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2485467050_1d5e2696d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2041867793_552819a40b.jpg: 480x640 3 persons, 2 wine glasss, 1 cup, 1 bowl, 1 chair, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2041867793_552819a40b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3470129475_9e58b6742c.jpg: 544x640 5 persons, 1 bicycle, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3470129475_9e58b6742c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/93922153_8d831f7f01.jpg: 448x640 2 persons, 1 backpack, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 93922153_8d831f7f01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516267455_ca17cc1323.jpg: 384x640 1 bird, 1 dog, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3516267455_ca17cc1323.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2180886307_5156460b2c.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2180886307_5156460b2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500505549_d848209837.jpg: 448x640 8 persons, 1 car, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3500505549_d848209837.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2225241766_f1e7132e3e.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2225241766_f1e7132e3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1341787777_4f1ebb1793.jpg: 320x640 1 dog, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 1341787777_4f1ebb1793.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1324816249_86600a6759.jpg: 416x640 1 dog, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1324816249_86600a6759.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613175012_bcd063e7c9.jpg: 480x640 5 persons, 1 car, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3613175012_bcd063e7c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/279728508_6bd7281f3c.jpg: 640x640 2 persons, 9.3ms\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 279728508_6bd7281f3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2003663004_5b70920a98.jpg: 544x640 8 persons, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2003663004_5b70920a98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2687529141_edee32649e.jpg: 448x640 14 persons, 1 umbrella, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2687529141_edee32649e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2774581025_81a3074e2e.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2774581025_81a3074e2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576250302_14779632bd.jpg: 448x640 1 person, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3576250302_14779632bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/136644343_0e2b423829.jpg: 480x640 2 persons, 1 bicycle, 1 motorcycle, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 136644343_0e2b423829.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589368949_0866846949.jpg: 640x448 5 persons, 1 umbrella, 1 chair, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3589368949_0866846949.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2354540393_a149722680.jpg: 640x448 1 person, 1 bed, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2354540393_a149722680.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3521201948_9049197f20.jpg: 352x640 2 dogs, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3521201948_9049197f20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2053006423_6adf69ca67.jpg: 640x448 2 persons, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2053006423_6adf69ca67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3290105461_7590f23371.jpg: 448x640 1 person, 1 baseball bat, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3290105461_7590f23371.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2203286182_b453e9d176.jpg: 480x640 1 dog, 1 frisbee, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2203286182_b453e9d176.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3272002857_ace031f564.jpg: 448x640 1 person, 1 horse, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3272002857_ace031f564.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639617775_149001232a.jpg: 448x640 12 persons, 3 ties, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3639617775_149001232a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/502115726_927dd684d3.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 502115726_927dd684d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/57422853_b5f6366081.jpg: 480x640 1 person, 1 umbrella, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 57422853_b5f6366081.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2789238858_14261dd25a.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2789238858_14261dd25a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154152744_4e93ec8a62.jpg: 640x480 2 dogs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3154152744_4e93ec8a62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2265367960_7928c5642f.jpg: 640x544 2 persons, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2265367960_7928c5642f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/282960970_574aa1ba49.jpg: 640x352 1 person, 8.6ms\n",
      "Speed: 1.3ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 282960970_574aa1ba49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/870710405_51e507b31a.jpg: 640x352 1 person, 8.1ms\n",
      "Speed: 1.4ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 870710405_51e507b31a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3699522388_2333f01f40.jpg: 448x640 3 persons, 1 surfboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3699522388_2333f01f40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3297272270_285b8878b2.jpg: 512x640 11 persons, 1 elephant, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3297272270_285b8878b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2870875612_2cbb9e4a3c.jpg: 448x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2870875612_2cbb9e4a3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/283428775_a3665bee7c.jpg: 512x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 283428775_a3665bee7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/560278886_df4bd2e212.jpg: 448x640 9 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 560278886_df4bd2e212.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2697909987_128f11d1b7.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2697909987_128f11d1b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3367758711_a8c09607ac.jpg: 448x640 2 persons, 1 car, 1 bus, 1 truck, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3367758711_a8c09607ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/255741044_1102982213.jpg: 544x640 2 dogs, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 255741044_1102982213.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344526059_4a097af285.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3344526059_4a097af285.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486324591_9f5eeb24b9.jpg: 640x448 1 person, 2 horses, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3486324591_9f5eeb24b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/293879742_5fe0ffd894.jpg: 640x512 1 dog, 2 cows, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 293879742_5fe0ffd894.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459858555_c3f0087a72.jpg: 448x640 (no detections), 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3459858555_c3f0087a72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2792212974_23b1ef05fa.jpg: 352x640 1 person, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2792212974_23b1ef05fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2963672852_c28043bb2c.jpg: 640x480 1 person, 1 bicycle, 1 bench, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2963672852_c28043bb2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2882893687_1d10d68f2b.jpg: 640x448 2 persons, 1 car, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2882893687_1d10d68f2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3053785363_50392f2c53.jpg: 288x640 9 persons, 1 baseball glove, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3053785363_50392f2c53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2181724497_dbb7fcb0a9.jpg: 448x640 1 person, 1 knife, 1 spoon, 1 pizza, 1 dining table, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2181724497_dbb7fcb0a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458434150_2b0d619244.jpg: 448x640 1 bird, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3458434150_2b0d619244.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/255091930_aa2b5c0eb9.jpg: 480x640 1 person, 1 knife, 1 banana, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 255091930_aa2b5c0eb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2045928594_92510c1c2a.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2045928594_92510c1c2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3304712466_18cbdb85fe.jpg: 448x640 14 persons, 5 bicycles, 1 truck, 2 backpacks, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3304712466_18cbdb85fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/416650559_cd08d3cd96.jpg: 640x448 9 persons, 1 bottle, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 416650559_cd08d3cd96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2292406847_f366350600.jpg: 480x640 1 person, 1 boat, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2292406847_f366350600.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3077781040_bc27778609.jpg: 448x640 1 person, 1 bicycle, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3077781040_bc27778609.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/630476551_2ee7399f77.jpg: 416x640 2 persons, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 630476551_2ee7399f77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602083686_e8a1af69cf.jpg: 448x640 13 persons, 1 motorcycle, 1 bus, 2 trucks, 11.3ms\n",
      "Speed: 2.6ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602083686_e8a1af69cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726130458_07df79e969.jpg: 448x640 19 persons, 1 baseball bat, 1 baseball glove, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3726130458_07df79e969.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899374885_f3b2b1a290.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2899374885_f3b2b1a290.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/272940778_a184dbea42.jpg: 608x640 2 dogs, 9.4ms\n",
      "Speed: 3.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 272940778_a184dbea42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524310507_51220580de.jpg: 640x576 1 person, 1 truck, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 524310507_51220580de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429465163_fb8ac7ce7f.jpg: 640x448 1 person, 1 bird, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3429465163_fb8ac7ce7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547499166_67fb4af4ea.jpg: 640x512 3 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3547499166_67fb4af4ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2914331767_8574e7703d.jpg: 640x448 6 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2914331767_8574e7703d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2773682293_3b712e47ff.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2773682293_3b712e47ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116731299_6139b25c18.jpg: 640x512 1 dog, 1 bear, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3116731299_6139b25c18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591461782_687e320042.jpg: 448x640 3 persons, 1 horse, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3591461782_687e320042.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041348852_872c027c16.jpg: 448x640 5 dogs, 3 sheeps, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3041348852_872c027c16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2817230861_d27341dec0.jpg: 640x480 2 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2817230861_d27341dec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/624742559_ff467d8ebc.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 624742559_ff467d8ebc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604391853_b4809fcb8c.jpg: 448x640 2 persons, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3604391853_b4809fcb8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1237985362_dbafc59280.jpg: 416x640 5 persons, 1 umbrella, 10.0ms\n",
      "Speed: 1.9ms preprocess, 10.0ms inference, 2.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1237985362_dbafc59280.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273403495_fcd09c453e.jpg: 448x640 1 person, 12.6ms\n",
      "Speed: 3.2ms preprocess, 12.6ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3273403495_fcd09c453e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3321956909_7b5ddf500f.jpg: 640x448 1 person, 11.4ms\n",
      "Speed: 2.5ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3321956909_7b5ddf500f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2814406547_a237ef0122.jpg: 448x640 4 persons, 1 car, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2814406547_a237ef0122.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1858963639_4588cd4be9.jpg: 640x480 1 person, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1858963639_4588cd4be9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472270112_0a7cb7b27c.jpg: 640x576 4 persons, 2 baseball gloves, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3472270112_0a7cb7b27c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2755362721_368cbde668.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2755362721_368cbde668.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084711346_fda0f5a3e6.jpg: 448x640 4 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3084711346_fda0f5a3e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2690538407_7ca157be85.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2690538407_7ca157be85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262301835_9f1a49b80a.jpg: 448x640 4 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3262301835_9f1a49b80a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2784408839_53a25a21eb.jpg: 480x640 1 person, 3 surfboards, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2784408839_53a25a21eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3314180199_2121e80368.jpg: 640x448 1 person, 2 sports balls, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3314180199_2121e80368.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041645937_ff7591d134.jpg: 640x512 3 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3041645937_ff7591d134.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2148695079_9ae6a9b1c7.jpg: 384x640 2 dogs, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2148695079_9ae6a9b1c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/733172023_5810350af6.jpg: 640x480 1 person, 2 chairs, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 733172023_5810350af6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2205328215_3ffc094cde.jpg: 640x448 1 person, 1 bicycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2205328215_3ffc094cde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/439569646_c917f1bc78.jpg: 480x640 1 person, 1 bed, 1 book, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 439569646_c917f1bc78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3145967019_1a83ebf712.jpg: 448x640 3 persons, 1 cup, 1 cell phone, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3145967019_1a83ebf712.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468911753_cc595f5da0.jpg: 448x640 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 468911753_cc595f5da0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344411431_6f4917bb2f.jpg: 448x640 7 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3344411431_6f4917bb2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3419197575_aa5b84a9f0.jpg: 640x480 4 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3419197575_aa5b84a9f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2795287622_da187a3e86.jpg: 640x480 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2795287622_da187a3e86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1082379191_ec1e53f996.jpg: 480x640 3 persons, 1 bench, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1082379191_ec1e53f996.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3574244361_715ac347cd.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3574244361_715ac347cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1659358141_0433c9bf99.jpg: 448x640 1 horse, 1 cow, 6.8ms\n",
      "Speed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1659358141_0433c9bf99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2920969723_83918fe909.jpg: 640x448 1 person, 1 skateboard, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2920969723_83918fe909.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439331800_e71e1d808f.jpg: 384x640 4 persons, 1 suitcase, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3439331800_e71e1d808f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/509778093_21236bb64d.jpg: 448x640 1 dog, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 509778093_21236bb64d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2741380826_cfe0ddf0a9.jpg: 640x480 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2741380826_cfe0ddf0a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/733965014_1a0b2b5ee9.jpg: 480x640 1 person, 1 bench, 1 dog, 1 cow, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 733965014_1a0b2b5ee9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3156991513_3bf03333d8.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3156991513_3bf03333d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259666643_ae49524c81.jpg: 640x480 3 persons, 1 frisbee, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3259666643_ae49524c81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3231751379_10ebf7150c.jpg: 448x640 1 person, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3231751379_10ebf7150c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2057305043_952b8dc82c.jpg: 480x640 1 kite, 10.5ms\n",
      "Speed: 2.5ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2057305043_952b8dc82c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2524003134_580e74328b.jpg: 512x640 1 bird, 9.4ms\n",
      "Speed: 2.6ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2524003134_580e74328b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/272546805_536c719648.jpg: 480x640 3 dogs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 272546805_536c719648.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3218480482_66af7587c8.jpg: 640x512 5 persons, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3218480482_66af7587c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3117336911_a729f42869.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3117336911_a729f42869.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432495898_a5859f06b6.jpg: 448x640 1 bicycle, 1 motorcycle, 1 cow, 1 giraffe, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3432495898_a5859f06b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3570800810_978c993133.jpg: 448x640 2 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3570800810_978c993133.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197482764_2f289cb726.jpg: 512x640 4 persons, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3197482764_2f289cb726.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485357535_b45ba5b6da.jpg: 384x640 4 persons, 1 handbag, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 485357535_b45ba5b6da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672105509_53b13b2ed4.jpg: 480x640 1 bicycle, 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3672105509_53b13b2ed4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2526041608_a9775ab8d7.jpg: 640x544 3 persons, 1 baseball glove, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2526041608_a9775ab8d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2890057168_c712f932e0.jpg: 448x640 12 persons, 4 cars, 1 motorcycle, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2890057168_c712f932e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3388330419_85d72f7cda.jpg: 640x448 2 persons, 3 chairs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3388330419_85d72f7cda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/874665322_9ad05c4065.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 874665322_9ad05c4065.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3738685861_8dfff28760.jpg: 640x480 1 person, 1 potted plant, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3738685861_8dfff28760.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102315758_a9148a842f.jpg: 448x640 2 persons, 13.9ms\n",
      "Speed: 3.6ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2102315758_a9148a842f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2404520067_87798dbaee.jpg: 384x640 1 dog, 12.4ms\n",
      "Speed: 2.3ms preprocess, 12.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2404520067_87798dbaee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3416013671_98b5c75046.jpg: 480x640 4 persons, 10.6ms\n",
      "Speed: 2.5ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3416013671_98b5c75046.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/597543181_6a85ef4c17.jpg: 640x448 1 person, 10.0ms\n",
      "Speed: 2.0ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 597543181_6a85ef4c17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/866841633_05d273b96d.jpg: 320x640 3 persons, 1 surfboard, 11.2ms\n",
      "Speed: 1.6ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 866841633_05d273b96d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2178306830_6af49375b4.jpg: 480x640 1 airplane, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2178306830_6af49375b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459492423_c881f12c9f.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3459492423_c881f12c9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587092143_c63030ed6d.jpg: 448x640 2 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3587092143_c63030ed6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1408958345_68eea9a4e4.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1408958345_68eea9a4e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3499720588_c32590108e.jpg: 640x512 1 person, 1 bicycle, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3499720588_c32590108e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3478877323_0a507a601b.jpg: 448x640 2 persons, 3 cars, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3478877323_0a507a601b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855910826_d075845288.jpg: 384x640 8 persons, 1 sports ball, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2855910826_d075845288.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1688699579_2f72328c7e.jpg: 640x480 2 dogs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1688699579_2f72328c7e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2248487950_c62d0c81a9.jpg: 576x640 4 persons, 1 car, 1 handbag, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2248487950_c62d0c81a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454988449_1de1ef4f20.jpg: 576x640 5 persons, 8.6ms\n",
      "Speed: 2.6ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3454988449_1de1ef4f20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2073105823_6dacade004.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2073105823_6dacade004.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295671644_0e10891b6d.jpg: 576x640 1 dog, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3295671644_0e10891b6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203878596_cbb307ce3b.jpg: 416x640 2 dogs, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3203878596_cbb307ce3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3643684616_9d2be87a5a.jpg: 480x640 1 person, 2 bottles, 3 cups, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3643684616_9d2be87a5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3634400263_c6fcaa48e1.jpg: 640x448 1 person, 3 bicycles, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3634400263_c6fcaa48e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/759015118_4bd3617e60.jpg: 640x480 3 persons, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 759015118_4bd3617e60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2632366677_43dee456a5.jpg: 544x640 1 dog, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2632366677_43dee456a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541491057_61a49588d9.jpg: 448x640 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3541491057_61a49588d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3357194782_c261bb6cbf.jpg: 384x640 4 persons, 1 train, 8.1ms\n",
      "Speed: 1.3ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3357194782_c261bb6cbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/574274795_57e0834e7d.jpg: 640x384 1 bird, 1 kite, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 574274795_57e0834e7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1679565118_d36f0d6d52.jpg: 480x640 3 dogs, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1679565118_d36f0d6d52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3698607223_22fe09763a.jpg: 640x448 4 persons, 1 skis, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3698607223_22fe09763a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1305564994_00513f9a5b.jpg: 352x640 2 persons, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 1305564994_00513f9a5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2595102568_347f6d4b07.jpg: 384x640 6 persons, 4 cars, 1 handbag, 8.0ms\n",
      "Speed: 1.4ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2595102568_347f6d4b07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109260218_fca831f933.jpg: 480x640 5 persons, 1 sports ball, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 109260218_fca831f933.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2568417021_afa68423e5.jpg: 416x640 1 person, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2568417021_afa68423e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3298199743_d8dd8f94a0.jpg: 640x448 14 persons, 2 kites, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3298199743_d8dd8f94a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3096918227_f9d26a7db2.jpg: 640x448 1 person, 1 skateboard, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3096918227_f9d26a7db2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2000459828_3c9e109106.jpg: 640x448 3 persons, 1 backpack, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2000459828_3c9e109106.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2298097636_c5de0079de.jpg: 480x640 1 dog, 11.3ms\n",
      "Speed: 3.1ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2298097636_c5de0079de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3682277595_55f8b16975.jpg: 480x640 1 horse, 8.6ms\n",
      "Speed: 2.4ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3682277595_55f8b16975.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514905846_b54d13946a.jpg: 640x448 1 person, 2 benchs, 1 tie, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 514905846_b54d13946a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3072782873_3278f3b3a7.jpg: 640x480 8 persons, 1 bicycle, 1 car, 2 traffic lights, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3072782873_3278f3b3a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204354161_caf89ec784.jpg: 480x640 1 person, 1 motorcycle, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3204354161_caf89ec784.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2960033435_c20cc7399a.jpg: 640x448 1 person, 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2960033435_c20cc7399a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604383863_5e387cb8e6.jpg: 448x640 4 persons, 1 tie, 1 chair, 1 cell phone, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3604383863_5e387cb8e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1501811302_5e723fc529.jpg: 448x640 1 dog, 1 cow, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1501811302_5e723fc529.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300922408_05a4f9938c.jpg: 448x640 1 person, 2 dogs, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300922408_05a4f9938c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3528966521_2e871ff6a1.jpg: 480x640 10 persons, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3528966521_2e871ff6a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351704877_28dea303aa.jpg: 480x640 3 persons, 1 horse, 1 elephant, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3351704877_28dea303aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247691240_3881777ab8.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 247691240_3881777ab8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3475581086_a533567561.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3475581086_a533567561.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3306951622_93b82cac21.jpg: 448x640 2 persons, 1 boat, 1 surfboard, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3306951622_93b82cac21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2444935470_7b0226b756.jpg: 640x352 1 person, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2444935470_7b0226b756.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3202255152_08973fa3d7.jpg: 640x448 1 person, 1 skateboard, 1 clock, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3202255152_08973fa3d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/96985174_31d4c6f06d.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 96985174_31d4c6f06d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155987659_b9ea318dd3.jpg: 480x640 1 dog, 2 frisbees, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3155987659_b9ea318dd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697153626_90fb177731.jpg: 512x640 1 person, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3697153626_90fb177731.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/708860480_1a956ae0f7.jpg: 448x640 1 person, 1 horse, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 708860480_1a956ae0f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3493479159_609ebe1b35.jpg: 640x448 1 person, 1 motorcycle, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3493479159_609ebe1b35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867845624_22e4fe0a23.jpg: 640x512 1 dog, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2867845624_22e4fe0a23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/288880576_818b6ecfef.jpg: 640x448 1 person, 1 umbrella, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 288880576_818b6ecfef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/961189263_0990f3bcb5.jpg: 640x640 1 person, 1 carrot, 7.5ms\n",
      "Speed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 961189263_0990f3bcb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3168123064_d1983b8f92.jpg: 416x640 1 person, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3168123064_d1983b8f92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3316725440_9ccd9b5417.jpg: 448x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3316725440_9ccd9b5417.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2358561039_e215a8d6cd.jpg: 640x448 5 persons, 3 dogs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2358561039_e215a8d6cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2922222717_12195af92d.jpg: 480x640 1 person, 1 cat, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2922222717_12195af92d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3475552729_a3abd81ee6.jpg: 448x640 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3475552729_a3abd81ee6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2097398349_ff178b3f1b.jpg: 480x640 1 person, 1 skis, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2097398349_ff178b3f1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2443229844_277cded27d.jpg: 448x640 1 person, 1 bench, 1 frisbee, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2443229844_277cded27d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616673985_fa4354cc53.jpg: 480x640 8 persons, 2 cars, 1 dog, 1 chair, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2616673985_fa4354cc53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3082196097_2d15455b00.jpg: 480x640 2 persons, 1 dog, 6.6ms\n",
      "Speed: 1.9ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3082196097_2d15455b00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3350260112_fcb47ff6b2.jpg: 448x640 12 persons, 2 umbrellas, 1 surfboard, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3350260112_fcb47ff6b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/934375844_dd149fed18.jpg: 480x640 1 person, 1 couch, 1 bed, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 934375844_dd149fed18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420260768_26a600b844.jpg: 640x512 4 persons, 1 sports ball, 1 tennis racket, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3420260768_26a600b844.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2551632823_0cb7dd779b.jpg: 608x640 11 persons, 1 handbag, 3 bottles, 7.3ms\n",
      "Speed: 2.4ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2551632823_0cb7dd779b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/311146855_0b65fdb169.jpg: 640x512 9 persons, 8.6ms\n",
      "Speed: 1.8ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 311146855_0b65fdb169.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2077346067_0a3a5aae65.jpg: 480x640 1 person, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2077346067_0a3a5aae65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294952558_96bb8c8cf3.jpg: 640x416 1 dog, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3294952558_96bb8c8cf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3057770908_3fd97f79f9.jpg: 640x320 1 person, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 3057770908_3fd97f79f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2462702522_1b25654762.jpg: 640x448 2 persons, 1 frisbee, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2462702522_1b25654762.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/478750151_e0adb5030a.jpg: 640x640 1 dog, 8.9ms\n",
      "Speed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 478750151_e0adb5030a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537359971_6e28f5e66e.jpg: 640x416 2 persons, 2 handbags, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 537359971_6e28f5e66e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2562483332_eb791a3ce5.jpg: 640x480 2 persons, 1 potted plant, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2562483332_eb791a3ce5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/462198798_89e2df0358.jpg: 480x640 1 bird, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 462198798_89e2df0358.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3488512097_e500cb499f.jpg: 640x448 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3488512097_e500cb499f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393924525_1bf45ca217.jpg: 512x640 1 person, 1 sports ball, 4 tennis rackets, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2393924525_1bf45ca217.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2404488732_ca1bbdacc2.jpg: 640x480 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2404488732_ca1bbdacc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2646116932_232573f030.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2646116932_232573f030.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2061397486_53a61e17c5.jpg: 480x640 1 dog, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2061397486_53a61e17c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3646820231_2abc736840.jpg: 352x640 7 sheeps, 13.6ms\n",
      "Speed: 2.5ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3646820231_2abc736840.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2378356400_f6bde5d9b3.jpg: 480x640 2 persons, 1 tie, 1 teddy bear, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2378356400_f6bde5d9b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1837976956_3c45d0f9b8.jpg: 640x480 1 person, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1837976956_3c45d0f9b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2594902417_f65d8866a8.jpg: 448x640 2 dogs, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2594902417_f65d8866a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2763044275_aa498eb88b.jpg: 576x640 3 dogs, 1 cow, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2763044275_aa498eb88b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2971298546_dd595cf297.jpg: 352x640 1 potted plant, 10.0ms\n",
      "Speed: 1.7ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2971298546_dd595cf297.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2429729667_42effc165d.jpg: 640x576 5 persons, 1 bench, 8.9ms\n",
      "Speed: 2.9ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2429729667_42effc165d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347635_e691395c2f.jpg: 640x448 8 persons, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347635_e691395c2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690189273_927d42ff43.jpg: 640x480 7 persons, 1 traffic light, 1 cell phone, 12.3ms\n",
      "Speed: 3.6ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3690189273_927d42ff43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2100735137_05c6079537.jpg: 480x640 1 person, 1 frisbee, 1 sports ball, 1 tennis racket, 9.8ms\n",
      "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2100735137_05c6079537.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431487300_0123195f9b.jpg: 544x640 3 persons, 1 motorcycle, 13.6ms\n",
      "Speed: 4.3ms preprocess, 13.6ms inference, 2.0ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3431487300_0123195f9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/449352117_63c359c6e7.jpg: 640x448 1 dog, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 449352117_63c359c6e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1271210445_7f7ecf3791.jpg: 480x640 6 persons, 1 wine glass, 1 cup, 1 bowl, 1 cake, 1 dining table, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1271210445_7f7ecf3791.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1797554350_20998753c0.jpg: 640x448 2 persons, 1 umbrella, 1 bottle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1797554350_20998753c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273091032_98f724b36b.jpg: 480x640 1 bear, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3273091032_98f724b36b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1425069308_488e5fcf9d.jpg: 448x640 1 cat, 2 dogs, 1 chair, 1 couch, 1 potted plant, 1 tv, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1425069308_488e5fcf9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3291255271_a185eba408.jpg: 640x480 1 person, 1 baseball bat, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3291255271_a185eba408.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/350529848_9569a3bcbc.jpg: 448x640 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 350529848_9569a3bcbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3422394336_e465f60b7c.jpg: 448x640 1 bird, 1 bear, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3422394336_e465f60b7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2893476169_f38dd32051.jpg: 448x640 8 persons, 1 skateboard, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2893476169_f38dd32051.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2624044128_641b38c0cf.jpg: 448x640 2 persons, 1 umbrella, 1 handbag, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2624044128_641b38c0cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2886411666_72d8b12ce4.jpg: 448x640 1 dog, 1 frisbee, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2886411666_72d8b12ce4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2542037086_58c833699c.jpg: 480x640 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2542037086_58c833699c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587009091_37188fd07e.jpg: 640x480 5 birds, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3587009091_37188fd07e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2097489021_ca1b9f5c3b.jpg: 640x448 3 persons, 1 bottle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2097489021_ca1b9f5c3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/105342180_4d4a40b47f.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 105342180_4d4a40b47f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1312227131_771b5ed201.jpg: 480x640 1 horse, 2 cows, 1 surfboard, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1312227131_771b5ed201.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2052953131_30834196fb.jpg: 480x640 1 person, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2052953131_30834196fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2501942587_e59b91d1da.jpg: 480x640 12 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2501942587_e59b91d1da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2872197070_4e97c3ccfa.jpg: 352x640 1 person, 1 cow, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2872197070_4e97c3ccfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/494921598_af73bda568.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 1.6ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 494921598_af73bda568.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319723910_af5b5f1fae.jpg: 448x640 1 person, 1 car, 1 truck, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3319723910_af5b5f1fae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/439916996_1ddb9dc8e7.jpg: 640x608 1 person, 4 dogs, 1 frisbee, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 439916996_1ddb9dc8e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2004674713_2883e63c67.jpg: 512x640 1 person, 2 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2004674713_2883e63c67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2310126952_7dc86d88f6.jpg: 352x640 2 dogs, 1 horse, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2310126952_7dc86d88f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2450486758_a66fd296ea.jpg: 480x640 13 persons, 1 dog, 4 frisbees, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2450486758_a66fd296ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1564614124_0ee6799935.jpg: 448x640 1 person, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1564614124_0ee6799935.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2756636539_cc1eacbf4a.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2756636539_cc1eacbf4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2431120202_b24fe2333a.jpg: 480x640 1 person, 2 skiss, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2431120202_b24fe2333a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1999444757_1b92efb590.jpg: 480x640 7 persons, 1 chair, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1999444757_1b92efb590.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3060969260_08f43e4f4f.jpg: 640x448 3 persons, 2 bicycles, 1 car, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3060969260_08f43e4f4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3473264983_67917a931f.jpg: 448x640 6 persons, 1 kite, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3473264983_67917a931f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2043520315_4a2c782c90.jpg: 416x640 3 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2043520315_4a2c782c90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606093421_eddd46c2c7.jpg: 448x640 2 persons, 1 boat, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3606093421_eddd46c2c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856700531_312528eea4.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2856700531_312528eea4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/842961005_692737888e.jpg: 288x640 1 person, 1 parking meter, 1 frisbee, 8.1ms\n",
      "Speed: 1.1ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 842961005_692737888e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2890113532_ab2003d74e.jpg: 480x640 2 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2890113532_ab2003d74e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2667783499_3a4f38f636.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2667783499_3a4f38f636.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2471974379_a4a4d2b389.jpg: 544x640 6 persons, 3 motorcycles, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2471974379_a4a4d2b389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3209564153_077ed4d246.jpg: 640x640 1 person, 1 skis, 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3209564153_077ed4d246.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271178748_630d269811.jpg: 640x448 6 persons, 1 sports ball, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3271178748_630d269811.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3558438174_d8f41438a4.jpg: 640x480 5 persons, 4 cars, 1 handbag, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3558438174_d8f41438a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/953941506_5082c9160c.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 953941506_5082c9160c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319899418_4bcf1b00d8.jpg: 640x448 1 person, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3319899418_4bcf1b00d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623930900_b9df917b82.jpg: 512x640 3 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2623930900_b9df917b82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2850719435_221f15e951.jpg: 640x480 2 persons, 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2850719435_221f15e951.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3328495660_ed0e3f29cf.jpg: 448x640 4 persons, 1 teddy bear, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3328495660_ed0e3f29cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562816250_6e14d436b1.jpg: 448x640 1 boat, 1 umbrella, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3562816250_6e14d436b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2516317118_10ae66b87a.jpg: 640x448 1 person, 1 remote, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2516317118_10ae66b87a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3652584682_5b5c43e445.jpg: 640x448 1 person, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3652584682_5b5c43e445.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2586532797_dcf22a5021.jpg: 416x640 1 dog, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2586532797_dcf22a5021.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089122314_40d5739aef.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2089122314_40d5739aef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2922981282_203f04bf9b.jpg: 448x640 2 persons, 2 beds, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2922981282_203f04bf9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/412101267_7257e6d8c0.jpg: 640x448 2 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 412101267_7257e6d8c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273871383_1ddb3562ea.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2273871383_1ddb3562ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2148013097_6a4f495bc5.jpg: 512x640 3 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2148013097_6a4f495bc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2313822078_282dc07531.jpg: 448x640 (no detections), 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2313822078_282dc07531.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106791484_13e18c33d8.jpg: 640x448 1 person, 1 bed, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3106791484_13e18c33d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/857914283_270d7d1c87.jpg: 384x640 2 persons, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 857914283_270d7d1c87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2189181027_a445b13438.jpg: 416x640 2 persons, 1 dog, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2189181027_a445b13438.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2757779501_c41c86a595.jpg: 640x512 1 person, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2757779501_c41c86a595.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2922512807_d382528a93.jpg: 640x448 2 persons, 1 skateboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2922512807_d382528a93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2302516347_26054442f9.jpg: 480x640 2 persons, 1 handbag, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2302516347_26054442f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/307327914_f98f576adb.jpg: 480x640 3 persons, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 307327914_f98f576adb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/527968666_1fcddf81ab.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 527968666_1fcddf81ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3727752439_907795603b.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3727752439_907795603b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/472396131_6e97068d93.jpg: 480x640 1 bird, 1 cat, 1 dog, 1 sheep, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 472396131_6e97068d93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1472249944_d887c3aeda.jpg: 544x640 5 persons, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1472249944_d887c3aeda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1296412797_85b6d2f8d6.jpg: 640x448 4 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1296412797_85b6d2f8d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2706430695_3b5667741c.jpg: 640x480 2 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2706430695_3b5667741c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116039960_54d1d68145.jpg: 416x640 8 persons, 1 frisbee, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3116039960_54d1d68145.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/19212715_20476497a3.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 19212715_20476497a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247706586_7e25c7adf8.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 247706586_7e25c7adf8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/326028454_fb396167e6.jpg: 640x480 5 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 326028454_fb396167e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3260768565_2b725be090.jpg: 448x640 6 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3260768565_2b725be090.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/808245064_8a7971fc5b.jpg: 512x640 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 808245064_8a7971fc5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2110692070_8aaaa1ae39.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2110692070_8aaaa1ae39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532036676_e88b13e0a1.jpg: 640x480 1 person, 1 bus, 1 train, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 532036676_e88b13e0a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346523971_d3f1e12ce4.jpg: 576x640 5 persons, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2346523971_d3f1e12ce4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3243094580_ccd01679f5.jpg: 448x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3243094580_ccd01679f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2376878930_dd3e7cc544.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2376878930_dd3e7cc544.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671935691_57bdd0e778.jpg: 480x640 10 persons, 2 chairs, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3671935691_57bdd0e778.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/536537638_f5ee42410b.jpg: 544x640 2 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 536537638_f5ee42410b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2694178830_116be6a6a9.jpg: 640x512 3 persons, 1 baseball glove, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2694178830_116be6a6a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2944836001_b38b516286.jpg: 448x640 2 persons, 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2944836001_b38b516286.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1177994172_10d143cb8d.jpg: 640x512 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1177994172_10d143cb8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/517102724_a0f3069156.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 517102724_a0f3069156.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3546891929_f31a99cd0d.jpg: 512x640 11 persons, 2 bicycles, 3 backpacks, 1 bottle, 1 cell phone, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3546891929_f31a99cd0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/481632457_7372f18275.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 481632457_7372f18275.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547313700_39368b9a2f.jpg: 448x640 2 dogs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3547313700_39368b9a2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2603033456_3584d95116.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2603033456_3584d95116.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1362128028_8422d53dc4.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1362128028_8422d53dc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/411216802_aead9e67e3.jpg: 544x640 3 dogs, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 411216802_aead9e67e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3268908792_c24529fe88.jpg: 448x640 3 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3268908792_c24529fe88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612825666_54f5a2bc06.jpg: 640x448 10 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3612825666_54f5a2bc06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2058472558_7dd5014abd.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2058472558_7dd5014abd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3448490813_f9623e864d.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3448490813_f9623e864d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3737711435_113ccd0a52.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3737711435_113ccd0a52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425414048_fa14d33067.jpg: 480x640 1 person, 1 motorcycle, 1 suitcase, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3425414048_fa14d33067.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/343073813_df822aceac.jpg: 512x640 1 dog, 10.4ms\n",
      "Speed: 3.0ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 343073813_df822aceac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2186087673_c7a73da7ce.jpg: 448x640 1 person, 1 bed, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2186087673_c7a73da7ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521770311_3086ca90de.jpg: 448x640 2 persons, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2521770311_3086ca90de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3453259666_9ecaa8bb4b.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3453259666_9ecaa8bb4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1052358063_eae6744153.jpg: 640x480 2 persons, 2 skateboards, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1052358063_eae6744153.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3524914023_4e96edb09f.jpg: 448x640 10 persons, 1 bench, 1 baseball bat, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3524914023_4e96edb09f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1456393634_74022d9056.jpg: 640x512 2 persons, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1456393634_74022d9056.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1163282319_b729b24c46.jpg: 448x640 1 person, 1 frisbee, 9.8ms\n",
      "Speed: 2.0ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1163282319_b729b24c46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/459814265_d48ba48978.jpg: 416x640 1 dog, 9.5ms\n",
      "Speed: 1.9ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 459814265_d48ba48978.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3515665835_22e6fb1193.jpg: 416x640 1 person, 1 motorcycle, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 2.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3515665835_22e6fb1193.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1683444418_815f660379.jpg: 480x640 13 persons, 1 backpack, 1 handbag, 11.9ms\n",
      "Speed: 2.9ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1683444418_815f660379.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3733074526_82aa8d5f8d.jpg: 640x352 7 persons, 11.0ms\n",
      "Speed: 1.9ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3733074526_82aa8d5f8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2640153227_57cf1a3d92.jpg: 640x608 1 person, 1 sandwich, 9.6ms\n",
      "Speed: 2.7ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2640153227_57cf1a3d92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590739067_58baffb3a7.jpg: 416x640 2 persons, 1 frisbee, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3590739067_58baffb3a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1554713437_61b64527dd.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1554713437_61b64527dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3270083123_fcc1208053.jpg: 416x640 (no detections), 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 0.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3270083123_fcc1208053.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426933951_2302a941d8.jpg: 384x640 5 persons, 1 sports ball, 9.6ms\n",
      "Speed: 1.9ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3426933951_2302a941d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3366105287_49a4bf71c6.jpg: 448x640 1 person, 1 car, 1 skateboard, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3366105287_49a4bf71c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1426014905_da60d72957.jpg: 640x448 1 person, 10.2ms\n",
      "Speed: 2.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1426014905_da60d72957.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1227655020_b11a1bb112.jpg: 480x640 2 bears, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1227655020_b11a1bb112.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1418266617_b32143275b.jpg: 640x448 1 dog, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1418266617_b32143275b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2465691083_894fc48af6.jpg: 480x640 1 dog, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2465691083_894fc48af6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1019077836_6fc9b15408.jpg: 640x640 1 car, 1 dog, 9.3ms\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1019077836_6fc9b15408.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2885891981_6b02620ae9.jpg: 640x512 1 person, 1 bicycle, 5 cars, 1 motorcycle, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2885891981_6b02620ae9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2543017787_9720b4fa1c.jpg: 544x640 1 person, 1 banana, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2543017787_9720b4fa1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2560388887_55abc9083d.jpg: 480x640 13 persons, 4 kites, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2560388887_55abc9083d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208571574_6dc1a461c5.jpg: 544x640 5 persons, 1 tie, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3208571574_6dc1a461c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/469021173_aa31c07108.jpg: 448x640 1 person, 2 backpacks, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 469021173_aa31c07108.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3391924827_53b31542ce.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3391924827_53b31542ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3553374585_25b1bd6970.jpg: 448x640 8 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3553374585_25b1bd6970.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1000268201_693b08cb0e.jpg: 640x480 1 person, 1 potted plant, 2 toilets, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1000268201_693b08cb0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3039214579_43ef79f931.jpg: 640x416 10 persons, 1 bicycle, 1 fire hydrant, 1 bench, 1 backpack, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3039214579_43ef79f931.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3151365121_e2a685a666.jpg: 480x640 1 dog, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3151365121_e2a685a666.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/746787916_ceb103069f.jpg: 448x640 2 persons, 1 dog, 2 frisbees, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 746787916_ceb103069f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3098714492_19939e3b19.jpg: 512x640 9 persons, 1 tie, 11.6ms\n",
      "Speed: 3.0ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3098714492_19939e3b19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/544301311_5e7d69a517.jpg: 480x640 4 persons, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 544301311_5e7d69a517.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2995935078_beedfe463a.jpg: 544x640 1 person, 2 baseball bats, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2995935078_beedfe463a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/307321761_606fc91673.jpg: 384x640 12 persons, 1 car, 1 dog, 1 cow, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 307321761_606fc91673.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2796801478_8ebd7e550b.jpg: 448x640 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2796801478_8ebd7e550b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2220175999_081aa9cce8.jpg: 288x640 2 cows, 10.0ms\n",
      "Speed: 1.5ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2220175999_081aa9cce8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176278670_195eea071c.jpg: 384x640 6 persons, 1 cup, 2 oranges, 1 dining table, 1 teddy bear, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3176278670_195eea071c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3440724965_03d6ca5399.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3440724965_03d6ca5399.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2266061169_dfbf8f0595.jpg: 640x512 1 person, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2266061169_dfbf8f0595.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3463523977_f2ed231585.jpg: 640x608 5 persons, 1 bench, 2 birds, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3463523977_f2ed231585.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/249394748_2e4acfbbb5.jpg: 448x640 2 dogs, 2 frisbees, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 249394748_2e4acfbbb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582066525_e9d6377f56.jpg: 480x640 2 persons, 2 birds, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3582066525_e9d6377f56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/895502702_5170ada2ee.jpg: 448x640 1 dog, 1 sports ball, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 895502702_5170ada2ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1429546659_44cb09cbe2.jpg: 448x640 2 dogs, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1429546659_44cb09cbe2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244910944_612b8ce98f.jpg: 416x640 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3244910944_612b8ce98f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532457586_bddfc5251d.jpg: 480x640 2 persons, 3 benchs, 1 dog, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 532457586_bddfc5251d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/309771854_952aabe3cc.jpg: 448x640 3 persons, 1 skateboard, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 309771854_952aabe3cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2306674172_dc07c7f847.jpg: 448x640 3 persons, 3 cars, 1 skateboard, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2306674172_dc07c7f847.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/162152393_52ecd33fc5.jpg: 480x640 4 persons, 6 kites, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 162152393_52ecd33fc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/267162122_c3437414ec.jpg: 480x640 1 dog, 1 frisbee, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 267162122_c3437414ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/583174725_6b522b621f.jpg: 480x640 1 person, 2 cars, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 583174725_6b522b621f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084011028_d1e2c40d7d.jpg: 480x640 2 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3084011028_d1e2c40d7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998504949_1022fec53b.jpg: 640x480 3 persons, 1 bus, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2998504949_1022fec53b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2992808092_5f677085b7.jpg: 608x640 2 persons, 1 sports ball, 8.3ms\n",
      "Speed: 2.6ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2992808092_5f677085b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1248940539_46d33ed487.jpg: 640x480 4 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1248940539_46d33ed487.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474999131_788cbf253f.jpg: 448x640 3 persons, 1 dog, 1 chair, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3474999131_788cbf253f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3510695264_ef460fa6cc.jpg: 512x640 6 persons, 1 handbag, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3510695264_ef460fa6cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294964868_16f4f9fa9d.jpg: 448x640 1 person, 1 skis, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3294964868_16f4f9fa9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3718964174_cb2dc1615e.jpg: 448x640 5 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3718964174_cb2dc1615e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220009216_10f088185e.jpg: 480x640 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3220009216_10f088185e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3266406566_d64e57e65a.jpg: 480x640 5 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3266406566_d64e57e65a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/509200598_171a1ab6c8.jpg: 448x640 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 509200598_171a1ab6c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/688210930_85c5675d5b.jpg: 576x640 10 persons, 1 baseball bat, 2 chairs, 1 potted plant, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 688210930_85c5675d5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3115901702_f07aa0ef74.jpg: 640x608 7 persons, 3 handbags, 1 cell phone, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3115901702_f07aa0ef74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563673070_71fa0903ed.jpg: 480x640 1 person, 1 boat, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3563673070_71fa0903ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631671718_d712821757.jpg: 640x448 3 persons, 1 sports ball, 1 baseball bat, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3631671718_d712821757.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245460937_2710a82709.jpg: 640x416 1 person, 1 sports ball, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3245460937_2710a82709.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2986671935_0c60bbb3fa.jpg: 448x640 13 persons, 10 bicycles, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2986671935_0c60bbb3fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3006093003_c211737232.jpg: 640x512 6 persons, 1 tie, 1 snowboard, 2 cell phones, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3006093003_c211737232.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323419265_7fefaa9d5d.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3323419265_7fefaa9d5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/290019324_23582048d4.jpg: 512x640 1 dog, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 290019324_23582048d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106223494_52d4d2d75d.jpg: 640x480 9 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3106223494_52d4d2d75d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561849813_ff9caa52ac.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2561849813_ff9caa52ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/381275595_b429fd1639.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 381275595_b429fd1639.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427852996_d383abd819.jpg: 640x480 1 person, 1 bicycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3427852996_d383abd819.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2358554995_54ed3baa83.jpg: 640x480 1 person, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2358554995_54ed3baa83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091382602_60b9b53ed1.jpg: 640x544 2 persons, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3091382602_60b9b53ed1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224369028_b1ac40d1fa.jpg: 448x640 3 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 224369028_b1ac40d1fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665987581_5e6b0a65f2.jpg: 448x640 1 person, 1 dog, 2 sheeps, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3665987581_5e6b0a65f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/481732592_b50194cb89.jpg: 640x608 5 persons, 1 handbag, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 481732592_b50194cb89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2115849046_2aa9fa8d13.jpg: 416x640 1 boat, 1 dog, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2115849046_2aa9fa8d13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541474181_489f19fae7.jpg: 640x448 2 persons, 1 snowboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3541474181_489f19fae7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189002057_3ef61b803e.jpg: 640x640 2 persons, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3189002057_3ef61b803e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2508249781_36e9282423.jpg: 640x640 1 person, 6.9ms\n",
      "Speed: 2.3ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2508249781_36e9282423.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/391106734_d374bc3080.jpg: 640x480 1 cat, 1 dog, 1 sports ball, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 391106734_d374bc3080.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695001634_127fe2f0d7.jpg: 480x640 4 persons, 4 handbags, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2695001634_127fe2f0d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/392976422_c8d0514bc3.jpg: 576x640 1 person, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 392976422_c8d0514bc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3557295488_600d387347.jpg: 480x640 4 persons, 3 sports balls, 1 baseball bat, 1 baseball glove, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3557295488_600d387347.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336808362_c17837afd8.jpg: 640x448 7 persons, 1 umbrella, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3336808362_c17837afd8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3496028495_431cfdc042.jpg: 416x640 1 dog, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3496028495_431cfdc042.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/521658170_a837af87e9.jpg: 448x640 1 person, 4 dogs, 1 cow, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 521658170_a837af87e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150315970_8f045c41a4.jpg: 640x448 3 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3150315970_8f045c41a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3676788491_01e9bc5f15.jpg: 480x640 1 person, 1 bottle, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3676788491_01e9bc5f15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1055623002_8195a43714.jpg: 480x640 4 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1055623002_8195a43714.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549011001_26cace3646.jpg: 448x640 4 persons, 1 boat, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3549011001_26cace3646.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2479553749_f7ac031940.jpg: 480x640 1 person, 1 car, 1 truck, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2479553749_f7ac031940.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1814086703_33390d5fc7.jpg: 640x640 1 dog, 1 sheep, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1814086703_33390d5fc7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1813266419_08bf66fe98.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1813266419_08bf66fe98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3216926094_bc975e84b9.jpg: 640x416 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3216926094_bc975e84b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/590445887_4d4fa43923.jpg: 480x640 10 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 590445887_4d4fa43923.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/326334188_8850b7bfd4.jpg: 480x640 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 326334188_8850b7bfd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2267923837_ae88678497.jpg: 640x448 5 persons, 11.4ms\n",
      "Speed: 3.0ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2267923837_ae88678497.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2165459064_5b81ff23eb.jpg: 448x640 2 dogs, 10.5ms\n",
      "Speed: 2.4ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2165459064_5b81ff23eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2649850541_59a6c7f01c.jpg: 448x640 2 persons, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2649850541_59a6c7f01c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084018061_df66d98325.jpg: 640x480 3 persons, 1 wine glass, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3084018061_df66d98325.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2797149878_bb8e27ecf9.jpg: 640x448 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2797149878_bb8e27ecf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1087168168_70280d024a.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1087168168_70280d024a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405720825_b6991005eb.jpg: 640x512 1 person, 1 dog, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3405720825_b6991005eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2420696992_22e0dd467d.jpg: 640x480 1 person, 1 motorcycle, 1 bowl, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2420696992_22e0dd467d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3029715635_43ab414dfb.jpg: 512x640 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3029715635_43ab414dfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2666078276_f7b3056997.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2666078276_f7b3056997.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046429283_08de594901.jpg: 640x256 4 persons, 10.5ms\n",
      "Speed: 1.3ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 256)\n",
      "Cropped images saved for 3046429283_08de594901.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428751994_88a6808246.jpg: 512x640 1 dog, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2428751994_88a6808246.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613375729_d0b3c41556.jpg: 448x640 2 persons, 1 motorcycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3613375729_d0b3c41556.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335547029_74d620fa6c.jpg: 640x640 2 persons, 1 surfboard, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3335547029_74d620fa6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185371756_ff4e9fa8a6.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3185371756_ff4e9fa8a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/363617160_6cb0c723be.jpg: 640x480 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 363617160_6cb0c723be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3334866049_f5933344aa.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3334866049_f5933344aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2259336826_0cb294e1f7.jpg: 640x416 2 persons, 2 cars, 1 traffic light, 8.7ms\n",
      "Speed: 1.9ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2259336826_0cb294e1f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2195419145_36722e8d54.jpg: 448x640 3 persons, 1 tie, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2195419145_36722e8d54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244586044_205d5ae2ba.jpg: 448x640 1 dog, 1 cell phone, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3244586044_205d5ae2ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724581378_41049da264.jpg: 448x640 1 person, 1 surfboard, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3724581378_41049da264.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319177177_130a72b8ae.jpg: 480x640 5 persons, 1 bicycle, 2 skateboards, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3319177177_130a72b8ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2904997007_23d4b94101.jpg: 640x352 1 person, 2 beds, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2904997007_23d4b94101.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2570365455_41cc9a7d2b.jpg: 640x480 2 persons, 1 cell phone, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2570365455_41cc9a7d2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3705688385_47651205d3.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3705688385_47651205d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326588088_172d1b2584.jpg: 480x640 2 persons, 1 bottle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3326588088_172d1b2584.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/781387473_208ba152b3.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 781387473_208ba152b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1478294229_7e1c822fea.jpg: 640x448 1 person, 3 bicycles, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1478294229_7e1c822fea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3433259846_800a6079f0.jpg: 576x640 5 persons, 1 umbrella, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3433259846_800a6079f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1394368714_3bc7c19969.jpg: 640x544 6 persons, 1 dog, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1394368714_3bc7c19969.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666574371_317b008d2a.jpg: 640x448 4 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3666574371_317b008d2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/213216174_0632af65a2.jpg: 512x640 3 persons, 2 cars, 1 traffic light, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 213216174_0632af65a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/245252561_4f20f1c89e.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 245252561_4f20f1c89e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1808370027_2088394eb4.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1808370027_2088394eb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/503794526_603a7954d3.jpg: 640x512 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 503794526_603a7954d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1253264731_e7c689eca5.jpg: 480x640 3 persons, 1 chair, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1253264731_e7c689eca5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249865395_dceaa59f54.jpg: 448x640 8 persons, 1 bench, 2 handbags, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3249865395_dceaa59f54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1282392036_5a0328eb86.jpg: 448x640 12 persons, 1 bicycle, 1 handbag, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1282392036_5a0328eb86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/442220883_ff3d6e507f.jpg: 448x640 1 dog, 1 sheep, 1 potted plant, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 442220883_ff3d6e507f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/237547381_aa17c805e0.jpg: 480x640 2 dogs, 2 chairs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 237547381_aa17c805e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2518219912_f47214aa16.jpg: 448x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2518219912_f47214aa16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401333624_4b6af8c1d7.jpg: 448x640 11 persons, 2 snowboards, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3401333624_4b6af8c1d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1378557186_4bd1da6834.jpg: 640x640 1 person, 1 dog, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1378557186_4bd1da6834.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482353373_03a9d5e8bc.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 482353373_03a9d5e8bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3230132205_dccfafa5ee.jpg: 640x480 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3230132205_dccfafa5ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3444684583_6656e38088.jpg: 640x448 1 person, 2 kites, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3444684583_6656e38088.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2449446913_28fa5b7c75.jpg: 448x640 10 persons, 3 baseball gloves, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2449446913_28fa5b7c75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1470536919_1f3fd6c65a.jpg: 448x640 1 dog, 1 bear, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1470536919_1f3fd6c65a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3277162496_dff7eeb59e.jpg: 640x480 1 bird, 2 dogs, 1 bear, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3277162496_dff7eeb59e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2442243868_abe8f74fb4.jpg: 448x640 1 dog, 1 sports ball, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2442243868_abe8f74fb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1132772170_600610c5df.jpg: 448x640 2 persons, 3 beds, 1 remote, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1132772170_600610c5df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873065944_29c01782e2.jpg: 640x544 11 persons, 1 baseball glove, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2873065944_29c01782e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3624676866_8f6d0efcc9.jpg: 448x640 7 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3624676866_8f6d0efcc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/688940111_325a74674a.jpg: 448x640 2 persons, 1 cup, 1 donut, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 688940111_325a74674a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214573346_d3a57f0328.jpg: 640x640 1 dog, 1 bed, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3214573346_d3a57f0328.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421547427_53411b6278.jpg: 640x512 10 persons, 2 tennis rackets, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3421547427_53411b6278.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3280672302_2967177653.jpg: 640x480 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3280672302_2967177653.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2545192257_142fe9e2de.jpg: 480x640 2 dogs, 1 chair, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2545192257_142fe9e2de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616009069_82561da2e5.jpg: 640x640 2 dogs, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2616009069_82561da2e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855695119_4342aae0a3.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2855695119_4342aae0a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677239603_95865a9073.jpg: 448x640 15 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677239603_95865a9073.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3713133789_f05e8daffd.jpg: 512x640 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3713133789_f05e8daffd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300594071_3450444752.jpg: 448x640 1 person, 1 car, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300594071_3450444752.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1355703632_5683a4b6fb.jpg: 640x480 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1355703632_5683a4b6fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1557838421_a33f2a4911.jpg: 544x640 2 dogs, 1 couch, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1557838421_a33f2a4911.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3422219732_3d0be52cc3.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3422219732_3d0be52cc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498354674_b636c7992f.jpg: 448x640 1 person, 1 skateboard, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3498354674_b636c7992f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2658360285_a0ec74ef48.jpg: 448x640 6 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2658360285_a0ec74ef48.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286543624_7a327f79ae.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286543624_7a327f79ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744321686_8811d8428c.jpg: 448x640 1 person, 2 chairs, 1 couch, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2744321686_8811d8428c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/289860281_94d699f36d.jpg: 640x480 4 persons, 1 chair, 1 bed, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 289860281_94d699f36d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2042009399_afad34e7c1.jpg: 448x640 14 persons, 1 baseball glove, 1 chair, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2042009399_afad34e7c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2968693931_52d161b8e7.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2968693931_52d161b8e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424779662_568f9606d0.jpg: 480x640 13 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 424779662_568f9606d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1298295313_db1f4c6522.jpg: 640x512 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1298295313_db1f4c6522.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766291711_4e13a2b594.jpg: 288x640 1 dog, 1 frisbee, 8.3ms\n",
      "Speed: 1.1ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2766291711_4e13a2b594.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3304030264_da3dd18c7b.jpg: 640x448 1 person, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3304030264_da3dd18c7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3302804312_0272091cd5.jpg: 640x640 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3302804312_0272091cd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547704737_57d42d5d9d.jpg: 640x576 9 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3547704737_57d42d5d9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442242092_e579538d82.jpg: 640x576 5 persons, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3442242092_e579538d82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561734666_344f260cce.jpg: 640x448 1 person, 1 baseball glove, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3561734666_344f260cce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1095580424_76f0aa8a3e.jpg: 512x640 1 cat, 1 dog, 1 couch, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1095580424_76f0aa8a3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3090593241_93a975fe2b.jpg: 480x640 7 persons, 2 handbags, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3090593241_93a975fe2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602679255_785b851b46.jpg: 448x640 7 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602679255_785b851b46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345942_ea76966542.jpg: 448x640 8 persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345942_ea76966542.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143688205_630813a466.jpg: 480x640 (no detections), 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 143688205_630813a466.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/299572828_4b38b80d16.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 299572828_4b38b80d16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547524138_4157f660b0.jpg: 576x640 14 persons, 4 cars, 1 tie, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3547524138_4157f660b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2461616306_3ee7ac1b4b.jpg: 640x480 1 person, 1 chair, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2461616306_3ee7ac1b4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3654103642_075f8af4f4.jpg: 256x640 1 person, 1 dog, 1 surfboard, 8.4ms\n",
      "Speed: 1.0ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 3654103642_075f8af4f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540598210_972f0ff573.jpg: 640x480 5 persons, 2 cars, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3540598210_972f0ff573.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/911795495_342bb15b97.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 911795495_342bb15b97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134092148_151154139a.jpg: 480x640 1 person, 2 surfboards, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3134092148_151154139a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609026563_9c66f2dc41.jpg: 640x608 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3609026563_9c66f2dc41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2685752892_9d5cd7f274.jpg: 448x640 4 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2685752892_9d5cd7f274.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1002674143_1b742ab4b8.jpg: 512x640 1 person, 1 kite, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1002674143_1b742ab4b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439243433_d5f3508612.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3439243433_d5f3508612.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183777589_460a4f445b.jpg: 448x640 1 person, 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3183777589_460a4f445b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174228611_6cf9d2266b.jpg: 640x448 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3174228611_6cf9d2266b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/99679241_adc853a5c0.jpg: 480x640 1 bird, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 99679241_adc853a5c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867937005_91c092b157.jpg: 608x640 1 person, 1 couch, 7.6ms\n",
      "Speed: 2.4ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2867937005_91c092b157.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911107495_e3cec16a24.jpg: 448x640 1 person, 2 cars, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2911107495_e3cec16a24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3581451227_618854cea4.jpg: 448x640 2 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3581451227_618854cea4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2574509968_e4692ae169.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2574509968_e4692ae169.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873252292_ebf23f5f10.jpg: 640x448 1 person, 1 bicycle, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2873252292_ebf23f5f10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3625957413_e475943aa3.jpg: 448x640 2 dogs, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3625957413_e475943aa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189941492_a3f4347b1a.jpg: 480x640 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3189941492_a3f4347b1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2872743471_30e0d1a90a.jpg: 480x640 8 persons, 1 car, 11.6ms\n",
      "Speed: 3.3ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2872743471_30e0d1a90a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/599366440_a238e805cf.jpg: 480x640 2 dogs, 1 potted plant, 9.8ms\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 599366440_a238e805cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2842849030_89548af61c.jpg: 480x640 2 persons, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2842849030_89548af61c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3600403707_527aa0596e.jpg: 416x640 3 persons, 6 potted plants, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3600403707_527aa0596e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585598356_8ce815bbb9.jpg: 640x640 2 persons, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3585598356_8ce815bbb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2200901777_f6c168bd32.jpg: 480x640 1 person, 1 tie, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2200901777_f6c168bd32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2522467011_cc825d89ac.jpg: 640x544 1 person, 3 surfboards, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2522467011_cc825d89ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2219805677_7b7cc188c7.jpg: 640x608 1 dog, 1 sports ball, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2219805677_7b7cc188c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2716903793_fb7a3d8ba6.jpg: 640x448 2 persons, 1 bench, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2716903793_fb7a3d8ba6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436395540_63bc8f2fe0.jpg: 448x640 1 person, 6 dogs, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3436395540_63bc8f2fe0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405615014_03be7ef618.jpg: 640x640 3 persons, 4 cars, 9.5ms\n",
      "Speed: 2.9ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 405615014_03be7ef618.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2370221025_be4d9a4431.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2370221025_be4d9a4431.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436313241_6c73153fb6.jpg: 640x448 1 dog, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3436313241_6c73153fb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173014908_b3e69594b6.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3173014908_b3e69594b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2483993827_243894a4f9.jpg: 448x640 2 dogs, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2483993827_243894a4f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3105623068_392b767a7b.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3105623068_392b767a7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2615623392_ab2b9759ae.jpg: 640x448 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2615623392_ab2b9759ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527926597_45af299eee.jpg: 448x640 1 person, 1 skateboard, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3527926597_45af299eee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/508929192_670910fdd2.jpg: 640x448 3 persons, 2 frisbees, 2 tennis rackets, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 508929192_670910fdd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159424456_f316bdc1d5.jpg: 480x640 5 persons, 1 cup, 1 chair, 1 dining table, 12.0ms\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3159424456_f316bdc1d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3069786374_804e1123ac.jpg: 640x480 12 persons, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3069786374_804e1123ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2186139563_e60c1d4b8b.jpg: 448x640 1 dog, 1 sheep, 1 sports ball, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2186139563_e60c1d4b8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346580_b3c035d65c.jpg: 448x640 7 persons, 1 baseball glove, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346580_b3c035d65c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2670637584_d96efb8afa.jpg: 640x480 1 person, 1 cell phone, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2670637584_d96efb8afa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030566410_393c36a6c5.jpg: 512x640 2 persons, 1 surfboard, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3030566410_393c36a6c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3423225860_16e26eef74.jpg: 640x512 1 person, 1 tennis racket, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3423225860_16e26eef74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1461667284_041c8a2475.jpg: 480x640 4 persons, 1 baseball bat, 1 chair, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1461667284_041c8a2475.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/315021440_122d56ebd7.jpg: 448x640 1 dog, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 315021440_122d56ebd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460799229_ce45a1d940.jpg: 480x640 3 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2460799229_ce45a1d940.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1398873613_7e3174dd6c.jpg: 640x512 2 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1398873613_7e3174dd6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2745811124_12c4426b8b.jpg: 448x640 16 persons, 2 sheeps, 2 handbags, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2745811124_12c4426b8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030793171_55cd646eed.jpg: 640x448 1 dog, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3030793171_55cd646eed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3712574653_e009f76d19.jpg: 480x640 7 persons, 6 wine glasss, 2 cups, 1 chair, 2 dining tables, 1 vase, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3712574653_e009f76d19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/888425986_e4b6c12324.jpg: 480x640 1 bird, 10.5ms\n",
      "Speed: 3.0ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 888425986_e4b6c12324.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2543247940_083f1b7969.jpg: 448x640 1 dog, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2543247940_083f1b7969.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/340667199_ecae5f6029.jpg: 640x640 9 persons, 9.1ms\n",
      "Speed: 3.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 340667199_ecae5f6029.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/493507605_48fe8e3739.jpg: 480x640 1 bear, 12.7ms\n",
      "Speed: 3.6ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 493507605_48fe8e3739.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533922605_a2b1e276f6.jpg: 448x640 2 persons, 10.8ms\n",
      "Speed: 2.6ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3533922605_a2b1e276f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/469029994_349e138606.jpg: 448x640 1 person, 1 backpack, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 469029994_349e138606.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2726301121_95a2fbd22b.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2726301121_95a2fbd22b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2488795251_c108c77b13.jpg: 448x640 8 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2488795251_c108c77b13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3616846215_d61881b60f.jpg: 448x640 3 persons, 11.6ms\n",
      "Speed: 2.4ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3616846215_d61881b60f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3043501068_be58ac47e1.jpg: 480x640 1 person, 1 motorcycle, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3043501068_be58ac47e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019473225_8e59b8ec4e.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3019473225_8e59b8ec4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2064417101_3b9d817f4a.jpg: 480x640 2 dogs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2064417101_3b9d817f4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3491607076_922ec561d9.jpg: 640x448 2 persons, 1 snowboard, 1 bottle, 9.5ms\n",
      "Speed: 2.0ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3491607076_922ec561d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1174629344_a2e1a2bdbf.jpg: 448x640 13 persons, 1 traffic light, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1174629344_a2e1a2bdbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2746839158_4195210d27.jpg: 480x640 4 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2746839158_4195210d27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2280354512_c0d035d53f.jpg: 416x640 1 dog, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2280354512_c0d035d53f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2369452202_8b0e8e25ca.jpg: 448x640 8 persons, 1 car, 1 sports ball, 1 kite, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2369452202_8b0e8e25ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/437527058_189f2a7eef.jpg: 640x480 2 persons, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 437527058_189f2a7eef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2100909581_b7dde5b704.jpg: 640x480 1 dog, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2100909581_b7dde5b704.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353950389_1153d5e452.jpg: 480x640 9 persons, 2 bottles, 4 wine glasss, 4 cups, 1 dining table, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3353950389_1153d5e452.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/456512643_0aac2fa9ce.jpg: 192x640 1 dog, 63.4ms\n",
      "Speed: 1.0ms preprocess, 63.4ms inference, 1.2ms postprocess per image at shape (1, 3, 192, 640)\n",
      "Cropped images saved for 456512643_0aac2fa9ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/536495604_b22bbc905a.jpg: 640x480 1 person, 5 cars, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 536495604_b22bbc905a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/141755290_4b954529f3.jpg: 544x640 3 persons, 2 bottles, 1 cup, 2 chairs, 1 potted plant, 7.3ms\n",
      "Speed: 2.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 141755290_4b954529f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2128119486_4407061c40.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2128119486_4407061c40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3347666612_659e6e2207.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3347666612_659e6e2207.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2130203183_49bae96b96.jpg: 480x640 1 dog, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2130203183_49bae96b96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088677667_4a8befb70e.jpg: 576x640 10 persons, 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3088677667_4a8befb70e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/733752482_ee01a419e5.jpg: 448x640 1 cow, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 733752482_ee01a419e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2599444370_9e40103027.jpg: 448x640 1 bird, 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2599444370_9e40103027.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229442620_fd47d01b59.jpg: 320x640 1 bird, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3229442620_fd47d01b59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662570182_350baa020f.jpg: 640x480 3 persons, 3 cars, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2662570182_350baa020f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354200211_35348e47d8.jpg: 480x640 12 persons, 1 frisbee, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3354200211_35348e47d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3098336319_a7e5b061d0.jpg: 640x448 1 person, 1 snowboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3098336319_a7e5b061d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263854883_0f320c1562.jpg: 512x640 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 263854883_0f320c1562.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472678549_67068a1566.jpg: 480x640 1 person, 1 sports ball, 1 apple, 1 toilet, 1 remote, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2472678549_67068a1566.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2637959357_dd64a03efa.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2637959357_dd64a03efa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/391020801_aaaae1e42b.jpg: 480x640 1 person, 1 tie, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 391020801_aaaae1e42b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3177799416_5bd0382370.jpg: 544x640 1 person, 1 snowboard, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3177799416_5bd0382370.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219301553_d2fffe9e0c.jpg: 640x448 8 persons, 1 backpack, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 219301553_d2fffe9e0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348191949_b0b925e5f1.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3348191949_b0b925e5f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2963573792_dd51b5fbfb.jpg: 640x448 1 person, 1 boat, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2963573792_dd51b5fbfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3685373706_37f2ced9ff.jpg: 640x448 6 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3685373706_37f2ced9ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223224391_be50bf4f43.jpg: 640x416 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3223224391_be50bf4f43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576259024_9c05b163aa.jpg: 640x448 2 persons, 1 car, 1 broccoli, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3576259024_9c05b163aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2718495608_d8533e3ac5.jpg: 640x640 3 persons, 1 chair, 7.7ms\n",
      "Speed: 2.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2718495608_d8533e3ac5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244747165_17028936e0.jpg: 448x640 4 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3244747165_17028936e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2913972180_547783dd3d.jpg: 640x416 1 person, 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2913972180_547783dd3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523920786_0eb63993fd.jpg: 416x640 1 dog, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3523920786_0eb63993fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1197800988_7fb0ca4888.jpg: 448x640 1 person, 1 bench, 8.5ms\n",
      "Speed: 1.8ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1197800988_7fb0ca4888.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242354561_54e5a34925.jpg: 448x640 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3242354561_54e5a34925.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122773470_b622205948.jpg: 448x640 8 persons, 1 backpack, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3122773470_b622205948.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2886837407_a4510ab1ef.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2886837407_a4510ab1ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1798209205_77dbf525b0.jpg: 640x448 2 persons, 3 wine glasss, 2 cups, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1798209205_77dbf525b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2044063458_fcc76a7636.jpg: 544x640 3 dogs, 1 sports ball, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2044063458_fcc76a7636.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/483841513_e660391880.jpg: 640x640 1 person, 1 bench, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 483841513_e660391880.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3274375509_4fe91a94c0.jpg: 448x640 3 persons, 2 chairs, 1 dining table, 2 tvs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3274375509_4fe91a94c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1244306891_8e78ae1620.jpg: 480x640 3 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1244306891_8e78ae1620.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224766705_b77996527f.jpg: 480x640 1 truck, 1 bench, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 224766705_b77996527f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3074265400_bf9e10621e.jpg: 544x640 11 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3074265400_bf9e10621e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2362377137_9528692825.jpg: 480x640 7 persons, 3 cars, 2 trucks, 1 handbag, 2 clocks, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2362377137_9528692825.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3594566537_55bd712fdb.jpg: 480x640 1 person, 1 bird, 1 kite, 1 surfboard, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3594566537_55bd712fdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2709359730_13bca100af.jpg: 480x640 4 dogs, 1 sheep, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2709359730_13bca100af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/294549892_babb130543.jpg: 480x640 4 persons, 1 backpack, 1 cell phone, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 294549892_babb130543.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2914800692_0c512d27b8.jpg: 640x448 9 persons, 1 bicycle, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2914800692_0c512d27b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482882307_7dcb9eee11.jpg: 448x640 7 persons, 1 boat, 1 surfboard, 10.2ms\n",
      "Speed: 2.4ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 482882307_7dcb9eee11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677734351_63d60844cb.jpg: 448x640 2 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677734351_63d60844cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/491964988_414b556228.jpg: 608x640 3 persons, 9.0ms\n",
      "Speed: 2.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 491964988_414b556228.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/268365231_a0acecdc45.jpg: 512x640 2 persons, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 268365231_a0acecdc45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/661749711_6f408dad62.jpg: 640x480 1 dog, 1 sports ball, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 661749711_6f408dad62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345323_f53eb5eec4.jpg: 448x640 7 persons, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345323_f53eb5eec4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3144705706_391d7b77c7.jpg: 640x480 3 persons, 1 car, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3144705706_391d7b77c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3689727848_b53f931130.jpg: 448x640 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3689727848_b53f931130.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1337792872_d01a390b33.jpg: 640x640 1 person, 1 teddy bear, 10.7ms\n",
      "Speed: 3.3ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1337792872_d01a390b33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/250406927_a5028a31d4.jpg: 480x640 1 dog, 9.7ms\n",
      "Speed: 2.3ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 250406927_a5028a31d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/700884207_d3ec546494.jpg: 640x448 1 person, 1 bed, 1 remote, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 700884207_d3ec546494.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2190227737_6e0bde2623.jpg: 640x384 1 person, 3 dogs, 10.1ms\n",
      "Speed: 1.8ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2190227737_6e0bde2623.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/181103691_fb2f956abd.jpg: 480x640 18 persons, 1 bicycle, 4 cars, 1 frisbee, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 181103691_fb2f956abd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349451628_4249a21c8f.jpg: 480x640 1 person, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3349451628_4249a21c8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2310108346_e82d209ccd.jpg: 448x640 (no detections), 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2310108346_e82d209ccd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336065481_2c21e622c8.jpg: 576x640 2 persons, 1 boat, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3336065481_2c21e622c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224702241_05af393148.jpg: 480x640 1 bear, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 224702241_05af393148.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3729405438_6e79077ab2.jpg: 640x448 1 person, 1 bicycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3729405438_6e79077ab2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665169936_01ae94c9fd.jpg: 640x448 1 person, 1 bicycle, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3665169936_01ae94c9fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1130017585_1a219257ac.jpg: 448x640 7 persons, 1 car, 1 teddy bear, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1130017585_1a219257ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3680218298_582e6a2289.jpg: 640x448 3 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3680218298_582e6a2289.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/186346360_541047336f.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 186346360_541047336f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3009047603_28612247d2.jpg: 480x640 1 person, 1 sports ball, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3009047603_28612247d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2805873509_4f68afc4b4.jpg: 480x640 1 person, 1 backpack, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2805873509_4f68afc4b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/171488318_fb26af58e2.jpg: 544x640 1 sheep, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 171488318_fb26af58e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3508051251_82422717b3.jpg: 512x640 1 person, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3508051251_82422717b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527590601_38d56abc29.jpg: 448x640 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3527590601_38d56abc29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414397449_2ac3b78e0d.jpg: 480x640 2 dogs, 2 cows, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2414397449_2ac3b78e0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2141713971_e25eb12712.jpg: 448x640 2 persons, 1 dog, 1 skis, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2141713971_e25eb12712.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2446601467_a35841cb1d.jpg: 384x640 1 person, 1 couch, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2446601467_a35841cb1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033825101_44a4390f16.jpg: 640x640 1 person, 1 sports ball, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3033825101_44a4390f16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/753285176_f21a2b984d.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 753285176_f21a2b984d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3685372942_6ae935b34e.jpg: 640x448 3 persons, 1 banana, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3685372942_6ae935b34e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3052196390_c59dd24ca8.jpg: 640x480 7 persons, 11.0ms\n",
      "Speed: 3.0ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3052196390_c59dd24ca8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263497678_8bb688ca01.jpg: 480x640 3 persons, 1 bottle, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3263497678_8bb688ca01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640348910_fcd627ec66.jpg: 448x640 2 persons, 1 kite, 1 clock, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3640348910_fcd627ec66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3172280520_e7655fb596.jpg: 480x640 3 persons, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3172280520_e7655fb596.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1397887419_e798697b93.jpg: 448x640 2 dogs, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1397887419_e798697b93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/621000329_84f48948eb.jpg: 640x448 1 person, 3 chairs, 1 bed, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 621000329_84f48948eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2865409854_afedf98860.jpg: 480x640 18 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2865409854_afedf98860.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1084040636_97d9633581.jpg: 480x640 1 dog, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1084040636_97d9633581.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3127888173_9a9a8ac3bd.jpg: 512x640 7 persons, 2 handbags, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3127888173_9a9a8ac3bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244470342_c08f6bb17e.jpg: 640x448 2 snowboards, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3244470342_c08f6bb17e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3521374954_37371b49a4.jpg: 640x480 2 persons, 1 sheep, 1 bowl, 1 potted plant, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3521374954_37371b49a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604314527_5077cd9d43.jpg: 480x640 2 persons, 1 motorcycle, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3604314527_5077cd9d43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/504385521_6e668691a3.jpg: 448x640 1 dog, 1 bear, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 504385521_6e668691a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2809793070_1a3387cd6e.jpg: 448x640 1 bird, 4 dogs, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2809793070_1a3387cd6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2465218087_fca77998c6.jpg: 448x640 1 person, 1 boat, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2465218087_fca77998c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3652764505_87139e71f8.jpg: 448x640 2 persons, 2 cars, 1 truck, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3652764505_87139e71f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3410899419_4f8dca6f3f.jpg: 480x640 5 persons, 1 cup, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3410899419_4f8dca6f3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3635194562_4c1dfa120a.jpg: 448x640 2 persons, 1 car, 1 dining table, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3635194562_4c1dfa120a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1022454428_b6b660a67b.jpg: 480x640 3 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1022454428_b6b660a67b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532782283_341f0381a3.jpg: 416x640 3 persons, 9.5ms\n",
      "Speed: 1.9ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3532782283_341f0381a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2332540384_4cf26406a9.jpg: 640x608 2 persons, 1 skateboard, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2332540384_4cf26406a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1526181215_c1a94325ae.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1526181215_c1a94325ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461437556_cc5e97f3ac.jpg: 448x640 3 dogs, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3461437556_cc5e97f3ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541483943_9776baee7c.jpg: 480x640 3 persons, 1 skateboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3541483943_9776baee7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2557922709_24d2a9655a.jpg: 640x512 1 person, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2557922709_24d2a9655a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2219805467_370ee1b7aa.jpg: 448x640 1 dog, 1 sports ball, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2219805467_370ee1b7aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2308978137_bfe776d541.jpg: 352x640 17 persons, 1 bicycle, 3 backpacks, 3 handbags, 9.6ms\n",
      "Speed: 1.6ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2308978137_bfe776d541.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300222673_573fd4044b.jpg: 640x608 2 persons, 1 cat, 1 tv, 1 toothbrush, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 300222673_573fd4044b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3132832452_c354c6396c.jpg: 480x640 6 persons, 1 chair, 1 couch, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3132832452_c354c6396c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404870997_7b0cd755de.jpg: 480x640 17 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3404870997_7b0cd755de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2244551043_21b8cca866.jpg: 640x448 5 persons, 1 tennis racket, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2244551043_21b8cca866.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2098174172_e57d86ea03.jpg: 640x480 1 person, 1 backpack, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2098174172_e57d86ea03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397220683_4aca010f86.jpg: 480x640 1 person, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3397220683_4aca010f86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3148647065_2d6cd88cf6.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3148647065_2d6cd88cf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3483140026_e14f64fdf5.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3483140026_e14f64fdf5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470887795_8443ce53d0.jpg: 608x640 7 persons, 2 remotes, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 470887795_8443ce53d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2250870111_8402d2319d.jpg: 640x448 1 person, 1 handbag, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2250870111_8402d2319d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/271660510_dd4ba34b35.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 271660510_dd4ba34b35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2307807200_91fa29cba1.jpg: 640x544 1 person, 1 handbag, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2307807200_91fa29cba1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2593406865_ab98490c1f.jpg: 640x448 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2593406865_ab98490c1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2689358407_9932f1b20c.jpg: 640x448 2 persons, 1 baseball glove, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2689358407_9932f1b20c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2088120475_d6318364f5.jpg: 416x640 4 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2088120475_d6318364f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109823395_6fb423a90f.jpg: 640x448 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 109823395_6fb423a90f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488416045_1c6d903fe0.jpg: 480x640 1 dog, 1 horse, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 488416045_1c6d903fe0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1356796100_b265479721.jpg: 640x448 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1356796100_b265479721.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2063277300_f7ff476914.jpg: 448x640 3 persons, 1 chair, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2063277300_f7ff476914.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3123351642_3794f2f601.jpg: 512x640 1 person, 1 snowboard, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3123351642_3794f2f601.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1526325728_74eb4153d8.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1526325728_74eb4153d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323528927_7b21081271.jpg: 640x448 1 person, 1 couch, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3323528927_7b21081271.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/239453674_0df7767208.jpg: 640x416 6 persons, 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 239453674_0df7767208.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1388373425_3c72b56639.jpg: 640x608 4 persons, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1388373425_3c72b56639.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562282690_cd2a95fe9e.jpg: 640x640 1 person, 1 bird, 1 dog, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3562282690_cd2a95fe9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/462288558_b31a8a976f.jpg: 448x640 1 person, 1 dog, 1 bed, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 462288558_b31a8a976f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2930616480_7fd45ca79b.jpg: 448x640 3 persons, 1 handbag, 1 remote, 1 toothbrush, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2930616480_7fd45ca79b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2304444199_05386d2e9c.jpg: 640x448 1 person, 1 bed, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2304444199_05386d2e9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1357724865_4faf4e1418.jpg: 480x640 1 person, 12.6ms\n",
      "Speed: 1.7ms preprocess, 12.6ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1357724865_4faf4e1418.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2759813381_73303113d9.jpg: 640x480 1 bird, 1 dog, 12.0ms\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2759813381_73303113d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543363241_74d8246fab.jpg: 640x384 1 person, 11.0ms\n",
      "Speed: 2.4ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 543363241_74d8246fab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2950393735_9969c4ec59.jpg: 640x448 1 person, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2950393735_9969c4ec59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/172092461_a9a9762e13.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 172092461_a9a9762e13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690425778_3b390b3ea5.jpg: 640x448 1 person, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3690425778_3b390b3ea5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568225554_73cdb19576.jpg: 640x448 2 persons, 1 surfboard, 10.6ms\n",
      "Speed: 2.6ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3568225554_73cdb19576.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2882483779_73c171ac19.jpg: 448x640 1 cat, 2 dogs, 1 chair, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2882483779_73c171ac19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1956944011_c5661d3f22.jpg: 640x448 2 persons, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1956944011_c5661d3f22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2737609659_efce872c24.jpg: 640x480 3 persons, 1 truck, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2737609659_efce872c24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2252635585_b48b3485b0.jpg: 576x640 (no detections), 9.5ms\n",
      "Speed: 2.9ms preprocess, 9.5ms inference, 0.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2252635585_b48b3485b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214579977_fa9fb006a6.jpg: 640x448 1 bird, 1 dog, 11.3ms\n",
      "Speed: 2.0ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3214579977_fa9fb006a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/473988700_570422001b.jpg: 640x480 1 dog, 2 bears, 11.1ms\n",
      "Speed: 2.7ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 473988700_570422001b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2326730558_75c20e5033.jpg: 640x448 1 dog, 1 horse, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2326730558_75c20e5033.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2644916196_16f91dae54.jpg: 640x448 1 person, 1 cup, 1 chair, 1 dining table, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2644916196_16f91dae54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1048710776_bb5b0a5c7c.jpg: 448x640 14 persons, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1048710776_bb5b0a5c7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2562166462_b43b141d40.jpg: 416x640 1 person, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2562166462_b43b141d40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/608257195_6ec6f48e37.jpg: 640x480 1 person, 1 bicycle, 10.4ms\n",
      "Speed: 2.7ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 608257195_6ec6f48e37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2249480913_e1695e5c28.jpg: 512x640 2 dogs, 1 frisbee, 13.3ms\n",
      "Speed: 3.9ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2249480913_e1695e5c28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/389643437_9a9830a3ba.jpg: 480x640 2 persons, 2 hot dogs, 11.2ms\n",
      "Speed: 2.9ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 389643437_9a9830a3ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088322308_b0c940b3a3.jpg: 480x640 4 persons, 1 bicycle, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3088322308_b0c940b3a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3137061312_eb5fdcf3fd.jpg: 384x640 3 persons, 1 train, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3137061312_eb5fdcf3fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/132489044_3be606baf7.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 132489044_3be606baf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1354318519_2f9baed754.jpg: 448x640 2 persons, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1354318519_2f9baed754.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2211593099_4a4f1c85d2.jpg: 480x640 10 persons, 1 umbrella, 12.0ms\n",
      "Speed: 3.1ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2211593099_4a4f1c85d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2548777800_d7b9cf1c2b.jpg: 640x640 3 persons, 9.8ms\n",
      "Speed: 3.2ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2548777800_d7b9cf1c2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2067362863_59577f9d4d.jpg: 608x640 1 person, 2 dogs, 8.9ms\n",
      "Speed: 2.7ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2067362863_59577f9d4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220151692_d398ef9779.jpg: 640x576 4 persons, 8.9ms\n",
      "Speed: 2.5ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3220151692_d398ef9779.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089539651_9e518ec7de.jpg: 480x640 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2089539651_9e518ec7de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354883962_170d19bfe4.jpg: 416x640 1 dog, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3354883962_170d19bfe4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217187564_0ffd89dec1.jpg: 448x640 2 birds, 1 dog, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3217187564_0ffd89dec1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2489602993_896c1ea40a.jpg: 448x640 1 person, 1 motorcycle, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2489602993_896c1ea40a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/127490019_7c5c08cb11.jpg: 640x480 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 127490019_7c5c08cb11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921793132_ef19f1dd44.jpg: 448x640 7 persons, 1 sports ball, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2921793132_ef19f1dd44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524105255_b346f288be.jpg: 544x640 1 person, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 524105255_b346f288be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3759492488_592cd78ed1.jpg: 640x544 7 persons, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3759492488_592cd78ed1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3177468217_56a9142e46.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3177468217_56a9142e46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189521080_133777bce5.jpg: 448x640 (no detections), 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3189521080_133777bce5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246231741_882b45c4e1.jpg: 480x640 2 dogs, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 246231741_882b45c4e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1235685934_be89b231fb.jpg: 448x640 2 persons, 1 chair, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1235685934_be89b231fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2125216241_5b265a2fbc.jpg: 640x544 4 persons, 1 boat, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2125216241_5b265a2fbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2780105274_52360c4cca.jpg: 640x480 2 persons, 1 handbag, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2780105274_52360c4cca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014015906_fdba461f36.jpg: 480x640 3 persons, 11.2ms\n",
      "Speed: 2.9ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3014015906_fdba461f36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3634785801_4b23184a06.jpg: 416x640 2 dogs, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3634785801_4b23184a06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444481722_690d0cadcf.jpg: 448x640 2 persons, 1 handbag, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 444481722_690d0cadcf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506882688_b37d549593.jpg: 448x640 2 persons, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 506882688_b37d549593.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3478176372_7c510a0cef.jpg: 640x640 8 persons, 1 sports ball, 9.0ms\n",
      "Speed: 2.8ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3478176372_7c510a0cef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1975531316_8b00eeaaf7.jpg: 480x640 4 persons, 1 dog, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1975531316_8b00eeaaf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2195620255_6693479734.jpg: 640x576 3 dogs, 9.1ms\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2195620255_6693479734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/290650302_ade636da35.jpg: 448x640 6 persons, 2 chairs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 290650302_ade636da35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2476214153_99a3998509.jpg: 448x640 1 person, 2 sports balls, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2476214153_99a3998509.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/615916000_5044047d71.jpg: 640x480 7 persons, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 615916000_5044047d71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516998046_1175674fcd.jpg: 608x640 1 dog, 9.3ms\n",
      "Speed: 3.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 516998046_1175674fcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3664297064_a4d45cbbbc.jpg: 576x640 11 persons, 9.1ms\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3664297064_a4d45cbbbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374103842_17873ce505.jpg: 640x512 2 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 374103842_17873ce505.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249014584_21dd9ddd9d.jpg: 448x640 10 persons, 1 skis, 3 snowboards, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3249014584_21dd9ddd9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/365274901_576b0f8241.jpg: 512x640 1 cat, 2 dogs, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 365274901_576b0f8241.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855594918_1d1e6a6061.jpg: 640x480 1 person, 1 bicycle, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2855594918_1d1e6a6061.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766630484_ce73f47031.jpg: 640x448 1 dog, 1 sheep, 1 cow, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2766630484_ce73f47031.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245266444_2e798096e6.jpg: 448x640 2 persons, 1 chair, 1 bed, 1 teddy bear, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3245266444_2e798096e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3006217970_90b42e6b27.jpg: 448x640 1 car, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3006217970_90b42e6b27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2896668718_0c3cff910f.jpg: 640x512 6 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2896668718_0c3cff910f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2939464283_fc1a834976.jpg: 448x640 8 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2939464283_fc1a834976.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1130401779_8c30182e3e.jpg: 448x640 1 dog, 1 elephant, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1130401779_8c30182e3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3086523890_fd9394af8b.jpg: 480x640 2 dogs, 1 sports ball, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3086523890_fd9394af8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3199645963_a681fe04f8.jpg: 640x640 4 persons, 2 umbrellas, 2 chairs, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3199645963_a681fe04f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2878272032_fda05ffac7.jpg: 480x640 2 dogs, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2878272032_fda05ffac7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2538423833_d1f492d1fb.jpg: 480x640 4 persons, 1 dog, 1 chair, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2538423833_d1f492d1fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/970641406_9a20ee636a.jpg: 640x640 5 persons, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 970641406_9a20ee636a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672109677_8caa992671.jpg: 608x640 6 persons, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3672109677_8caa992671.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3086526292_f799d237c7.jpg: 448x640 3 persons, 1 bottle, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3086526292_f799d237c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/167295035_336f5f5f27.jpg: 448x640 2 persons, 7.5ms\n",
      "Speed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 167295035_336f5f5f27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/911527312_f81ee36b97.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 911527312_f81ee36b97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2712352554_1cafd32812.jpg: 640x448 1 person, 1 skateboard, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2712352554_1cafd32812.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3086790344_9487c58624.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3086790344_9487c58624.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2137071442_1c9658c81a.jpg: 640x640 1 person, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2137071442_1c9658c81a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708839890_ed448012cf.jpg: 640x640 1 bird, 6.7ms\n",
      "Speed: 2.3ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3708839890_ed448012cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3332202255_a30c522664.jpg: 480x640 3 persons, 2 bottles, 16.8ms\n",
      "Speed: 3.7ms preprocess, 16.8ms inference, 2.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3332202255_a30c522664.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1679617928_a73c1769be.jpg: 640x448 1 dog, 13.6ms\n",
      "Speed: 3.7ms preprocess, 13.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1679617928_a73c1769be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636055584_65a60426f8.jpg: 512x640 9 persons, 1 sports ball, 2 tennis rackets, 11.4ms\n",
      "Speed: 3.3ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3636055584_65a60426f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3362871440_6c0f27c480.jpg: 448x640 2 cows, 1 bear, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3362871440_6c0f27c480.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3465791729_5bf9bd8635.jpg: 448x640 8 persons, 2 umbrellas, 1 chair, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3465791729_5bf9bd8635.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/854333409_38bc1da9dc.jpg: 512x640 2 dogs, 1 frisbee, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 854333409_38bc1da9dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3411595210_8e0893b266.jpg: 480x640 1 bear, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3411595210_8e0893b266.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2684489465_32ba1d0344.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2684489465_32ba1d0344.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2076428547_738e0a132f.jpg: 416x640 1 person, 1 backpack, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2076428547_738e0a132f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2799871904_3b3125518a.jpg: 512x640 1 person, 1 chair, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2799871904_3b3125518a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1329832826_432538d331.jpg: 448x640 5 persons, 5 cars, 1 traffic light, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1329832826_432538d331.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527715826_ea5b4e8de4.jpg: 448x640 1 person, 1 sports ball, 2 tennis rackets, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3527715826_ea5b4e8de4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621177753_1718c30ea0.jpg: 640x640 10 persons, 1 frisbee, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3621177753_1718c30ea0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884651479_12e400ee58.jpg: 480x640 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2884651479_12e400ee58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2204695848_3d1b140212.jpg: 640x640 1 person, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2204695848_3d1b140212.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2529205842_bdcb49d65b.jpg: 448x640 4 persons, 1 frisbee, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2529205842_bdcb49d65b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2635483351_bc1a8273aa.jpg: 480x640 1 person, 3 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2635483351_bc1a8273aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347243_c751557497.jpg: 448x640 6 persons, 1 sports ball, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347243_c751557497.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649224118_abe73c672c.jpg: 448x640 5 persons, 1 sports ball, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3649224118_abe73c672c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3446191973_1db572ed8a.jpg: 448x640 6 persons, 1 traffic light, 1 parking meter, 1 frisbee, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3446191973_1db572ed8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2167644298_100ca79f54.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2167644298_100ca79f54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2777021428_0b2ac3e987.jpg: 640x448 1 person, 1 sports ball, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2777021428_0b2ac3e987.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502007071_82a8c639cf.jpg: 480x640 5 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2502007071_82a8c639cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1007129816_e794419615.jpg: 608x640 2 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1007129816_e794419615.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/328916930_e4d4be1730.jpg: 448x640 6 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 328916930_e4d4be1730.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3210359094_ee51285301.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3210359094_ee51285301.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1752454466_723790dbd6.jpg: 640x544 1 dog, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1752454466_723790dbd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2497608431_8dfefc7a1a.jpg: 640x480 1 person, 1 dog, 1 baseball bat, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2497608431_8dfefc7a1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3257182199_5fda78d870.jpg: 384x640 13 persons, 1 surfboard, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3257182199_5fda78d870.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1348113612_5bfc5f429e.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1348113612_5bfc5f429e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183195185_cd0ff994a1.jpg: 480x640 2 persons, 1 backpack, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3183195185_cd0ff994a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/578644583_da3ff18dd1.jpg: 640x480 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 578644583_da3ff18dd1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697359692_8a5cdbe4fe.jpg: 448x640 1 person, 2 cars, 1 chair, 1 cell phone, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3697359692_8a5cdbe4fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2354792215_eef2bdc753.jpg: 448x640 6 persons, 2 sports balls, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2354792215_eef2bdc753.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860035355_3fe7a5caa4.jpg: 480x640 2 persons, 2 cars, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2860035355_3fe7a5caa4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911245290_b2c79f328a.jpg: 480x640 1 person, 1 surfboard, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2911245290_b2c79f328a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/466956209_2ffcea3941.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 466956209_2ffcea3941.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335370208_460fc19bfa.jpg: 448x640 1 person, 1 motorcycle, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3335370208_460fc19bfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2710698257_2e4ca8dd44.jpg: 384x640 1 person, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2710698257_2e4ca8dd44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2124040721_bffc0a091a.jpg: 608x640 1 person, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2124040721_bffc0a091a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534952095_975cca0056.jpg: 448x640 6 persons, 1 motorcycle, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3534952095_975cca0056.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2947274789_a1a35b33c3.jpg: 480x640 2 persons, 1 cell phone, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2947274789_a1a35b33c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3202360797_2084743e90.jpg: 640x576 1 dog, 1 bed, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3202360797_2084743e90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1258913059_07c613f7ff.jpg: 384x640 5 persons, 6 benchs, 3 umbrellas, 1 bottle, 2 chairs, 2 potted plants, 1 clock, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1258913059_07c613f7ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/487894806_352d9b5e66.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 487894806_352d9b5e66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3058439373_9276a4702a.jpg: 480x640 12 persons, 3 handbags, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3058439373_9276a4702a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3455405300_aa3069ecaa.jpg: 640x448 7 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3455405300_aa3069ecaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401548798_3a93f2caa5.jpg: 480x640 2 persons, 1 snowboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3401548798_3a93f2caa5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042380610_c5ea61eef8.jpg: 480x640 9 persons, 2 bottles, 1 chair, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3042380610_c5ea61eef8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/617038406_4092ee91dd.jpg: 640x512 7 persons, 2 cars, 1 tie, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 617038406_4092ee91dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286761458_34af7e4499.jpg: 448x640 6 persons, 1 snowboard, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3286761458_34af7e4499.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3494345896_dd6b32cfa3.jpg: 640x480 1 dog, 1 cow, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3494345896_dd6b32cfa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3462396164_ba9849c14b.jpg: 480x640 18 persons, 1 traffic light, 1 dog, 1 umbrella, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3462396164_ba9849c14b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/197107117_4b438b1872.jpg: 480x640 1 bird, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 197107117_4b438b1872.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877159456_ea4a46b0d2.jpg: 448x640 2 persons, 3 bicycles, 1 car, 1 bottle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2877159456_ea4a46b0d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514222285_aa0c8d05b7.jpg: 640x480 7 persons, 1 skis, 1 snowboard, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 514222285_aa0c8d05b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3624327440_bef4f33f32.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3624327440_bef4f33f32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1342780478_bacc32344d.jpg: 640x480 4 persons, 1 car, 1 tie, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1342780478_bacc32344d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/701816897_221bbe761a.jpg: 480x640 4 persons, 4 surfboards, 1 chair, 1 dining table, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 701816897_221bbe761a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474985112_24ef46e82d.jpg: 640x448 1 person, 1 bicycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3474985112_24ef46e82d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2759596272_e0ce0a965a.jpg: 640x448 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2759596272_e0ce0a965a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524282699_71e678a6bd.jpg: 480x640 2 dogs, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 524282699_71e678a6bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2268601066_b018b124fd.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2268601066_b018b124fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2285152690_3fb93f65f1.jpg: 448x640 3 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2285152690_3fb93f65f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2501742763_b2cb322087.jpg: 480x640 2 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2501742763_b2cb322087.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1572532018_64c030c974.jpg: 480x640 3 persons, 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1572532018_64c030c974.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300314926_0b2e4b64f5.jpg: 448x640 1 person, 1 surfboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300314926_0b2e4b64f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1153704539_542f7aa3a5.jpg: 640x544 5 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1153704539_542f7aa3a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3713882697_6dd30c7505.jpg: 640x640 1 person, 1 hot dog, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3713882697_6dd30c7505.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371887001_44ab0c2f17.jpg: 448x640 1 person, 1 bicycle, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3371887001_44ab0c2f17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396153660_f729d9f9b9.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3396153660_f729d9f9b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/732468337_a37075225e.jpg: 640x640 2 persons, 7.4ms\n",
      "Speed: 2.4ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 732468337_a37075225e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1394620454_bf708cc501.jpg: 448x640 1 person, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1394620454_bf708cc501.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534046564_4f8546e364.jpg: 384x640 2 dogs, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3534046564_4f8546e364.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396817186_b299ee0531.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3396817186_b299ee0531.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527261343_efa07ea596.jpg: 640x544 1 person, 2 motorcycles, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3527261343_efa07ea596.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1928319708_ccf1f4ee72.jpg: 480x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1928319708_ccf1f4ee72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3635991166_f95304af0a.jpg: 480x640 8 persons, 2 cars, 1 frisbee, 2 sports balls, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3635991166_f95304af0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2591455200_2319651f2f.jpg: 480x640 1 person, 1 bed, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2591455200_2319651f2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3377570617_d2f2225a74.jpg: 480x640 2 persons, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3377570617_d2f2225a74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2834752476_3177e617f1.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2834752476_3177e617f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/190965502_0b9ed331d9.jpg: 640x448 3 persons, 2 boats, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 190965502_0b9ed331d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273969811_42e9fa8f63.jpg: 480x640 1 bird, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3273969811_42e9fa8f63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2510029990_7014f907cb.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2510029990_7014f907cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155365418_43df5486f9.jpg: 640x288 1 person, 1 snowboard, 8.3ms\n",
      "Speed: 1.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Cropped images saved for 3155365418_43df5486f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3542771548_fcb8fa0cba.jpg: 640x608 2 persons, 1 bench, 1 horse, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3542771548_fcb8fa0cba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3713922357_e0a013fb97.jpg: 640x448 2 persons, 1 motorcycle, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3713922357_e0a013fb97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3303648823_53cf750acd.jpg: 640x448 1 person, 1 skis, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3303648823_53cf750acd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2448793019_5881c025f9.jpg: 640x480 1 person, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2448793019_5881c025f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2252403744_148fc11f68.jpg: 384x640 3 persons, 1 sports ball, 1 keyboard, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2252403744_148fc11f68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3387661249_33e5ba0bc5.jpg: 448x640 2 horses, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3387661249_33e5ba0bc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3255482333_5bcee79f7e.jpg: 448x640 1 person, 15.7ms\n",
      "Speed: 3.9ms preprocess, 15.7ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3255482333_5bcee79f7e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/427557693_1108566fd2.jpg: 448x640 2 persons, 1 car, 12.7ms\n",
      "Speed: 2.6ms preprocess, 12.7ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 427557693_1108566fd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353400143_8b9543f7dc.jpg: 448x640 1 person, 1 motorcycle, 11.2ms\n",
      "Speed: 2.2ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3353400143_8b9543f7dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1434607942_da5432c28c.jpg: 640x448 2 persons, 12.5ms\n",
      "Speed: 2.1ms preprocess, 12.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1434607942_da5432c28c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1674612291_7154c5ab61.jpg: 512x640 1 dog, 12.6ms\n",
      "Speed: 2.4ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1674612291_7154c5ab61.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2470519275_65725fd38d.jpg: 640x480 4 persons, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2470519275_65725fd38d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2848977044_446a31d86e.jpg: 576x640 16 persons, 3 horses, 12.1ms\n",
      "Speed: 2.9ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2848977044_446a31d86e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3306212559_731ba9bd05.jpg: 384x640 5 persons, 11.6ms\n",
      "Speed: 1.8ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3306212559_731ba9bd05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3216085740_699c2ce1ae.jpg: 448x640 4 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3216085740_699c2ce1ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2940594396_20c40947b0.jpg: 640x448 7 persons, 1 bicycle, 11.2ms\n",
      "Speed: 2.1ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2940594396_20c40947b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537230454_1f09199476.jpg: 448x640 4 persons, 11.3ms\n",
      "Speed: 2.0ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 537230454_1f09199476.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1563731247_7f21d8bec0.jpg: 640x448 2 persons, 1 kite, 13.9ms\n",
      "Speed: 3.7ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1563731247_7f21d8bec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490517179_76dbd690de.jpg: 640x448 6 persons, 11.2ms\n",
      "Speed: 3.1ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3490517179_76dbd690de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2259203920_6b93b721ce.jpg: 352x640 1 dog, 12.0ms\n",
      "Speed: 1.8ms preprocess, 12.0ms inference, 1.6ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2259203920_6b93b721ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3725814794_30db172f67.jpg: 448x640 3 persons, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3725814794_30db172f67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107463441_7c68606450.jpg: 480x640 2 persons, 12.6ms\n",
      "Speed: 2.8ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3107463441_7c68606450.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2208631481_3e4a5675e1.jpg: 448x640 2 dogs, 10.3ms\n",
      "Speed: 2.2ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2208631481_3e4a5675e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/642987597_03b21a1437.jpg: 480x640 2 persons, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 642987597_03b21a1437.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934121315_4969eeda1b.jpg: 480x640 1 dog, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2934121315_4969eeda1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085403342_a17b0654fe.jpg: 512x640 1 person, 1 bench, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2085403342_a17b0654fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2980348138_91cc6f6d0f.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2980348138_91cc6f6d0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197981073_3156963446.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3197981073_3156963446.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/525863257_053333e612.jpg: 512x640 1 person, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 525863257_053333e612.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2420730259_86e7f8a815.jpg: 640x448 7 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2420730259_86e7f8a815.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911238432_33ec2d8cec.jpg: 480x640 5 persons, 3 surfboards, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2911238432_33ec2d8cec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183883750_b6acc40397.jpg: 480x640 2 persons, 1 chair, 1 laptop, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3183883750_b6acc40397.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2277299634_e14bdb7ff7.jpg: 416x640 7 persons, 2 cars, 2 traffic lights, 9.8ms\n",
      "Speed: 1.9ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2277299634_e14bdb7ff7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1433577867_39a1510c43.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1433577867_39a1510c43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3051384385_c5c850c1f8.jpg: 480x640 1 person, 1 chair, 1 dining table, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3051384385_c5c850c1f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/478754346_addb53893c.jpg: 640x448 1 person, 1 bicycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 478754346_addb53893c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3074617663_2f2634081d.jpg: 480x640 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3074617663_2f2634081d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537452619_3bd79f24e0.jpg: 384x640 1 person, 2 benchs, 1 backpack, 1 skateboard, 9.8ms\n",
      "Speed: 1.8ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3537452619_3bd79f24e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627076769_3b71e73018.jpg: 416x640 2 persons, 1 backpack, 1 bottle, 1 cell phone, 13.2ms\n",
      "Speed: 3.2ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3627076769_3b71e73018.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185662156_c877583c53.jpg: 384x640 1 person, 1 snowboard, 11.7ms\n",
      "Speed: 2.3ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3185662156_c877583c53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2256320794_0286c31bfa.jpg: 448x640 1 dog, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2256320794_0286c31bfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503471307_464a8f588c.jpg: 448x640 1 person, 1 bicycle, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3503471307_464a8f588c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/610590753_cd69ce081a.jpg: 640x640 1 dog, 10.0ms\n",
      "Speed: 2.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 610590753_cd69ce081a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612249030_e2829ffa31.jpg: 384x640 1 person, 10.1ms\n",
      "Speed: 1.7ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3612249030_e2829ffa31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3446586125_cafa0bfd67.jpg: 640x448 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3446586125_cafa0bfd67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3481859121_3d3e566ec0.jpg: 416x640 6 persons, 2 backpacks, 2 handbags, 9.1ms\n",
      "Speed: 1.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3481859121_3d3e566ec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2678798732_2998f9969c.jpg: 640x640 1 person, 9.2ms\n",
      "Speed: 3.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2678798732_2998f9969c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320411267_df70b90501.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3320411267_df70b90501.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/861661418_8a37024ace.jpg: 640x576 1 dog, 9.8ms\n",
      "Speed: 2.6ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 861661418_8a37024ace.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2933637854_984614e18b.jpg: 640x512 5 persons, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2933637854_984614e18b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2532262109_87429a2cae.jpg: 480x640 15 persons, 3 cars, 1 handbag, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2532262109_87429a2cae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1874617189_e85d3f4326.jpg: 480x640 5 persons, 1 boat, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1874617189_e85d3f4326.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482098572_e83153b300.jpg: 416x640 1 person, 1 surfboard, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 482098572_e83153b300.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3317333893_9d0faa8d30.jpg: 448x640 1 giraffe, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3317333893_9d0faa8d30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2301867590_98c0ecb0cb.jpg: 320x640 2 persons, 10.6ms\n",
      "Speed: 1.5ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2301867590_98c0ecb0cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236095034_c983bdfbbf.jpg: 480x640 3 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 236095034_c983bdfbbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215081286_d55541aa6b.jpg: 480x640 7 persons, 2 parking meters, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3215081286_d55541aa6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535123126_c06c1ab9bf.jpg: 608x640 1 dog, 9.8ms\n",
      "Speed: 2.7ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 535123126_c06c1ab9bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2320125735_27fe729948.jpg: 480x640 1 dog, 1 sports ball, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2320125735_27fe729948.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458625738_297857369c.jpg: 448x640 4 persons, 1 tie, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3458625738_297857369c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534875358_6ea30d3091.jpg: 640x448 1 person, 1 handbag, 1 baseball glove, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 534875358_6ea30d3091.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1236951314_0308dc4138.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1236951314_0308dc4138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2814028429_561a215259.jpg: 640x448 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2814028429_561a215259.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3054997030_797096dd12.jpg: 640x448 1 person, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3054997030_797096dd12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3333675897_0043f992d3.jpg: 640x448 1 person, 1 dog, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3333675897_0043f992d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3131107810_7e9b96cddc.jpg: 416x640 5 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3131107810_7e9b96cddc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2533076864_d799996433.jpg: 416x640 2 persons, 1 parking meter, 6.4ms\n",
      "Speed: 1.4ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2533076864_d799996433.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3018847610_0bf4d7e43d.jpg: 640x448 7 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3018847610_0bf4d7e43d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/121971540_0a986ee176.jpg: 640x480 2 persons, 1 boat, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 121971540_0a986ee176.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2434006663_207a284cec.jpg: 384x640 2 dogs, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2434006663_207a284cec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3707283973_5cdaa39340.jpg: 448x640 3 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3707283973_5cdaa39340.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524698457_77ba13840a.jpg: 512x640 2 dogs, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 524698457_77ba13840a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442622076_c3abe955e5.jpg: 448x640 11 persons, 2 handbags, 1 tie, 1 tv, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3442622076_c3abe955e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2729655904_1dd01922fb.jpg: 448x640 2 persons, 2 dogs, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2729655904_1dd01922fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2895700779_fac1d9d278.jpg: 480x640 4 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2895700779_fac1d9d278.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899089320_3e7f6bbaca.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2899089320_3e7f6bbaca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671933270_d124e9a1a4.jpg: 448x640 1 person, 1 bicycle, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3671933270_d124e9a1a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3580741947_cc64a83648.jpg: 480x640 2 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3580741947_cc64a83648.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2147199188_d2d70b88ec.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2147199188_d2d70b88ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/562928217_21f967a807.jpg: 640x416 3 dogs, 1 frisbee, 1 sports ball, 7.5ms\n",
      "Speed: 1.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 562928217_21f967a807.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2951750234_a4741f708b.jpg: 448x640 3 persons, 1 car, 1 horse, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2951750234_a4741f708b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1721637099_93e9ec2a2f.jpg: 448x640 1 person, 1 bed, 1 cell phone, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1721637099_93e9ec2a2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/272988646_1588bde6a8.jpg: 640x544 8 persons, 2 handbags, 1 potted plant, 1 cell phone, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 272988646_1588bde6a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676015068_690b0fb2cd.jpg: 480x640 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2676015068_690b0fb2cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1937104503_313d22a2d0.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1937104503_313d22a2d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325974730_3ee192e4ff.jpg: 448x640 1 person, 1 snowboard, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3325974730_3ee192e4ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/979201222_75b6456d34.jpg: 640x384 12 persons, 1 car, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 979201222_75b6456d34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374828031_9d087da5cf.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 374828031_9d087da5cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/578274277_652cae32ba.jpg: 640x448 1 person, 1 frisbee, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 578274277_652cae32ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183519385_311555d5f5.jpg: 512x640 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3183519385_311555d5f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442540072_b22ca2410f.jpg: 480x640 1 person, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3442540072_b22ca2410f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1523984678_edd68464da.jpg: 640x448 1 dog, 1 frisbee, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1523984678_edd68464da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1670592963_39731a3dac.jpg: 608x640 1 dog, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1670592963_39731a3dac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/222369445_5b6af347dd.jpg: 480x640 2 persons, 1 surfboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 222369445_5b6af347dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/956164675_9ee084364e.jpg: 640x576 2 persons, 1 skateboard, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 956164675_9ee084364e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2498897831_0bbb5d5b51.jpg: 448x640 4 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2498897831_0bbb5d5b51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490528249_6aae9b867b.jpg: 448x640 3 trucks, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3490528249_6aae9b867b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/771366843_a66304161b.jpg: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 771366843_a66304161b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344948183_5b89379585.jpg: 480x640 2 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3344948183_5b89379585.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2760371526_63f3d01760.jpg: 512x640 1 person, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2760371526_63f3d01760.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3017220118_6a9212dfdb.jpg: 448x640 6 persons, 1 backpack, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3017220118_6a9212dfdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2419221084_01a14176b4.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2419221084_01a14176b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/289616152_012a9f16c6.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 289616152_012a9f16c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576741633_671340544c.jpg: 640x448 7 persons, 1 bird, 1 skateboard, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3576741633_671340544c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695085862_2ed62df354.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2695085862_2ed62df354.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339586622_a7676b30e1.jpg: 512x640 1 person, 1 surfboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3339586622_a7676b30e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/446514680_ff5ca15ece.jpg: 448x640 2 dogs, 1 horse, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 446514680_ff5ca15ece.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3452982513_36f2bc81fa.jpg: 384x640 1 person, 1 snowboard, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3452982513_36f2bc81fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2718024196_3ff660416a.jpg: 640x640 2 persons, 1 surfboard, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2718024196_3ff660416a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3529211822_1dabdb3a9c.jpg: 448x640 2 dogs, 1 cow, 1 bowl, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3529211822_1dabdb3a9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/439037721_cdf1fc7358.jpg: 416x640 3 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 439037721_cdf1fc7358.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1808504612_3508f3c9bb.jpg: 480x640 (no detections), 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1808504612_3508f3c9bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2808098783_c56b44befa.jpg: 480x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2808098783_c56b44befa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2418191216_82711d5c5c.jpg: 448x640 1 person, 1 baseball glove, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2418191216_82711d5c5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3292277400_f95cdd13d1.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3292277400_f95cdd13d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405100926_e96308ce89.jpg: 448x640 6 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3405100926_e96308ce89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860872588_f2c7b30e1a.jpg: 448x640 2 dogs, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2860872588_f2c7b30e1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3551281733_b43bf6f870.jpg: 640x512 1 bird, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3551281733_b43bf6f870.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/318667317_108c402140.jpg: 544x640 1 person, 1 traffic light, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 318667317_108c402140.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/525968880_82623392d1.jpg: 480x640 1 fire hydrant, 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 525968880_82623392d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/946051430_8db7e4ce09.jpg: 384x640 5 persons, 1 skateboard, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 946051430_8db7e4ce09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3257277774_aba333a94c.jpg: 352x640 1 dog, 8.1ms\n",
      "Speed: 1.3ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3257277774_aba333a94c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582048078_7bac2d8473.jpg: 448x640 2 persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3582048078_7bac2d8473.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924908529_0ecb3cdbaa.jpg: 640x480 6 persons, 1 umbrella, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2924908529_0ecb3cdbaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3261666285_86fceb762d.jpg: 640x448 6 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3261666285_86fceb762d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2730938963_c4ed3e2258.jpg: 544x640 3 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2730938963_c4ed3e2258.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3047751696_78c2efe5e6.jpg: 480x640 4 persons, 1 toothbrush, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3047751696_78c2efe5e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376439178_159e4126de.jpg: 640x448 1 person, 1 snowboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3376439178_159e4126de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1510078253_96e9ec50e7.jpg: 640x640 1 dog, 1 elephant, 1 donut, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1510078253_96e9ec50e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3172384527_b107385a20.jpg: 384x640 12 persons, 1 wine glass, 1 cup, 8 chairs, 1 potted plant, 1 tv, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3172384527_b107385a20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3581818450_546c89ca38.jpg: 416x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3581818450_546c89ca38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3445544288_68fdb25969.jpg: 384x640 (no detections), 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3445544288_68fdb25969.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3389321512_b11f499dab.jpg: 640x480 2 persons, 1 car, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3389321512_b11f499dab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627676364_1dc9294ec5.jpg: 384x640 7 persons, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3627676364_1dc9294ec5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004291289_c4892898ae.jpg: 640x288 1 person, 8.5ms\n",
      "Speed: 1.1ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Cropped images saved for 3004291289_c4892898ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/509241560_00e5b20562.jpg: 480x640 5 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 509241560_00e5b20562.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3362592729_893e26b806.jpg: 640x416 2 persons, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3362592729_893e26b806.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1251558317_4ef844b775.jpg: 640x480 9 persons, 2 cars, 1 motorcycle, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1251558317_4ef844b775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/451597318_4f370b1339.jpg: 416x640 1 boat, 1 dog, 1 frisbee, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 451597318_4f370b1339.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2692635048_16c279ff9e.jpg: 480x640 1 dog, 1 sports ball, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2692635048_16c279ff9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2239938351_43c73c887c.jpg: 480x640 3 dogs, 1 sports ball, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2239938351_43c73c887c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873837796_543e415e98.jpg: 608x640 1 dog, 1 cow, 1 frisbee, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2873837796_543e415e98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1429723917_6af585e4c0.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1429723917_6af585e4c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591094476_b61acd63d6.jpg: 640x448 1 person, 3 dogs, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3591094476_b61acd63d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3168841415_c0705a327a.jpg: 640x448 1 person, 1 frisbee, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3168841415_c0705a327a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315353266_70f0bbb1c3.jpg: 640x384 1 person, 1 suitcase, 1 tennis racket, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3315353266_70f0bbb1c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3190677999_60bbd330fd.jpg: 480x640 3 persons, 1 backpack, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3190677999_60bbd330fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639704469_fe83e1c9b7.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3639704469_fe83e1c9b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2084157130_f288e492e4.jpg: 448x640 2 dogs, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2084157130_f288e492e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3737492755_bcfb800ed1.jpg: 640x480 6 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3737492755_bcfb800ed1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405253184_5f611f3880.jpg: 480x640 1 dog, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 405253184_5f611f3880.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1259936608_e3f0064f23.jpg: 480x640 11 persons, 1 bench, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1259936608_e3f0064f23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3619416477_9d18580a14.jpg: 640x480 1 bird, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3619416477_9d18580a14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3048211972_db71d104c2.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3048211972_db71d104c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/160792599_6a7ec52516.jpg: 384x640 5 persons, 8.6ms\n",
      "Speed: 1.5ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 160792599_6a7ec52516.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364160101_c5e6c52b25.jpg: 640x480 1 person, 1 sports ball, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3364160101_c5e6c52b25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1001773457_577c3a7d70.jpg: 480x640 2 dogs, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1001773457_577c3a7d70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701226275_952547ba0f.jpg: 480x640 1 person, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3701226275_952547ba0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3623331945_df0f51d7dd.jpg: 512x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3623331945_df0f51d7dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/434938585_fbf913dfb4.jpg: 480x640 4 persons, 11 chairs, 4 dining tables, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 434938585_fbf913dfb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242007318_01e82171aa.jpg: 640x544 2 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3242007318_01e82171aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427301653_4ff0d6fd93.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3427301653_4ff0d6fd93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532914728_c5d8d56b0b.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 532914728_c5d8d56b0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2711720095_0b98426d3c.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2711720095_0b98426d3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2228167286_7089ab236a.jpg: 640x480 1 person, 1 apple, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2228167286_7089ab236a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286270205_16038dec5a.jpg: 416x640 1 boat, 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2286270205_16038dec5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991994415_504d1c0a03.jpg: 640x640 1 person, 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2991994415_504d1c0a03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/284441196_8ebb216d0d.jpg: 448x640 1 dog, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 284441196_8ebb216d0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/365584746_681f33fa46.jpg: 640x384 8 persons, 1 bicycle, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 365584746_681f33fa46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2542415282_9240e1b5fc.jpg: 640x480 3 persons, 1 carrot, 10.7ms\n",
      "Speed: 1.7ms preprocess, 10.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2542415282_9240e1b5fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2260369648_e21ae6494a.jpg: 576x640 4 persons, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2260369648_e21ae6494a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3700322513_50f0d45bfa.jpg: 448x640 3 persons, 1 snowboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3700322513_50f0d45bfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109202801_c6381eef15.jpg: 640x480 1 person, 2 horses, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 109202801_c6381eef15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2717686269_80c4b5ac9e.jpg: 480x640 (no detections), 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2717686269_80c4b5ac9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629295654_59ea1472a1.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2629295654_59ea1472a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534183988_3763593dfb.jpg: 480x640 3 persons, 1 cup, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3534183988_3763593dfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3165936115_cb4017d94e.jpg: 448x640 9 persons, 2 ties, 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3165936115_cb4017d94e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530502404_f8c3a3f61c.jpg: 544x640 1 person, 2 bicycles, 1 motorcycle, 1 parking meter, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3530502404_f8c3a3f61c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287064529_aa4e4f3c31.jpg: 480x640 1 dog, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1287064529_aa4e4f3c31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454355269_6185e29f95.jpg: 480x640 1 person, 1 bicycle, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3454355269_6185e29f95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/494329594_6e751372a0.jpg: 512x640 2 persons, 1 bench, 1 cup, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 494329594_6e751372a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2560278143_aa5110aa37.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2560278143_aa5110aa37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860400846_2c1026a573.jpg: 448x640 1 dog, 1 frisbee, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2860400846_2c1026a573.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353036763_4cbeba03b2.jpg: 640x448 7 persons, 2 horses, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3353036763_4cbeba03b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/408573233_1fff966798.jpg: 640x448 1 dog, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 408573233_1fff966798.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1800601130_1c0f248d12.jpg: 448x640 1 dog, 1 frisbee, 1 sports ball, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1800601130_1c0f248d12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3588417747_b152a51c52.jpg: 416x640 3 persons, 1 skateboard, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3588417747_b152a51c52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3013469764_30e84e9a0d.jpg: 448x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3013469764_30e84e9a0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532539748_795d16ef07.jpg: 448x640 3 persons, 2 cars, 1 bus, 2 trucks, 1 bench, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532539748_795d16ef07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2591486448_48d5438343.jpg: 448x640 2 persons, 1 dog, 1 cell phone, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2591486448_48d5438343.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1339596997_8ac29c1841.jpg: 640x576 4 persons, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 1339596997_8ac29c1841.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/287999021_998c2eeb91.jpg: 480x640 2 persons, 1 backpack, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 287999021_998c2eeb91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991771557_d98fa0a69f.jpg: 640x448 8 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2991771557_d98fa0a69f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2107838729_a527e434bd.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2107838729_a527e434bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/490390951_87395fcb1c.jpg: 480x640 1 person, 1 remote, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 490390951_87395fcb1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364151356_eecd07a23e.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3364151356_eecd07a23e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326086533_23a0a54a8e.jpg: 512x640 1 dog, 1 frisbee, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3326086533_23a0a54a8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/341665272_80d4d61376.jpg: 640x608 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 341665272_80d4d61376.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3321516504_5ee97771cb.jpg: 640x448 4 persons, 1 pizza, 1 couch, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3321516504_5ee97771cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/145721496_687af9bb18.jpg: 384x640 1 bird, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 145721496_687af9bb18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2479180530_7ebba2d8bf.jpg: 448x640 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2479180530_7ebba2d8bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2467803152_70eeca1334.jpg: 480x640 3 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2467803152_70eeca1334.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3328535573_33c3cd5b59.jpg: 640x640 13 persons, 3 horses, 1 elephant, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3328535573_33c3cd5b59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3021318991_fa28e3bca7.jpg: 288x640 1 person, 1 sports ball, 1 baseball bat, 8.8ms\n",
      "Speed: 1.1ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3021318991_fa28e3bca7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2183967273_d182e18cf6.jpg: 640x480 2 dogs, 1 tv, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2183967273_d182e18cf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1397295388_8a5b6b525d.jpg: 448x640 16 persons, 1 backpack, 1 tv, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1397295388_8a5b6b525d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2331510788_986809bbb4.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2331510788_986809bbb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480021389_dda9fb2818.jpg: 448x640 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2480021389_dda9fb2818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431261634_c73360406a.jpg: 512x640 1 person, 2 surfboards, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3431261634_c73360406a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1240297429_c36ae0c58f.jpg: 224x640 1 person, 1 dog, 52.7ms\n",
      "Speed: 0.9ms preprocess, 52.7ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Cropped images saved for 1240297429_c36ae0c58f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1423997242_ea2189ec5e.jpg: 480x640 1 person, 1 backpack, 1 bottle, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1423997242_ea2189ec5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1176580356_9810d877bf.jpg: 640x640 1 dog, 1 sports ball, 7.0ms\n",
      "Speed: 3.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1176580356_9810d877bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3593222804_c187808ac3.jpg: 448x640 2 persons, 3 surfboards, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3593222804_c187808ac3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109260216_85b0be5378.jpg: 640x480 (no detections), 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 109260216_85b0be5378.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2646046871_c3a5dbb971.jpg: 640x448 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2646046871_c3a5dbb971.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556485995_9cd40269e9.jpg: 480x640 2 persons, 2 baseball bats, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3556485995_9cd40269e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271468944_3264d29208.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2271468944_3264d29208.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585117340_73e96b6173.jpg: 480x640 2 dogs, 1 couch, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3585117340_73e96b6173.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300371487_daec5d11ab.jpg: 448x640 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300371487_daec5d11ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2364774105_fbaf0c191f.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2364774105_fbaf0c191f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467843559_a457ce37b6.jpg: 448x640 5 persons, 1 cell phone, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3467843559_a457ce37b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601843201_4809e66909.jpg: 448x640 1 motorcycle, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3601843201_4809e66909.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1303335399_b3facd47ab.jpg: 608x640 1 person, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1303335399_b3facd47ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/161905204_247c6ca6de.jpg: 480x640 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 161905204_247c6ca6de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1105959054_9c3a738096.jpg: 480x640 1 person, 1 refrigerator, 12.4ms\n",
      "Speed: 3.8ms preprocess, 12.4ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1105959054_9c3a738096.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2114126343_a0f74ff63b.jpg: 480x640 6 persons, 1 sports ball, 10.2ms\n",
      "Speed: 2.9ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2114126343_a0f74ff63b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3496983524_b21ecdb0c7.jpg: 448x640 14 persons, 1 tennis racket, 14.6ms\n",
      "Speed: 3.5ms preprocess, 14.6ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3496983524_b21ecdb0c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3608752424_01a1cfd8a6.jpg: 448x640 2 cows, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3608752424_01a1cfd8a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249597269_935e0a375a.jpg: 352x640 1 dog, 10.8ms\n",
      "Speed: 1.7ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3249597269_935e0a375a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2959941749_fa99097463.jpg: 480x640 6 persons, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2959941749_fa99097463.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2907073768_08fd7bdf60.jpg: 640x448 1 motorcycle, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2907073768_08fd7bdf60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/804087017_330bcc8c56.jpg: 448x640 8 persons, 1 bench, 2 umbrellas, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 804087017_330bcc8c56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564007203_df2b8010f1.jpg: 640x512 11 persons, 1 horse, 12.2ms\n",
      "Speed: 3.3ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3564007203_df2b8010f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396251819_1efa69310f.jpg: 640x448 1 person, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3396251819_1efa69310f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500139659_b2a60b0141.jpg: 448x640 2 persons, 2 motorcycles, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3500139659_b2a60b0141.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/354642192_3b7666a2dd.jpg: 640x480 1 person, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 354642192_3b7666a2dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241031254_0c6f30e3d1.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241031254_0c6f30e3d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3451984463_37ac1ff7a8.jpg: 448x640 1 person, 1 bicycle, 1 backpack, 12.4ms\n",
      "Speed: 3.0ms preprocess, 12.4ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3451984463_37ac1ff7a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623982903_58ec7c5026.jpg: 448x640 1 dog, 1 cow, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2623982903_58ec7c5026.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203872773_6c30f64be3.jpg: 480x640 1 person, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3203872773_6c30f64be3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2278776373_fe499a93be.jpg: 512x640 1 person, 1 truck, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2278776373_fe499a93be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495341977_b27279f962.jpg: 448x640 2 dogs, 1 frisbee, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 495341977_b27279f962.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2603690144_7a28b1d13c.jpg: 448x640 9 persons, 1 dog, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2603690144_7a28b1d13c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2792409624_2731b1072c.jpg: 416x640 1 dog, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2792409624_2731b1072c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3646970605_d25c25340b.jpg: 448x640 9 persons, 2 handbags, 2 ties, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3646970605_d25c25340b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/267164457_2e8b4d30aa.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 267164457_2e8b4d30aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2798651021_2566f2a47e.jpg: 480x640 2 persons, 1 car, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2798651021_2566f2a47e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/972381743_5677b420ab.jpg: 640x480 7 persons, 1 umbrella, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 972381743_5677b420ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2573667207_a1bf49befc.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2573667207_a1bf49befc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2600386812_8790879d9a.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2600386812_8790879d9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2603792708_18a97bac97.jpg: 480x640 10 persons, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2603792708_18a97bac97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2550109269_bc4262bd27.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2550109269_bc4262bd27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3131632154_098f86f4cb.jpg: 448x640 1 dog, 1 bear, 1 frisbee, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3131632154_098f86f4cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640104986_5d8c9a9948.jpg: 384x640 8 persons, 1 umbrella, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3640104986_5d8c9a9948.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134341610_3c55e373a7.jpg: 448x640 2 persons, 1 wine glass, 1 couch, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3134341610_3c55e373a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1982852140_56425fa7a2.jpg: 480x640 6 persons, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1982852140_56425fa7a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396275223_ee080df8b5.jpg: 416x640 3 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3396275223_ee080df8b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3567061016_62768dcce1.jpg: 640x640 2 birds, 7.9ms\n",
      "Speed: 2.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3567061016_62768dcce1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182258223_5b9c8a8c55.jpg: 480x640 2 persons, 1 cell phone, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3182258223_5b9c8a8c55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/325576658_59f68bdbd6.jpg: 448x640 5 persons, 1 car, 2 benchs, 1 sports ball, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 325576658_59f68bdbd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3138504165_c7ae396294.jpg: 512x640 1 person, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3138504165_c7ae396294.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/302241178_a582c1b953.jpg: 448x640 1 person, 2 cows, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 302241178_a582c1b953.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496129405_b9feeda1ab.jpg: 448x640 2 sheeps, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 496129405_b9feeda1ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2643263887_a32ffb878f.jpg: 640x448 2 persons, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2643263887_a32ffb878f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259110412_9908c45144.jpg: 384x640 1 cake, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3259110412_9908c45144.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2715289538_d77c8d0a85.jpg: 640x448 1 person, 1 toothbrush, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2715289538_d77c8d0a85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566384456_97da3c7bd6.jpg: 512x640 2 persons, 8.0ms\n",
      "Speed: 2.1ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 566384456_97da3c7bd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3637966641_1b108a35ba.jpg: 544x640 6 persons, 6 cars, 1 baseball glove, 7.9ms\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3637966641_1b108a35ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/526955751_f519d62b58.jpg: 448x640 4 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 526955751_f519d62b58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/359082432_c1fd5aa2d6.jpg: 640x480 1 person, 1 sheep, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 359082432_c1fd5aa2d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3299418821_21531b5b3c.jpg: 640x544 3 persons, 5 cars, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3299418821_21531b5b3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/166654939_80ea4ddbcc.jpg: 448x640 1 car, 6 trucks, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 166654939_80ea4ddbcc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262642489_f5c6b9e65b.jpg: 576x640 1 dog, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 262642489_f5c6b9e65b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171451305_f87b9e09ee.jpg: 480x640 6 persons, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3171451305_f87b9e09ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189293145_35dea42679.jpg: 640x384 1 bird, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3189293145_35dea42679.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457455611_94ee93929f.jpg: 640x640 1 person, 7.5ms\n",
      "Speed: 2.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3457455611_94ee93929f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2960422620_81889a3764.jpg: 448x640 11 persons, 39 birds, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2960422620_81889a3764.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/323657582_b6b6d8f7bd.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 323657582_b6b6d8f7bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701878677_8f2c26227b.jpg: 480x640 1 person, 2 bicycles, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3701878677_8f2c26227b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/156967462_72db9b722c.jpg: 544x640 1 person, 2 dogs, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 156967462_72db9b722c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3688797852_89ed3cb056.jpg: 448x640 5 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3688797852_89ed3cb056.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3000722396_1ae2e976c2.jpg: 480x640 1 dog, 1 frisbee, 1 sports ball, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3000722396_1ae2e976c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2270483627_16fe41b063.jpg: 448x640 1 person, 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2270483627_16fe41b063.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3433470650_a8b1c27173.jpg: 640x640 14 persons, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3433470650_a8b1c27173.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154886184_ac842655b6.jpg: 640x448 1 person, 1 skateboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3154886184_ac842655b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3476451861_5b9c9ce191.jpg: 448x640 1 person, 2 bicycles, 11.7ms\n",
      "Speed: 3.2ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3476451861_5b9c9ce191.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1569687608_0e3b3ad044.jpg: 448x640 3 persons, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1569687608_0e3b3ad044.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2579572274_826598b14a.jpg: 640x448 1 person, 1 couch, 1 bed, 2 cell phones, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2579572274_826598b14a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247617035_9f2e821534.jpg: 448x640 3 persons, 1 bottle, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 247617035_9f2e821534.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2231847779_1148d1c919.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2231847779_1148d1c919.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2728276857_3f83757ef2.jpg: 448x640 13 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2728276857_3f83757ef2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/336460583_6c8ccb7188.jpg: 480x640 1 dog, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 336460583_6c8ccb7188.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/361183669_52be9662b9.jpg: 480x640 3 persons, 4 boats, 1 bench, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 361183669_52be9662b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638908276_b1751d30ff.jpg: 640x480 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3638908276_b1751d30ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2622624460_207dbcc4cf.jpg: 384x640 (no detections), 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2622624460_207dbcc4cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2654514044_a70a6e2c21.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2654514044_a70a6e2c21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/791338571_7f38510bf7.jpg: 416x640 1 car, 1 cow, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 791338571_7f38510bf7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/950411653_20d0335946.jpg: 480x640 3 persons, 2 surfboards, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 950411653_20d0335946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2986620935_e97763983d.jpg: 640x544 2 persons, 2 motorcycles, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2986620935_e97763983d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394919002_ed7527ff93.jpg: 448x640 10 persons, 1 bench, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2394919002_ed7527ff93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3304484212_b950233c30.jpg: 640x448 1 person, 1 snowboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3304484212_b950233c30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2473293833_78820d2eaa.jpg: 640x640 2 persons, 1 donut, 10.1ms\n",
      "Speed: 3.6ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2473293833_78820d2eaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2193980605_4221c6474d.jpg: 480x640 6 persons, 1 sports ball, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2193980605_4221c6474d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2461372011_ebbf513766.jpg: 480x640 1 person, 1 dog, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2461372011_ebbf513766.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934628301_41ea2e6cf9.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2934628301_41ea2e6cf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296124052_6f1d1c9f8d.jpg: 640x448 1 person, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3296124052_6f1d1c9f8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2586911841_41b7a48c91.jpg: 640x448 1 person, 12.0ms\n",
      "Speed: 3.4ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2586911841_41b7a48c91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2863848437_f2592ab42d.jpg: 448x640 10 persons, 11.2ms\n",
      "Speed: 2.7ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2863848437_f2592ab42d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150380412_7021e5444a.jpg: 448x640 1 dog, 12.0ms\n",
      "Speed: 2.2ms preprocess, 12.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3150380412_7021e5444a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272491304_cb7c7ed16f.jpg: 480x640 6 persons, 2 backpacks, 2 skiss, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2272491304_cb7c7ed16f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2281768510_9cc5728c55.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2281768510_9cc5728c55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518118675_5053b3f738.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3518118675_5053b3f738.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2848895544_6d06210e9d.jpg: 416x640 3 persons, 1 sports ball, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2848895544_6d06210e9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271755053_e1b1ec8442.jpg: 448x640 7 persons, 1 sports ball, 1 baseball glove, 1 teddy bear, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2271755053_e1b1ec8442.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3213395965_2a823c6865.jpg: 448x640 4 persons, 1 car, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3213395965_2a823c6865.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2604305843_ebe3e8a328.jpg: 448x640 (no detections), 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2604305843_ebe3e8a328.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2953861572_d654d9b6f2.jpg: 640x448 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2953861572_d654d9b6f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549277110_24d4064ccd.jpg: 512x640 2 persons, 1 couch, 1 book, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3549277110_24d4064ccd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3124549928_10904a5a83.jpg: 448x640 3 persons, 1 skis, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3124549928_10904a5a83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2704379125_9c35650d16.jpg: 320x640 5 sheeps, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2704379125_9c35650d16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/393958545_48c17c66d1.jpg: 608x640 1 dog, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 393958545_48c17c66d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2797185895_4d9e1e9508.jpg: 480x640 2 persons, 1 boat, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2797185895_4d9e1e9508.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197247245_9c93b60b8a.jpg: 448x640 1 person, 1 bicycle, 1 clock, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3197247245_9c93b60b8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2622971954_59f192922d.jpg: 448x640 11 persons, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2622971954_59f192922d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439128755_84409b8823.jpg: 416x640 2 dogs, 1 frisbee, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3439128755_84409b8823.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1802092493_7b44fdb6b9.jpg: 640x480 5 persons, 2 potted plants, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1802092493_7b44fdb6b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2942094037_f6b36fd3db.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2942094037_f6b36fd3db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347204_007d83e252.jpg: 448x640 8 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347204_007d83e252.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3044746136_8b89da5f40.jpg: 480x640 17 persons, 13 chairs, 2 tvs, 4 laptops, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3044746136_8b89da5f40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/215214751_e913b6ff09.jpg: 448x640 1 boat, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 215214751_e913b6ff09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1131340021_83f46b150a.jpg: 640x480 1 person, 1 bench, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1131340021_83f46b150a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3036382555_30b7312cf3.jpg: 448x640 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3036382555_30b7312cf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/428408242_b32faf2240.jpg: 448x640 1 dog, 1 frisbee, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 428408242_b32faf2240.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187117682_986ffd6b67.jpg: 576x640 1 dog, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3187117682_986ffd6b67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2170222061_e8bce4a32d.jpg: 512x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2170222061_e8bce4a32d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260850192_fd03ea26f1.jpg: 480x640 (no detections), 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 260850192_fd03ea26f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1406010299_5755339f08.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1406010299_5755339f08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1876536922_8fdf8d7028.jpg: 448x640 (no detections), 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1876536922_8fdf8d7028.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3398788809_25c71ba018.jpg: 448x640 7 persons, 4 cars, 2 handbags, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3398788809_25c71ba018.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457572788_e1fe4f6480.jpg: 448x640 1 person, 1 skis, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3457572788_e1fe4f6480.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3343106500_27176fc544.jpg: 640x512 7 persons, 1 sports ball, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3343106500_27176fc544.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3440952969_608eb5e42b.jpg: 640x448 1 person, 1 skateboard, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3440952969_608eb5e42b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2297471897_3419605c16.jpg: 640x640 1 dog, 7.0ms\n",
      "Speed: 2.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2297471897_3419605c16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/195962790_3380aea352.jpg: 448x640 6 persons, 2 dogs, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 195962790_3380aea352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3104182973_5bb1c31275.jpg: 640x480 10 persons, 2 cars, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3104182973_5bb1c31275.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537520829_aab733e16c.jpg: 448x640 1 person, 2 skateboards, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3537520829_aab733e16c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752230113_b5dab6f0f1.jpg: 480x640 3 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2752230113_b5dab6f0f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1104133405_c04a00707f.jpg: 448x640 6 persons, 2 benchs, 1 sports ball, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1104133405_c04a00707f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2040941056_7f5fd50794.jpg: 640x448 1 bear, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2040941056_7f5fd50794.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2475162978_2c51048dca.jpg: 640x640 2 persons, 1 sports ball, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2475162978_2c51048dca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2718376488_3c62f7642c.jpg: 448x640 6 persons, 3 cows, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2718376488_3c62f7642c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2194494220_bb2178832c.jpg: 640x448 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2194494220_bb2178832c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3003612178_8230d65833.jpg: 608x640 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3003612178_8230d65833.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3317145805_071b15debb.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3317145805_071b15debb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3230101918_7d81cb0fc8.jpg: 448x640 2 persons, 1 chair, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3230101918_7d81cb0fc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/735787579_617b047319.jpg: 448x640 6 persons, 2 chairs, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 735787579_617b047319.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2098646162_e3b3bbf14c.jpg: 480x640 1 cat, 1 dog, 1 laptop, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2098646162_e3b3bbf14c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019667009_20db160195.jpg: 640x480 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3019667009_20db160195.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1321723162_9d4c78b8af.jpg: 480x640 6 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1321723162_9d4c78b8af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2903617548_d3e38d7f88.jpg: 640x384 1 person, 1 sports ball, 1 baseball bat, 1 tennis racket, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2903617548_d3e38d7f88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3625519177_4c2bb9e7f0.jpg: 384x640 1 person, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3625519177_4c2bb9e7f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2295447147_458cfea65a.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2295447147_458cfea65a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3268191118_ba25fabab6.jpg: 640x448 2 persons, 1 frisbee, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3268191118_ba25fabab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211029717_2affe6bbd5.jpg: 288x640 4 persons, 7.9ms\n",
      "Speed: 1.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3211029717_2affe6bbd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/798343627_7492fe0c12.jpg: 640x480 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 798343627_7492fe0c12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425069551_aba046a1b6.jpg: 640x512 7 persons, 1 tennis racket, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425069551_aba046a1b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/698107542_3aa0ba78b4.jpg: 416x640 3 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 698107542_3aa0ba78b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1466307485_5e6743332e.jpg: 640x608 1 person, 1 car, 2 trucks, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1466307485_5e6743332e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330680118_4e541889c1.jpg: 480x640 1 person, 1 skis, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3330680118_4e541889c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2702506716_17a7fb3ba4.jpg: 512x640 1 bench, 1 dog, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2702506716_17a7fb3ba4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604928725_58147c87cd.jpg: 448x640 2 persons, 2 surfboards, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3604928725_58147c87cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500136982_bf7a85531e.jpg: 448x640 8 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3500136982_bf7a85531e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3557324238_6ba58831d9.jpg: 480x640 1 person, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3557324238_6ba58831d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346794_4319f8af67.jpg: 448x640 14 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346794_4319f8af67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2079110798_ad1370a646.jpg: 480x640 2 persons, 1 chair, 1 tv, 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2079110798_ad1370a646.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501936223_6122a13d27.jpg: 384x640 1 person, 11.3ms\n",
      "Speed: 2.3ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3501936223_6122a13d27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676937700_456134c7b5.jpg: 480x640 7 persons, 1 handbag, 1 remote, 9.7ms\n",
      "Speed: 2.4ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2676937700_456134c7b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/322791392_aa3b142f43.jpg: 480x640 1 dog, 1 frisbee, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 322791392_aa3b142f43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2466495935_623b144183.jpg: 416x640 9 persons, 1 sports ball, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2466495935_623b144183.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2376694294_9a4ecc3b90.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2376694294_9a4ecc3b90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3135826945_f7c741e5b7.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3135826945_f7c741e5b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220650628_4ed964e5b4.jpg: 448x640 17 persons, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3220650628_4ed964e5b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223055565_68973f5d20.jpg: 640x576 2 persons, 1 dog, 9.9ms\n",
      "Speed: 3.7ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3223055565_68973f5d20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3015891201_2c1a9e5cd7.jpg: 640x448 5 persons, 1 laptop, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3015891201_2c1a9e5cd7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/454686980_7517fe0c2e.jpg: 480x640 6 persons, 1 backpack, 1 handbag, 1 baseball glove, 1 cell phone, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 454686980_7517fe0c2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3282897060_8c584e2ce8.jpg: 480x640 2 persons, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3282897060_8c584e2ce8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672944692_8d24a44fc6.jpg: 640x448 2 traffic lights, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3672944692_8d24a44fc6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354330935_de75be9d2f.jpg: 448x640 4 persons, 2 skiss, 11.8ms\n",
      "Speed: 2.0ms preprocess, 11.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3354330935_de75be9d2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187492926_8aa85f80c6.jpg: 448x640 1 person, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3187492926_8aa85f80c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2975253472_0f0c2dea70.jpg: 640x544 3 persons, 10.1ms\n",
      "Speed: 2.6ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2975253472_0f0c2dea70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2812590023_50182bc417.jpg: 512x640 5 persons, 1 traffic light, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2812590023_50182bc417.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2396025708_e4a72e2558.jpg: 448x640 1 person, 1 surfboard, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2396025708_e4a72e2558.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2324917982_f3db8c11e9.jpg: 480x640 3 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2324917982_f3db8c11e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/384465575_31294122c0.jpg: 480x640 2 persons, 3 bicycles, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 384465575_31294122c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2982881046_45765ced2c.jpg: 448x640 1 person, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2982881046_45765ced2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3388836914_c267cf3a59.jpg: 640x512 1 person, 2 tennis rackets, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3388836914_c267cf3a59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468310111_d9396abcbd.jpg: 480x640 11 cows, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 468310111_d9396abcbd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1476241331_2f43b67aed.jpg: 640x608 2 persons, 7.5ms\n",
      "Speed: 2.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1476241331_2f43b67aed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397259310_1ed1a346b5.jpg: 448x640 1 person, 1 car, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3397259310_1ed1a346b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1401961581_76921a75c5.jpg: 640x352 6 persons, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 1401961581_76921a75c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524031846_28b11bc0e5.jpg: 640x448 1 person, 1 chair, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 524031846_28b11bc0e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2477623312_58e8e8c8af.jpg: 640x448 5 persons, 1 backpack, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2477623312_58e8e8c8af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/269361490_a22ae818bf.jpg: 448x640 5 persons, 1 car, 2 handbags, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 269361490_a22ae818bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/476760133_c33d2bd83d.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 476760133_c33d2bd83d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2543679402_9359e1ee4e.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2543679402_9359e1ee4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3120266797_47e7d91614.jpg: 448x640 3 persons, 9.8ms\n",
      "Speed: 1.5ms preprocess, 9.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3120266797_47e7d91614.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884420269_225d27f242.jpg: 640x448 3 persons, 1 chair, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2884420269_225d27f242.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2317714088_bcd081f926.jpg: 480x640 4 persons, 13.4ms\n",
      "Speed: 1.6ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2317714088_bcd081f926.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219843859_94b6d0a580.jpg: 640x480 1 person, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 219843859_94b6d0a580.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431194126_ca78f5fde6.jpg: 640x448 2 persons, 9.6ms\n",
      "Speed: 2.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3431194126_ca78f5fde6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3737539561_d1dc161040.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3737539561_d1dc161040.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335375223_b4da8df523.jpg: 480x640 1 person, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3335375223_b4da8df523.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2543589122_ec3e55f434.jpg: 640x512 1 cat, 1 dog, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2543589122_ec3e55f434.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2872806249_00bea3c4e7.jpg: 512x640 4 persons, 1 bicycle, 7 cars, 1 truck, 1 traffic light, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2872806249_00bea3c4e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/947664583_7c63172366.jpg: 480x640 15 persons, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 947664583_7c63172366.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454149297_01454a2554.jpg: 448x640 6 persons, 1 frisbee, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3454149297_01454a2554.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2368266191_87d77750f1.jpg: 640x448 3 cars, 1 dog, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2368266191_87d77750f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/308014594_f1d5e75507.jpg: 448x640 1 person, 1 teddy bear, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 308014594_f1d5e75507.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590593467_be497a6139.jpg: 320x640 2 persons, 8.6ms\n",
      "Speed: 1.1ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3590593467_be497a6139.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517056462_483ee5a914.jpg: 448x640 15 persons, 1 sports ball, 3 baseball gloves, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3517056462_483ee5a914.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2764178773_d63b502812.jpg: 448x640 1 person, 1 bicycle, 1 kite, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2764178773_d63b502812.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217056901_fe2c70377d.jpg: 640x480 10 persons, 3 umbrellas, 1 chair, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3217056901_fe2c70377d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3630102841_b4c3e00b2c.jpg: 384x640 9 persons, 1 tie, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3630102841_b4c3e00b2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539751252_2bd88c456b.jpg: 512x640 1 person, 4 surfboards, 1 chair, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 539751252_2bd88c456b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2838085973_42b6e9b5b1.jpg: 640x512 9 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2838085973_42b6e9b5b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1290894194_8a4ffdc7eb.jpg: 640x544 2 persons, 2 dogs, 1 handbag, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1290894194_8a4ffdc7eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415228562_4efa9c9b70.jpg: 640x480 7 persons, 1 tie, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3415228562_4efa9c9b70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2130851544_d36f4f2ea6.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2130851544_d36f4f2ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506412121_67ecc7ec05.jpg: 256x640 1 person, 9.5ms\n",
      "Speed: 0.9ms preprocess, 9.5ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 506412121_67ecc7ec05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2180356743_b3a3c9a7f6.jpg: 640x480 11 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2180356743_b3a3c9a7f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673878924_506c9d767b.jpg: 448x640 2 persons, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3673878924_506c9d767b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/454709143_9c513f095c.jpg: 640x544 1 person, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 454709143_9c513f095c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/873633312_a756d8b381.jpg: 640x576 1 person, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 873633312_a756d8b381.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2672588619_3849930e99.jpg: 448x640 3 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2672588619_3849930e99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2426724282_237bca30b5.jpg: 480x640 1 car, 1 truck, 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2426724282_237bca30b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347214_5f19e7998c.jpg: 640x448 4 persons, 1 handbag, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347214_5f19e7998c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629294578_853a08bb43.jpg: 448x640 1 person, 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2629294578_853a08bb43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3377344932_6dfce93248.jpg: 640x480 1 person, 1 dog, 1 chair, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3377344932_6dfce93248.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258874419_23fec1bdc1.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3258874419_23fec1bdc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346709_23204cc2bc.jpg: 448x640 8 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346709_23204cc2bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/394463341_5311c53783.jpg: 480x640 1 person, 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 394463341_5311c53783.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2144050118_3e7d2e05b1.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2144050118_3e7d2e05b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/521186251_e97d1f50f8.jpg: 448x640 4 persons, 1 surfboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 521186251_e97d1f50f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/265223843_9ef21e1872.jpg: 640x416 1 person, 1 kite, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 265223843_9ef21e1872.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3512033861_a357bb58b6.jpg: 448x640 29 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3512033861_a357bb58b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3290465391_258429e2f9.jpg: 640x448 1 person, 1 suitcase, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3290465391_258429e2f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394763838_99d1435b85.jpg: 384x640 1 dog, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2394763838_99d1435b85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415165462_e1cb536d08.jpg: 448x640 1 bear, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415165462_e1cb536d08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2923475135_a6b6e13d26.jpg: 448x640 1 person, 1 horse, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2923475135_a6b6e13d26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3136674757_57406c305c.jpg: 640x640 2 persons, 1 bed, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3136674757_57406c305c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346215_037e18403a.jpg: 448x640 9 persons, 1 teddy bear, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346215_037e18403a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3683592946_262e9bfbfd.jpg: 448x640 5 birds, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3683592946_262e9bfbfd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3765374230_cb1bbee0cb.jpg: 576x640 1 person, 1 sports ball, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3765374230_cb1bbee0cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/297169473_d3974e0275.jpg: 352x640 1 dog, 1 cow, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 297169473_d3974e0275.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585487286_ef9a8d4c56.jpg: 416x640 2 dogs, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3585487286_ef9a8d4c56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544573946_e03aebbfde.jpg: 448x640 2 persons, 10 cars, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3544573946_e03aebbfde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1745110280_0cbff5e273.jpg: 640x416 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 1745110280_0cbff5e273.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/292780636_72e1968949.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 292780636_72e1968949.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459871361_92d1ecda36.jpg: 640x448 1 person, 1 bench, 2 tennis rackets, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3459871361_92d1ecda36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/371364900_5167d4dd7f.jpg: 640x480 1 fire hydrant, 2 birds, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 371364900_5167d4dd7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1921398767_771743bf4e.jpg: 640x480 1 dog, 1 cow, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1921398767_771743bf4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623146491_b64698b875.jpg: 640x480 1 person, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2623146491_b64698b875.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3321334180_8f764e0e0f.jpg: 480x640 1 bird, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3321334180_8f764e0e0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2905948395_ca3e6b3c9a.jpg: 640x448 2 persons, 1 bicycle, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2905948395_ca3e6b3c9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1402641725_5e027ecaa7.jpg: 448x640 4 persons, 1 elephant, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1402641725_5e027ecaa7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/106514190_bae200f463.jpg: 640x480 1 person, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 106514190_bae200f463.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2284239186_c827f4defa.jpg: 448x640 1 dog, 1 bed, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2284239186_c827f4defa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2937942758_712be5c610.jpg: 448x640 1 person, 1 bicycle, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2937942758_712be5c610.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070130228_67dcfee9ae.jpg: 640x512 3 persons, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3070130228_67dcfee9ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2339573065_91f1e3be60.jpg: 352x640 1 person, 1 bottle, 14.2ms\n",
      "Speed: 3.0ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2339573065_91f1e3be60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405051459_3b3a3ba5b3.jpg: 640x320 4 persons, 1 car, 1 handbag, 12.9ms\n",
      "Speed: 2.1ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 405051459_3b3a3ba5b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2109911919_af45b93ef3.jpg: 480x640 1 person, 2 backpacks, 10.4ms\n",
      "Speed: 2.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2109911919_af45b93ef3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563668905_689ed479c5.jpg: 480x640 1 person, 1 bicycle, 8.5ms\n",
      "Speed: 2.3ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3563668905_689ed479c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/511749704_3037806cb1.jpg: 448x640 3 persons, 2 skateboards, 13.8ms\n",
      "Speed: 3.4ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 511749704_3037806cb1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342309960_c694b2cce9.jpg: 640x448 1 person, 1 skateboard, 12.1ms\n",
      "Speed: 2.8ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3342309960_c694b2cce9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673165148_67f217064f.jpg: 448x640 1 dog, 10.9ms\n",
      "Speed: 2.5ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3673165148_67f217064f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222702477_34d2d24f1f.jpg: 576x640 11 persons, 1 sports ball, 9.8ms\n",
      "Speed: 3.0ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3222702477_34d2d24f1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2682194299_92005b26c6.jpg: 640x480 1 dog, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2682194299_92005b26c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2243904502_2d265fed80.jpg: 512x640 1 dog, 1 sports ball, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2243904502_2d265fed80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3028561714_83fb921067.jpg: 480x640 1 dog, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3028561714_83fb921067.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143159297_6f2f663ea6.jpg: 480x640 4 persons, 1 tv, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143159297_6f2f663ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2244171992_a4beb04d8e.jpg: 480x640 12 persons, 1 tie, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2244171992_a4beb04d8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2909609550_070eea49b5.jpg: 480x640 1 person, 1 dog, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2909609550_070eea49b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346629210_8d6668d22d.jpg: 640x480 2 persons, 1 car, 1 tie, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2346629210_8d6668d22d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222250187_ef610f267e.jpg: 640x448 1 person, 1 sports ball, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3222250187_ef610f267e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3365348059_9773165302.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3365348059_9773165302.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2894217628_f1a4153dca.jpg: 512x640 11 persons, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2894217628_f1a4153dca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2913965136_2d00136697.jpg: 480x640 4 persons, 1 sports ball, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2913965136_2d00136697.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330019493_fd36fbc2ea.jpg: 640x448 1 person, 1 bicycle, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3330019493_fd36fbc2ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516725192_c9cdd63878.jpg: 352x640 1 person, 15.0ms\n",
      "Speed: 3.1ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 516725192_c9cdd63878.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3118505332_b0792489b5.jpg: 448x640 3 persons, 12.7ms\n",
      "Speed: 2.8ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3118505332_b0792489b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349968447_b5d4a477b2.jpg: 480x640 1 dog, 1 frisbee, 11.2ms\n",
      "Speed: 2.7ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3349968447_b5d4a477b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2666205903_8d287669e1.jpg: 448x640 8 persons, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2666205903_8d287669e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2445442929_8c81d42460.jpg: 480x640 3 persons, 1 bench, 1 cup, 1 dining table, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2445442929_8c81d42460.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498748832_941faaaf40.jpg: 640x480 3 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 498748832_941faaaf40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263850317_5bb3a18a08.jpg: 640x448 7 persons, 4 handbags, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 263850317_5bb3a18a08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/497579819_f91b26f7d3.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 497579819_f91b26f7d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3072611047_109bf8b7c3.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3072611047_109bf8b7c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2160266952_a2ab39191b.jpg: 480x640 3 persons, 1 chair, 1 toothbrush, 12.5ms\n",
      "Speed: 3.2ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2160266952_a2ab39191b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3300019891_8f404d94a1.jpg: 480x640 12 persons, 9.7ms\n",
      "Speed: 2.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3300019891_8f404d94a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1424775129_ffea9c13ab.jpg: 480x640 1 person, 10.7ms\n",
      "Speed: 2.4ms preprocess, 10.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1424775129_ffea9c13ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/447733067_09cfac3286.jpg: 448x640 5 persons, 1 car, 1 skis, 1 baseball glove, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 447733067_09cfac3286.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2466093839_33bbc8cbd9.jpg: 448x640 2 persons, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2466093839_33bbc8cbd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3346289227_198fced308.jpg: 608x640 1 person, 9.9ms\n",
      "Speed: 2.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3346289227_198fced308.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2830755303_2b5444ab4c.jpg: 640x448 1 person, 6 birds, 1 sheep, 2 kites, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2830755303_2b5444ab4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3072730593_b7322d2e05.jpg: 480x640 4 persons, 14.0ms\n",
      "Speed: 3.6ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3072730593_b7322d2e05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3729525173_7f984ed776.jpg: 448x640 1 person, 12.2ms\n",
      "Speed: 2.9ms preprocess, 12.2ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3729525173_7f984ed776.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/349889354_4b2889a9bd.jpg: 448x640 1 person, 1 surfboard, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 349889354_4b2889a9bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1926129518_4350f4f552.jpg: 640x640 1 bench, 1 dog, 10.3ms\n",
      "Speed: 3.1ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1926129518_4350f4f552.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3519155763_045a6a55e2.jpg: 384x640 1 cow, 1 bear, 1 teddy bear, 11.2ms\n",
      "Speed: 2.2ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3519155763_045a6a55e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3064716525_b8418d4946.jpg: 512x640 1 person, 1 dog, 9.9ms\n",
      "Speed: 3.0ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3064716525_b8418d4946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472485022_5d03e9852d.jpg: 512x640 1 dog, 8.5ms\n",
      "Speed: 2.4ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3472485022_5d03e9852d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3241726740_6d256d61ec.jpg: 480x640 2 persons, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3241726740_6d256d61ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2813784259_d201044d71.jpg: 448x640 5 persons, 1 baseball bat, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2813784259_d201044d71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423292784_166ee54e0b.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2423292784_166ee54e0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2699342860_5288e203ea.jpg: 640x448 1 person, 9.9ms\n",
      "Speed: 2.0ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2699342860_5288e203ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1319634306_816f21677f.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1319634306_816f21677f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1332815795_8eea44375e.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1332815795_8eea44375e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2083434441_a93bc6306b.jpg: 480x640 7 persons, 4 cars, 1 truck, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 4.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2083434441_a93bc6306b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2814037463_74de97da86.jpg: 448x640 3 persons, 1 wine glass, 1 cup, 1 bowl, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2814037463_74de97da86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155451946_c0862c70cb.jpg: 448x640 3 persons, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3155451946_c0862c70cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300577375_26cc2773a1.jpg: 448x640 1 person, 9 cars, 1 bus, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300577375_26cc2773a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1389651420_8d95d8f6ed.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1389651420_8d95d8f6ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1167662968_e466f1e80a.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1167662968_e466f1e80a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3333921867_6cc7d7c73d.jpg: 448x640 1 person, 2 dogs, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3333921867_6cc7d7c73d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3051998298_38da5746fa.jpg: 480x640 5 persons, 2 boats, 1 chair, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3051998298_38da5746fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3367053761_8ec5834bf3.jpg: 640x480 2 persons, 1 bench, 1 chair, 1 dining table, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3367053761_8ec5834bf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2715155329_1ed1756000.jpg: 544x640 1 person, 8.0ms\n",
      "Speed: 2.1ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2715155329_1ed1756000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3089107423_81a24eaf18.jpg: 480x640 5 persons, 1 train, 1 chair, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3089107423_81a24eaf18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109823394_83fcb735e1.jpg: 448x640 2 persons, 1 motorcycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 109823394_83fcb735e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3480051754_18e5802558.jpg: 480x640 18 persons, 3 umbrellas, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3480051754_18e5802558.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3488087117_2719647989.jpg: 416x640 1 dog, 7.8ms\n",
      "Speed: 1.5ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3488087117_2719647989.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/492341908_1ef53be265.jpg: 640x480 2 persons, 1 motorcycle, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 492341908_1ef53be265.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/232874193_c691df882d.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 232874193_c691df882d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/639120223_7db6bdb61f.jpg: 448x640 2 persons, 1 cup, 1 tv, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 639120223_7db6bdb61f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2438085746_588dce8724.jpg: 640x480 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2438085746_588dce8724.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217266166_4e0091860b.jpg: 480x640 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3217266166_4e0091860b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3299820401_c2589186c5.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3299820401_c2589186c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2392460773_2aa01eb340.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2392460773_2aa01eb340.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390360326_26f5936189.jpg: 480x640 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 390360326_26f5936189.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3029928396_99ac250788.jpg: 448x640 3 persons, 3 sheeps, 7 oranges, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3029928396_99ac250788.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271061953_700b96520c.jpg: 448x640 17 persons, 1 truck, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3271061953_700b96520c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/324208502_674488bcea.jpg: 448x640 2 persons, 2 benchs, 1 handbag, 1 teddy bear, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 324208502_674488bcea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2920305300_a5b1b2329a.jpg: 512x640 1 person, 1 skateboard, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2920305300_a5b1b2329a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482474257_a88bfe5c57.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3482474257_a88bfe5c57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3416246113_1745559b6b.jpg: 640x512 1 person, 1 tennis racket, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3416246113_1745559b6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401647850_685c03ffff.jpg: 448x640 1 person, 1 bear, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3401647850_685c03ffff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3409947123_a8d07edecf.jpg: 640x640 8 persons, 1 handbag, 7.4ms\n",
      "Speed: 2.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3409947123_a8d07edecf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204686006_88f04547b9.jpg: 480x640 4 persons, 1 dog, 2 snowboards, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3204686006_88f04547b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/384577800_fc325af410.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 384577800_fc325af410.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3170897628_3054087f8c.jpg: 448x640 2 persons, 1 sports ball, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3170897628_3054087f8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3434452829_62cee280bc.jpg: 640x640 5 persons, 6.8ms\n",
      "Speed: 2.1ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3434452829_62cee280bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2750867389_4b815f793a.jpg: 480x640 2 persons, 1 car, 1 truck, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2750867389_4b815f793a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1030985833_b0902ea560.jpg: 480x640 2 dogs, 1 frisbee, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1030985833_b0902ea560.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3015368588_ef0a06076d.jpg: 448x640 2 persons, 1 baseball glove, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3015368588_ef0a06076d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521062020_f8b983e4b2.jpg: 480x640 4 persons, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2521062020_f8b983e4b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3596959859_a7cb1e194b.jpg: 448x640 6 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3596959859_a7cb1e194b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2553188198_da1123a723.jpg: 448x640 1 dog, 1 horse, 1 sheep, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2553188198_da1123a723.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2236016316_f476cbbf06.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2236016316_f476cbbf06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2453990033_df53f0d8c8.jpg: 448x640 1 person, 5 cars, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2453990033_df53f0d8c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2915183095_4ed4aa4f37.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2915183095_4ed4aa4f37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/445655284_c29e6d7323.jpg: 640x480 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 445655284_c29e6d7323.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2267682214_e1434d853b.jpg: 640x480 1 dog, 1 frisbee, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2267682214_e1434d853b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1347519824_e402241e4f.jpg: 448x640 4 persons, 1 handbag, 1 cup, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1347519824_e402241e4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428086758_bce4733f7e.jpg: 416x640 1 dog, 1 horse, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2428086758_bce4733f7e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3206058778_7053ee6b52.jpg: 576x640 2 persons, 7.5ms\n",
      "Speed: 4.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3206058778_7053ee6b52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3272541970_ac0f1de274.jpg: 640x416 1 person, 2 skateboards, 8.0ms\n",
      "Speed: 1.5ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3272541970_ac0f1de274.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587596696_9c5964c94d.jpg: 480x640 3 persons, 1 frisbee, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3587596696_9c5964c94d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219070971_ae43410b9e.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 219070971_ae43410b9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1350948838_fdebe4ff65.jpg: 416x640 1 dog, 16.7ms\n",
      "Speed: 1.6ms preprocess, 16.7ms inference, 2.9ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1350948838_fdebe4ff65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3156113206_53c2a7b5d8.jpg: 640x480 3 persons, 14.8ms\n",
      "Speed: 3.3ms preprocess, 14.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3156113206_53c2a7b5d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563461991_de05537878.jpg: 480x640 2 persons, 3 boats, 13.0ms\n",
      "Speed: 3.4ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3563461991_de05537878.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084011664_76d37c6559.jpg: 480x640 2 persons, 1 cup, 16.4ms\n",
      "Speed: 4.0ms preprocess, 16.4ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3084011664_76d37c6559.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584930205_a3f58a4b7c.jpg: 448x640 3 persons, 1 skateboard, 14.8ms\n",
      "Speed: 2.8ms preprocess, 14.8ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584930205_a3f58a4b7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3374054694_fa56f29267.jpg: 448x640 2 persons, 1 dog, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3374054694_fa56f29267.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949014128_0d96196261.jpg: 640x640 6 persons, 1 sports ball, 12.0ms\n",
      "Speed: 3.1ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2949014128_0d96196261.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1295669416_21cabf594d.jpg: 640x448 2 persons, 10.4ms\n",
      "Speed: 2.1ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1295669416_21cabf594d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/486300784_2cc7a770ff.jpg: 480x640 1 dog, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 486300784_2cc7a770ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3558796959_fc4450be56.jpg: 448x640 1 person, 1 sports ball, 1 tennis racket, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3558796959_fc4450be56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95734038_2ab5783da7.jpg: 448x640 1 person, 1 bicycle, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 95734038_2ab5783da7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472720629_d9a6736356.jpg: 416x640 2 persons, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2472720629_d9a6736356.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677860841_3aa9d8036c.jpg: 448x640 4 persons, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677860841_3aa9d8036c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/281754914_bc8119a0ed.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 281754914_bc8119a0ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656104088_a0d1642fa9.jpg: 448x640 12 birds, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3656104088_a0d1642fa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236144859_52f9e38885.jpg: 480x640 3 dogs, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 236144859_52f9e38885.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2258662398_2797d0eca8.jpg: 448x640 1 person, 1 surfboard, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2258662398_2797d0eca8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1351315701_6580b51c41.jpg: 576x640 8 persons, 3 cows, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 1351315701_6580b51c41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3696246123_99d4d10140.jpg: 448x640 2 dogs, 1 sports ball, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3696246123_99d4d10140.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431121650_056db85987.jpg: 480x640 4 birds, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3431121650_056db85987.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475816542_f5c2736815.jpg: 448x640 2 dogs, 1 bear, 8.9ms\n",
      "Speed: 1.6ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 475816542_f5c2736815.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2148916767_644ea6a7fa.jpg: 640x480 2 dogs, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2148916767_644ea6a7fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726120436_740bda8416.jpg: 480x640 3 persons, 1 truck, 1 dog, 2 chairs, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3726120436_740bda8416.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/458213442_12c59e61a0.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 458213442_12c59e61a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2745441424_5659f6acc8.jpg: 480x640 5 persons, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2745441424_5659f6acc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3639967449_137f48b43d.jpg: 448x640 2 persons, 1 toothbrush, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3639967449_137f48b43d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090723611_318031cfa5.jpg: 416x640 1 dog, 1 cow, 8.9ms\n",
      "Speed: 1.6ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2090723611_318031cfa5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/229951087_4c20600c32.jpg: 448x640 1 person, 1 boat, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 229951087_4c20600c32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1770036088_08abe4f6e9.jpg: 640x512 3 persons, 1 bed, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1770036088_08abe4f6e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3526150930_580908dab6.jpg: 480x640 2 persons, 8.5ms\n",
      "Speed: 1.7ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3526150930_580908dab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1386251841_5f384a0fea.jpg: 640x480 4 persons, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1386251841_5f384a0fea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2076865206_53918c820c.jpg: 384x640 1 person, 10.2ms\n",
      "Speed: 1.5ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2076865206_53918c820c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3513362553_5fc5779e20.jpg: 448x640 6 persons, 1 sports ball, 2 baseball bats, 1 baseball glove, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3513362553_5fc5779e20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1077546505_a4f6c4daa9.jpg: 480x640 1 person, 1 boat, 1 surfboard, 8.9ms\n",
      "Speed: 1.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1077546505_a4f6c4daa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/410413536_11f1127c46.jpg: 256x640 2 persons, 1 dog, 10.9ms\n",
      "Speed: 1.0ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 410413536_11f1127c46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2618866067_07cbc83dc5.jpg: 640x480 1 dog, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2618866067_07cbc83dc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3571147934_d1c8af1d6e.jpg: 512x640 4 persons, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3571147934_d1c8af1d6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568505408_4e30def669.jpg: 512x640 7 persons, 1 car, 1 truck, 1 bench, 1 horse, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3568505408_4e30def669.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3643074723_94d42b7a0c.jpg: 640x480 1 person, 9.4ms\n",
      "Speed: 1.8ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3643074723_94d42b7a0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3350786891_6d39b234e9.jpg: 640x448 11 persons, 1 backpack, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3350786891_6d39b234e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/264928854_d9e61f3a8e.jpg: 608x640 1 person, 10.2ms\n",
      "Speed: 2.3ms preprocess, 10.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 264928854_d9e61f3a8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1663454406_5e2cf8c5bb.jpg: 448x640 8 persons, 2 umbrellas, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1663454406_5e2cf8c5bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3120189281_1938460e85.jpg: 480x640 2 persons, 3 dogs, 10.8ms\n",
      "Speed: 1.8ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3120189281_1938460e85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3626642428_3396568c3c.jpg: 480x640 4 dogs, 1 cow, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3626642428_3396568c3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3128856481_86e5df4160.jpg: 512x640 3 birds, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3128856481_86e5df4160.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544233095_4bca71df1d.jpg: 480x640 2 persons, 1 kite, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3544233095_4bca71df1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2084217208_7bd9bc85e5.jpg: 480x640 1 person, 1 bench, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2084217208_7bd9bc85e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475042270_719ebe6c48.jpg: 544x640 3 dogs, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 475042270_719ebe6c48.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3346614841_698f9aa486.jpg: 640x448 1 person, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3346614841_698f9aa486.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2353119813_685bace18e.jpg: 448x640 1 dog, 1 sports ball, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2353119813_685bace18e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/965444691_fe7e85bf0e.jpg: 448x640 2 persons, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 965444691_fe7e85bf0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3525417522_7beb617f8b.jpg: 448x640 3 persons, 2 dogs, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3525417522_7beb617f8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/251792066_b5233b3d86.jpg: 640x640 2 dogs, 1 frisbee, 9.3ms\n",
      "Speed: 3.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 251792066_b5233b3d86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609645320_815c294b65.jpg: 448x640 5 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3609645320_815c294b65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421808539_57abee6d55.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 421808539_57abee6d55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107592525_0bcd00777e.jpg: 640x608 2 persons, 9.9ms\n",
      "Speed: 2.7ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3107592525_0bcd00777e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245250964_9d3e37111e.jpg: 640x384 4 persons, 1 sports ball, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3245250964_9d3e37111e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3201594926_cd2009eb13.jpg: 640x448 3 persons, 3 handbags, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3201594926_cd2009eb13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2951092164_4940b9a517.jpg: 640x448 2 persons, 12.5ms\n",
      "Speed: 3.4ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2951092164_4940b9a517.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2201951969_0d7520d648.jpg: 448x640 1 person, 1 surfboard, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2201951969_0d7520d648.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2214847438_4993210d4c.jpg: 448x640 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2214847438_4993210d4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2687328779_b4356dab16.jpg: 480x640 2 dogs, 1 baseball bat, 1 bed, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2687328779_b4356dab16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286111436_891ae7dab9.jpg: 640x512 11 persons, 1 tie, 1 frisbee, 1 sports ball, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3286111436_891ae7dab9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595216998_0a19efebd0.jpg: 640x512 1 person, 1 dog, 8.4ms\n",
      "Speed: 2.3ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3595216998_0a19efebd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564436847_57825db87d.jpg: 448x640 1 person, 3 cars, 1 skateboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3564436847_57825db87d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423550887_ffc9bbcf71.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2423550887_ffc9bbcf71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1884727806_d84f209868.jpg: 640x448 7 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1884727806_d84f209868.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2278766574_f71f1704a8.jpg: 480x640 4 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2278766574_f71f1704a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296500180_0d7a6650dc.jpg: 448x640 1 person, 6 cars, 1 skateboard, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3296500180_0d7a6650dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/54501196_a9ac9d66f2.jpg: 480x640 1 person, 1 backpack, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 54501196_a9ac9d66f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2544426580_317b1f1f73.jpg: 640x480 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2544426580_317b1f1f73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3312779887_7682db7827.jpg: 416x640 17 persons, 1 surfboard, 12.1ms\n",
      "Speed: 1.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3312779887_7682db7827.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3044359043_627488ddf0.jpg: 416x640 1 person, 1 dog, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3044359043_627488ddf0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2255685792_f70474c6db.jpg: 640x480 3 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2255685792_f70474c6db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1445123245_c7b9db0e0c.jpg: 480x640 1 dog, 2 teddy bears, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1445123245_c7b9db0e0c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2649406158_ded6be38de.jpg: 448x640 1 horse, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2649406158_ded6be38de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244760289_f4467b2b67.jpg: 480x640 5 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244760289_f4467b2b67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2555535057_007501dae5.jpg: 640x576 1 dog, 8.1ms\n",
      "Speed: 2.4ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2555535057_007501dae5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1505686764_9e3bcd854a.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1505686764_9e3bcd854a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1207159468_425b902bfb.jpg: 416x640 2 persons, 1 umbrella, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1207159468_425b902bfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2698197294_ccd9327ef6.jpg: 512x640 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2698197294_ccd9327ef6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3334300164_e75e0479ae.jpg: 448x640 1 person, 1 skis, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3334300164_e75e0479ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2096771662_984441d20d.jpg: 448x640 3 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2096771662_984441d20d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222055946_45f7293bb2.jpg: 640x448 14 persons, 1 baseball glove, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3222055946_45f7293bb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/765298136_7805fbb079.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 765298136_7805fbb079.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1461329041_c623b06e5b.jpg: 480x640 2 persons, 1 bicycle, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1461329041_c623b06e5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380515798_c2abbf46b0.jpg: 448x640 12 persons, 1 umbrella, 1 banana, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 380515798_c2abbf46b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2700147489_f1664f2b61.jpg: 480x640 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2700147489_f1664f2b61.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649387275_75295baa28.jpg: 480x640 1 dog, 1 horse, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3649387275_75295baa28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2249865945_f432c8e5da.jpg: 448x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2249865945_f432c8e5da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3643684688_2f7157b23d.jpg: 448x640 6 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3643684688_2f7157b23d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2065309381_705b774f51.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2065309381_705b774f51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640241166_b1ab7a8e7a.jpg: 512x640 1 person, 1 car, 1 cow, 2 cell phones, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3640241166_b1ab7a8e7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631344685_ed0f3e091b.jpg: 640x448 5 persons, 1 car, 1 baseball bat, 1 baseball glove, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3631344685_ed0f3e091b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534886684_a6c9f40fa1.jpg: 448x640 1 person, 2 cars, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 534886684_a6c9f40fa1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2061944672_0383e65c8a.jpg: 384x640 6 persons, 1 bicycle, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2061944672_0383e65c8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1022454332_6af2c1449a.jpg: 480x640 3 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1022454332_6af2c1449a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488089932_c3a5fa4140.jpg: 448x640 17 persons, 1 frisbee, 2 chairs, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 488089932_c3a5fa4140.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475317104_1cdc1653b4.jpg: 640x576 3 persons, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 475317104_1cdc1653b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2529116152_4331dabf50.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2529116152_4331dabf50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/373394550_1b2296b8c4.jpg: 640x448 (no detections), 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 373394550_1b2296b8c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3675742996_02ccef16a3.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3675742996_02ccef16a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2923497185_c64004ff2d.jpg: 512x640 1 dog, 1 cow, 1 baseball bat, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2923497185_c64004ff2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/872135364_8c1e47d163.jpg: 640x640 1 cow, 1 elephant, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 872135364_8c1e47d163.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2807209904_389d81f33a.jpg: 640x384 8 persons, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2807209904_389d81f33a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2046222127_a6f300e202.jpg: 640x480 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2046222127_a6f300e202.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3132006797_04822b5866.jpg: 480x640 3 persons, 1 chair, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3132006797_04822b5866.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2380464803_a64f05bfa9.jpg: 640x448 3 persons, 2 chairs, 2 dining tables, 1 clock, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2380464803_a64f05bfa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2181846120_3744ca3942.jpg: 448x640 1 dog, 1 bed, 1 laptop, 1 keyboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2181846120_3744ca3942.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3148193539_de9dd48fc8.jpg: 640x480 5 persons, 8.5ms\n",
      "Speed: 1.8ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3148193539_de9dd48fc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3579842996_3a62ec1bc7.jpg: 640x512 1 person, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3579842996_3a62ec1bc7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380034515_4fbdfa6b26.jpg: 640x640 1 dog, 2 potted plants, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 380034515_4fbdfa6b26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246508774_1e9885f1b7.jpg: 640x480 9 persons, 1 backpack, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 246508774_1e9885f1b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2448393373_80c011d301.jpg: 640x416 1 person, 1 frisbee, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2448393373_80c011d301.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/197504190_fd1fc3d4b7.jpg: 448x640 2 persons, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 197504190_fd1fc3d4b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319847643_df7c2a1d25.jpg: 448x640 2 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 319847643_df7c2a1d25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2789937754_5d1fa62e95.jpg: 480x640 12 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2789937754_5d1fa62e95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2683985894_167d267dcb.jpg: 640x544 4 persons, 1 car, 1 tie, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2683985894_167d267dcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638992163_a085cc0c24.jpg: 448x640 1 dog, 1 frisbee, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3638992163_a085cc0c24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2902486045_6298eb22ef.jpg: 480x640 1 fire hydrant, 3 dogs, 11.0ms\n",
      "Speed: 2.9ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2902486045_6298eb22ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2917057791_3d68a055ca.jpg: 480x640 4 persons, 1 bench, 1 bird, 11.8ms\n",
      "Speed: 3.4ms preprocess, 11.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2917057791_3d68a055ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2647062476_5ef31ba867.jpg: 448x640 6 persons, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2647062476_5ef31ba867.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436063693_15c8d377a2.jpg: 640x448 1 dog, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3436063693_15c8d377a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2657643451_b9ddb0b58f.jpg: 640x640 1 person, 1 dog, 9.7ms\n",
      "Speed: 3.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2657643451_b9ddb0b58f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2298077331_f9a1488067.jpg: 640x480 1 car, 1 bird, 1 dog, 10.6ms\n",
      "Speed: 2.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2298077331_f9a1488067.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556390715_65c6d1e88b.jpg: 448x640 1 dog, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3556390715_65c6d1e88b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3380407617_07b53cbcce.jpg: 448x640 1 person, 1 snowboard, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3380407617_07b53cbcce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/742073622_1206be8f7f.jpg: 480x640 12 persons, 4 chairs, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 742073622_1206be8f7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2994107810_af56326389.jpg: 640x512 3 persons, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2994107810_af56326389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/428796930_476a3d6395.jpg: 576x640 1 dog, 10.0ms\n",
      "Speed: 2.9ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 428796930_476a3d6395.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1332722096_1e3de8ae70.jpg: 640x512 3 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1332722096_1e3de8ae70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3038045802_93f2cd5fbc.jpg: 448x640 1 skis, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3038045802_93f2cd5fbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1432342377_3e41603f26.jpg: 640x480 4 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1432342377_3e41603f26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873431806_86a56cdae8.jpg: 640x512 1 person, 1 car, 2 trucks, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2873431806_86a56cdae8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2894576909_99c85fd7a7.jpg: 448x640 1 person, 1 surfboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2894576909_99c85fd7a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934837034_a8ca5b1f50.jpg: 416x640 4 persons, 1 bicycle, 1 bench, 1 sports ball, 1 cup, 9.8ms\n",
      "Speed: 1.8ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2934837034_a8ca5b1f50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2271667421_7b21fc23b8.jpg: 448x640 7 persons, 3 backpacks, 3 skiss, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2271667421_7b21fc23b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2735792721_b8fe85e803.jpg: 640x448 1 person, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2735792721_b8fe85e803.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565685680_c30972455d.jpg: 448x640 (no detections), 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2565685680_c30972455d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/540873795_ae62ae6f60.jpg: 640x448 1 person, 1 bench, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 540873795_ae62ae6f60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649802021_8a689bc153.jpg: 640x448 1 dog, 1 bear, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3649802021_8a689bc153.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/381514859_b40418d9c3.jpg: 448x640 1 dog, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 381514859_b40418d9c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3686924335_3c51e8834a.jpg: 640x448 1 sheep, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3686924335_3c51e8834a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/476233374_e1396998ef.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 476233374_e1396998ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331525712_af1dcc47f2.jpg: 448x640 4 persons, 1 skateboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3331525712_af1dcc47f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2825540754_5e0c13e6b8.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2825540754_5e0c13e6b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2653552905_4301449235.jpg: 480x640 2 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2653552905_4301449235.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295391572_cbfde03a10.jpg: 448x640 5 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3295391572_cbfde03a10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/309238565_2d5d8dc8bf.jpg: 640x512 3 persons, 1 tv, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 309238565_2d5d8dc8bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/269986132_91b71e8aaa.jpg: 416x640 9 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 269986132_91b71e8aaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393410666_b8c20fff61.jpg: 416x640 1 person, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2393410666_b8c20fff61.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1463732807_0cdf4f22c7.jpg: 608x640 1 person, 7.7ms\n",
      "Speed: 2.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 1463732807_0cdf4f22c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264350290_f50494e835.jpg: 640x480 1 person, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3264350290_f50494e835.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2181117039_c4eea8036e.jpg: 448x640 7 persons, 13 birds, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2181117039_c4eea8036e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665549027_d7fb05d157.jpg: 416x640 1 person, 1 bird, 1 horse, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3665549027_d7fb05d157.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2773011586_6f4cd41e84.jpg: 480x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2773011586_6f4cd41e84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3616771728_2c16bf8d85.jpg: 448x640 3 persons, 2 horses, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3616771728_2c16bf8d85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3758787457_1a903ee1e9.jpg: 448x640 3 persons, 8.1ms\n",
      "Speed: 1.6ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3758787457_1a903ee1e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2905942129_2b4bf59bc0.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2905942129_2b4bf59bc0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3276895962_c053263d01.jpg: 352x640 1 dog, 2 cows, 9.2ms\n",
      "Speed: 1.3ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3276895962_c053263d01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3317960829_78bbfafbb6.jpg: 640x480 3 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3317960829_78bbfafbb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677693858_62f2f3163f.jpg: 640x544 1 person, 1 baseball bat, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3677693858_62f2f3163f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2578161080_e007c9177a.jpg: 480x640 6 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2578161080_e007c9177a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/510510783_b2cf5d57bb.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 510510783_b2cf5d57bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342855466_44038a8aa3.jpg: 416x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3342855466_44038a8aa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543940240_a54a3c7989.jpg: 448x640 4 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 543940240_a54a3c7989.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/140526326_da07305c1c.jpg: 640x384 1 person, 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 140526326_da07305c1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488408004_a1e26d4886.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 488408004_a1e26d4886.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/632608471_a70461f123.jpg: 448x640 8 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 632608471_a70461f123.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3702038926_966fdaa311.jpg: 480x640 6 persons, 1 bicycle, 1 tennis racket, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3702038926_966fdaa311.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176131893_7181c733aa.jpg: 448x640 4 persons, 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3176131893_7181c733aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3212456649_40a3052682.jpg: 480x640 1 dog, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3212456649_40a3052682.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3723690961_729dd5d617.jpg: 640x480 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3723690961_729dd5d617.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2828583747_8cfb7217af.jpg: 512x640 2 persons, 5 cars, 1 motorcycle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2828583747_8cfb7217af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3703960010_1e4c922a25.jpg: 640x448 2 persons, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3703960010_1e4c922a25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578981202_efef47e264.jpg: 640x512 2 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3578981202_efef47e264.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265578645_4044a7049a.jpg: 640x448 9 persons, 1 car, 1 motorcycle, 1 bus, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3265578645_4044a7049a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831672255_d779807c14.jpg: 640x416 1 dog, 7.8ms\n",
      "Speed: 1.5ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2831672255_d779807c14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134585858_a8c3493ca5.jpg: 640x512 3 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3134585858_a8c3493ca5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1998457059_c9ac9a1e1a.jpg: 480x640 15 persons, 1 surfboard, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1998457059_c9ac9a1e1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1659358133_95cd1027bd.jpg: 640x640 1 dog, 7.8ms\n",
      "Speed: 2.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1659358133_95cd1027bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/635444010_bd81c89ab7.jpg: 640x480 1 person, 3 cars, 1 bus, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 635444010_bd81c89ab7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/416960865_048fd3f294.jpg: 448x640 1 person, 9.4ms\n",
      "Speed: 1.6ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 416960865_048fd3f294.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2100816230_ff866fb352.jpg: 640x448 1 person, 1 skis, 1 kite, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2100816230_ff866fb352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518755601_cebf11e515.jpg: 416x640 6 persons, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3518755601_cebf11e515.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2971478694_79e46ea7e5.jpg: 448x640 15 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2971478694_79e46ea7e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/272283076_2d4aa1d5cf.jpg: 480x640 2 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 272283076_2d4aa1d5cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535304540_0247e8cf8c.jpg: 480x640 1 airplane, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3535304540_0247e8cf8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527682660_c5e9fa644a.jpg: 448x640 4 persons, 2 chairs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3527682660_c5e9fa644a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2788945468_74a9618cfa.jpg: 640x640 2 persons, 7.4ms\n",
      "Speed: 2.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2788945468_74a9618cfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150252702_828a570d46.jpg: 448x640 1 person, 2 snowboards, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3150252702_828a570d46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3162442331_c9711857c6.jpg: 448x640 2 persons, 3 elephants, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3162442331_c9711857c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356284586_21c6f155a5.jpg: 640x480 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3356284586_21c6f155a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2493974889_50ae29f1e1.jpg: 640x448 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2493974889_50ae29f1e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3101796900_59c15e0edc.jpg: 416x640 1 dog, 1 horse, 1 frisbee, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3101796900_59c15e0edc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/641893280_36fd6e886a.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 641893280_36fd6e886a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532200762_b28c39d311.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3532200762_b28c39d311.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1187435567_18173c148b.jpg: 480x640 1 dog, 1 chair, 1 bed, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1187435567_18173c148b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2527303359_6c3dc3f282.jpg: 480x640 2 persons, 1 sports ball, 1 surfboard, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2527303359_6c3dc3f282.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2787276494_82703f570a.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2787276494_82703f570a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3266306177_7994dc2865.jpg: 448x640 1 horse, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3266306177_7994dc2865.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437147889_4cf26dd525.jpg: 448x640 2 persons, 2 motorcycles, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3437147889_4cf26dd525.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2613021139_4b0dc3d4c8.jpg: 448x640 1 person, 1 skateboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2613021139_4b0dc3d4c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041487045_b48ac7ed08.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3041487045_b48ac7ed08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315033940_e91f87b7f2.jpg: 448x640 1 person, 1 boat, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3315033940_e91f87b7f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3076052114_233f42ae5b.jpg: 448x640 1 dog, 2 sheeps, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3076052114_233f42ae5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/136886677_6026c622eb.jpg: 576x640 1 boat, 1 bird, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 136886677_6026c622eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030824089_e5a840265e.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3030824089_e5a840265e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3179952488_c1c812a03b.jpg: 448x640 18 persons, 4 ties, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3179952488_c1c812a03b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1418503947_953d373632.jpg: 640x480 7 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1418503947_953d373632.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102360862_264452db8e.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 3.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2102360862_264452db8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/44856031_0d82c2c7d1.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 44856031_0d82c2c7d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2503629305_055e9ec4b1.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2503629305_055e9ec4b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3757598567_739b7da835.jpg: 448x640 1 person, 1 toothbrush, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3757598567_739b7da835.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134586018_ae03ba20a0.jpg: 512x640 4 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3134586018_ae03ba20a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/58363930_0544844edd.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 58363930_0544844edd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348385580_10b53391f9.jpg: 640x448 1 person, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3348385580_10b53391f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/297724467_e8918a6f90.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 297724467_e8918a6f90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2754898893_95239c1f19.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2754898893_95239c1f19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1348891916_ebd4413033.jpg: 480x640 1 dog, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1348891916_ebd4413033.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3281611946_f42deed2e1.jpg: 480x640 5 persons, 1 tie, 1 wine glass, 1 cup, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3281611946_f42deed2e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1263801010_5c74bf1715.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1263801010_5c74bf1715.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2483993772_f64e9e4724.jpg: 640x480 1 car, 1 dog, 1 sports ball, 15.6ms\n",
      "Speed: 3.6ms preprocess, 15.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2483993772_f64e9e4724.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665996775_6d7d9a46f1.jpg: 640x448 1 person, 12.9ms\n",
      "Speed: 2.6ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3665996775_6d7d9a46f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672940355_47f30e2b28.jpg: 640x640 1 person, 1 sports ball, 1 baseball glove, 11.7ms\n",
      "Speed: 3.0ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3672940355_47f30e2b28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/633456174_b768c1d6cd.jpg: 640x480 1 person, 11.6ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 633456174_b768c1d6cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1022975728_75515238d8.jpg: 384x640 1 dog, 12.8ms\n",
      "Speed: 1.8ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1022975728_75515238d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204922011_185e48949a.jpg: 448x640 1 person, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3204922011_185e48949a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2759089516_cbb993cb92.jpg: 640x512 1 person, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2759089516_cbb993cb92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3252457866_b86614064c.jpg: 384x640 1 dog, 1 cow, 12.1ms\n",
      "Speed: 1.9ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3252457866_b86614064c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/155221027_b23a4331b7.jpg: 448x640 4 persons, 2 kites, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 155221027_b23a4331b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3683644335_b70bed1d83.jpg: 256x640 1 person, 13.0ms\n",
      "Speed: 1.3ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 3683644335_b70bed1d83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3051341320_1d0166e775.jpg: 512x640 (no detections), 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 0.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3051341320_1d0166e775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084731832_8e518e320d.jpg: 640x480 1 dog, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3084731832_8e518e320d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2251418114_2b0cd4c139.jpg: 640x448 1 person, 13.8ms\n",
      "Speed: 2.1ms preprocess, 13.8ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2251418114_2b0cd4c139.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877503811_4e311253ec.jpg: 480x640 11 persons, 11.9ms\n",
      "Speed: 2.3ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2877503811_4e311253ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556037801_3992ce6826.jpg: 480x640 3 persons, 1 bicycle, 1 handbag, 10.6ms\n",
      "Speed: 2.2ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3556037801_3992ce6826.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347580_a1e20321d3.jpg: 448x640 7 persons, 11.7ms\n",
      "Speed: 2.1ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347580_a1e20321d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3553225222_f5ebe44af1.jpg: 640x448 1 person, 11.7ms\n",
      "Speed: 2.1ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3553225222_f5ebe44af1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3378553508_e37e281d25.jpg: 384x640 3 persons, 1 traffic light, 12.4ms\n",
      "Speed: 1.9ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3378553508_e37e281d25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2261550615_b6c25d987b.jpg: 480x640 8 persons, 11.6ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2261550615_b6c25d987b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/505955292_026f1489f2.jpg: 480x640 1 person, 1 elephant, 10.5ms\n",
      "Speed: 2.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 505955292_026f1489f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3447155358_5b5b59b15e.jpg: 640x448 1 person, 1 tennis racket, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3447155358_5b5b59b15e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2593695271_4d9cc9bd6f.jpg: 640x448 4 persons, 1 backpack, 2 umbrellas, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2593695271_4d9cc9bd6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/825918657_d92f1761f4.jpg: 448x640 1 person, 1 car, 1 dog, 4 frisbees, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 825918657_d92f1761f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621741935_54d243f25f.jpg: 640x448 2 persons, 1 motorcycle, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3621741935_54d243f25f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2663794355_e726ec7e05.jpg: 416x640 4 persons, 1 backpack, 9.7ms\n",
      "Speed: 1.8ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2663794355_e726ec7e05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2367139509_1ee4530b28.jpg: 640x480 1 dog, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2367139509_1ee4530b28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2874984466_1aafec2c9f.jpg: 448x640 1 dog, 1 sheep, 2 sports balls, 9.3ms\n",
      "Speed: 1.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2874984466_1aafec2c9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1473618073_7db56a5237.jpg: 640x480 2 persons, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1473618073_7db56a5237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3704698586_a42c25d0c1.jpg: 448x640 5 persons, 2 bicycles, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3704698586_a42c25d0c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/883040210_3c4a10f030.jpg: 640x480 1 person, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 883040210_3c4a10f030.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3468130925_2b1489d19a.jpg: 640x608 7 persons, 1 chair, 10.3ms\n",
      "Speed: 2.3ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3468130925_2b1489d19a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2896298341_92d718366a.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 1.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2896298341_92d718366a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2255342813_5b2ac6d633.jpg: 640x448 (no detections), 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2255342813_5b2ac6d633.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1260816604_570fc35836.jpg: 448x640 1 person, 2 cars, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1260816604_570fc35836.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532131603_c82d454c8a.jpg: 448x640 1 dog, 8.5ms\n",
      "Speed: 1.8ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 532131603_c82d454c8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862469183_a4334b904a.jpg: 448x640 1 dog, 1 cow, 8.5ms\n",
      "Speed: 3.8ms preprocess, 8.5ms inference, 2.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2862469183_a4334b904a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240351042_5d29c94b0e.jpg: 640x608 2 persons, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3240351042_5d29c94b0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2097420505_439f63c863.jpg: 480x640 1 person, 9.3ms\n",
      "Speed: 1.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2097420505_439f63c863.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3252588185_3210fe94be.jpg: 448x640 1 person, 9.5ms\n",
      "Speed: 1.7ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3252588185_3210fe94be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171651115_e07b9d08f6.jpg: 448x640 1 person, 1 surfboard, 8.4ms\n",
      "Speed: 1.8ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3171651115_e07b9d08f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/812196663_0c969970b5.jpg: 480x640 2 persons, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 812196663_0c969970b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2751602672_ca5e1f6447.jpg: 448x640 1 person, 1 dog, 11.3ms\n",
      "Speed: 2.8ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2751602672_ca5e1f6447.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2972864304_481aeffe50.jpg: 640x640 1 person, 10.0ms\n",
      "Speed: 3.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2972864304_481aeffe50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214151585_f2d0b00b41.jpg: 480x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3214151585_f2d0b00b41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2497074804_b4f3e7fd90.jpg: 640x480 14 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2497074804_b4f3e7fd90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016708786_3591106cca.jpg: 640x480 5 persons, 1 tie, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3016708786_3591106cca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/269630255_c3ec75c792.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 269630255_c3ec75c792.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3195969533_98f5de0fab.jpg: 480x640 3 dogs, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3195969533_98f5de0fab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3170551725_1276644eab.jpg: 640x480 4 persons, 1 snowboard, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3170551725_1276644eab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3114944484_28b5bb9842.jpg: 448x640 16 persons, 1 chair, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3114944484_28b5bb9842.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2806710650_e201acd913.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2806710650_e201acd913.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690883532_d883f34617.jpg: 448x640 3 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3690883532_d883f34617.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2405599120_ec5f32af6f.jpg: 416x640 1 person, 1 suitcase, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2405599120_ec5f32af6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1273001772_1585562051.jpg: 480x640 14 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1273001772_1585562051.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2140747429_62cfd89ae9.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2140747429_62cfd89ae9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423085253_6c19149855.jpg: 640x448 1 person, 1 kite, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2423085253_6c19149855.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/353180303_6a24179c50.jpg: 448x640 3 persons, 1 donut, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 353180303_6a24179c50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3679502342_7fe6ef8a36.jpg: 448x640 5 persons, 5 cars, 1 bus, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3679502342_7fe6ef8a36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347803_afb04b12c4.jpg: 640x448 4 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347803_afb04b12c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/398413603_166896900f.jpg: 640x416 1 cow, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 398413603_166896900f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1119463452_69d4eecd08.jpg: 480x640 1 person, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1119463452_69d4eecd08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/205682549_713aa6cd88.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 205682549_713aa6cd88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/432248727_e7b623adbf.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 432248727_e7b623adbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3396157719_6807d52a81.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3396157719_6807d52a81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2932498509_27cb0038ec.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2932498509_27cb0038ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1024138940_f1fefbdce1.jpg: 448x640 2 dogs, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1024138940_f1fefbdce1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1267711451_e2a754b4f8.jpg: 448x640 1 cow, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1267711451_e2a754b4f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584534971_b44f82c4b9.jpg: 448x640 4 persons, 1 bicycle, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584534971_b44f82c4b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300274198_eefd8e057e.jpg: 640x640 1 dog, 1 bowl, 1 teddy bear, 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 300274198_eefd8e057e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376435746_1593d9b243.jpg: 448x640 2 persons, 1 boat, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376435746_1593d9b243.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2111360187_d2505437b7.jpg: 640x480 2 persons, 2 teddy bears, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2111360187_d2505437b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3168796547_0c14b368f9.jpg: 640x448 2 persons, 1 horse, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3168796547_0c14b368f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272548482_0b4aec5cdd.jpg: 448x640 7 persons, 2 chairs, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2272548482_0b4aec5cdd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2256218522_53b92bcbb2.jpg: 448x640 4 persons, 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2256218522_53b92bcbb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520079657_b828d96d50.jpg: 384x640 9 persons, 8.6ms\n",
      "Speed: 1.5ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3520079657_b828d96d50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831314869_5025300133.jpg: 512x640 3 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2831314869_5025300133.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2293149170_38fb2257ea.jpg: 448x640 2 persons, 1 bed, 2 teddy bears, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2293149170_38fb2257ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2930514856_784f17064a.jpg: 416x640 3 persons, 2 surfboards, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2930514856_784f17064a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2929669711_b2d5a640f0.jpg: 416x640 1 person, 1 surfboard, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2929669711_b2d5a640f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597921737_3fd1d0665b.jpg: 640x640 4 persons, 7.1ms\n",
      "Speed: 2.3ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3597921737_3fd1d0665b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3526897578_3cf77da99b.jpg: 448x640 1 dog, 1 frisbee, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3526897578_3cf77da99b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/69710411_2cf537f61f.jpg: 480x640 6 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 69710411_2cf537f61f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2756591658_3ca6db1595.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2756591658_3ca6db1595.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1527333441_af65636a74.jpg: 640x384 4 persons, 1 chair, 8.4ms\n",
      "Speed: 1.5ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 1527333441_af65636a74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/411011549_1298d2b4d2.jpg: 448x640 1 bird, 1 dog, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 411011549_1298d2b4d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/308487515_7852928f90.jpg: 512x640 1 bird, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 308487515_7852928f90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2657663775_bc98bf67ac.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2657663775_bc98bf67ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348208268_6d97d951eb.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3348208268_6d97d951eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262681159_e5fed3acf0.jpg: 640x576 6 persons, 7.9ms\n",
      "Speed: 2.1ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 262681159_e5fed3acf0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/872512911_ca383b40e4.jpg: 416x640 14 persons, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 872512911_ca383b40e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3127629248_a955b5763b.jpg: 640x512 10 persons, 1 chair, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3127629248_a955b5763b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347547_902725b9f8.jpg: 640x448 12 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347547_902725b9f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/56494233_1824005879.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 56494233_1824005879.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2507312812_768b53b023.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2507312812_768b53b023.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537390477_7dd3407f96.jpg: 480x640 3 dogs, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 537390477_7dd3407f96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3228517564_74b00a923b.jpg: 640x448 2 persons, 2 motorcycles, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3228517564_74b00a923b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3652572138_34d6b72999.jpg: 640x448 4 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3652572138_34d6b72999.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/295258727_eaf75e0887.jpg: 448x640 1 bird, 2 cows, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 295258727_eaf75e0887.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2435166927_28b8130660.jpg: 640x640 5 persons, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2435166927_28b8130660.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3414734842_beb543f400.jpg: 512x640 3 persons, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3414734842_beb543f400.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444057017_f1e0fcaef7.jpg: 480x640 1 person, 1 baseball bat, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 444057017_f1e0fcaef7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3402081035_a54cfab1d9.jpg: 544x640 2 persons, 1 umbrella, 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3402081035_a54cfab1d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/571130875_30051ac02d.jpg: 480x640 13 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 571130875_30051ac02d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3103340819_46de7954a9.jpg: 640x640 2 persons, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3103340819_46de7954a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2127031632_77505e4218.jpg: 640x448 1 person, 1 boat, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2127031632_77505e4218.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1814391289_83a1eb71d3.jpg: 640x480 1 person, 1 bench, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1814391289_83a1eb71d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3453313865_1ebff5393c.jpg: 448x640 4 persons, 1 skateboard, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3453313865_1ebff5393c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2196050115_e236d91f52.jpg: 448x640 1 person, 1 baseball bat, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2196050115_e236d91f52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/530888330_a18343e38d.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 530888330_a18343e38d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2496399593_a24954a5ca.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2496399593_a24954a5ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/583087629_a09334e1fb.jpg: 480x640 1 person, 1 car, 1 frisbee, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 583087629_a09334e1fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372022051_132b8e6233.jpg: 640x448 1 person, 1 motorcycle, 1 skateboard, 1 potted plant, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3372022051_132b8e6233.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/611910909_4f8e43e070.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 611910909_4f8e43e070.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3657503733_9888ccf05e.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3657503733_9888ccf05e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/418667611_b9995000f4.jpg: 448x640 2 persons, 1 surfboard, 10.0ms\n",
      "Speed: 1.6ms preprocess, 10.0ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 418667611_b9995000f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2402744031_11f9f2f2b8.jpg: 640x448 5 persons, 1 handbag, 4 wine glasss, 1 dining table, 12.3ms\n",
      "Speed: 3.3ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2402744031_11f9f2f2b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/528500099_7be78a0ca5.jpg: 640x448 1 person, 9.7ms\n",
      "Speed: 2.4ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 528500099_7be78a0ca5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3557316485_574a5f7a89.jpg: 640x448 1 person, 1 skateboard, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3557316485_574a5f7a89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2916009941_34a0013803.jpg: 640x448 5 persons, 1 bench, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2916009941_34a0013803.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242808166_8638150274.jpg: 448x640 1 motorcycle, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3242808166_8638150274.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/280932151_ae14a67be5.jpg: 448x640 1 dog, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 280932151_ae14a67be5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2081679622_6f1442367d.jpg: 416x640 1 person, 1 couch, 1 bed, 1 tv, 13.3ms\n",
      "Speed: 2.0ms preprocess, 13.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2081679622_6f1442367d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/918886676_3323fb2a01.jpg: 480x640 1 bear, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 918886676_3323fb2a01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3678100844_e3a9802471.jpg: 448x640 13 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3678100844_e3a9802471.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025438110_40af7e6a80.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3025438110_40af7e6a80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1346529555_e916816cfe.jpg: 640x480 1 dog, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1346529555_e916816cfe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631023049_8a85bab19f.jpg: 448x640 4 persons, 1 car, 1 bench, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3631023049_8a85bab19f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2774705720_1cb85812dc.jpg: 480x640 2 persons, 1 bench, 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2774705720_1cb85812dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1594038143_57f299aa8a.jpg: 480x640 4 persons, 8.7ms\n",
      "Speed: 2.4ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1594038143_57f299aa8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532194771_07faf20d76.jpg: 448x640 2 persons, 3 surfboards, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532194771_07faf20d76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/609681901_66809d2dc1.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 609681901_66809d2dc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866974237_e3c1e267c0.jpg: 640x480 6 persons, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2866974237_e3c1e267c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/384465370_9918873f9a.jpg: 480x640 2 persons, 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 384465370_9918873f9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171188674_717eee0183.jpg: 512x640 15 persons, 1 backpack, 9.1ms\n",
      "Speed: 1.9ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3171188674_717eee0183.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2481003841_06086eafc2.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2481003841_06086eafc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241109594_3cb90fe2a3.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241109594_3cb90fe2a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2448210587_9fe7ea5f42.jpg: 480x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2448210587_9fe7ea5f42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597326009_3678a98a43.jpg: 448x640 4 persons, 1 bench, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3597326009_3678a98a43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2260649048_ae45d17e68.jpg: 448x640 2 dogs, 1 cow, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2260649048_ae45d17e68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3504158556_1d410c8ff7.jpg: 448x640 5 persons, 1 truck, 1 bench, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3504158556_1d410c8ff7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/576093768_e78f91c176.jpg: 416x640 6 persons, 1 handbag, 1 dining table, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 576093768_e78f91c176.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2739332078_13d9acce59.jpg: 640x448 2 persons, 1 umbrella, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2739332078_13d9acce59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/216172386_9ac5356dae.jpg: 448x640 1 person, 1 motorcycle, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 216172386_9ac5356dae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2612040125_0a93889f06.jpg: 480x640 1 dog, 1 horse, 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2612040125_0a93889f06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2795866891_7559fd8422.jpg: 640x448 1 person, 1 car, 1 skateboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2795866891_7559fd8422.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1917265421_aeccf1ca38.jpg: 448x640 31 persons, 3 cars, 1 bus, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1917265421_aeccf1ca38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535056297_e16f014cb7.jpg: 416x640 13 persons, 7.5ms\n",
      "Speed: 1.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3535056297_e16f014cb7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3528251308_481a28283a.jpg: 448x640 1 person, 1 skateboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3528251308_481a28283a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744330402_824240184c.jpg: 448x640 7 persons, 1 boat, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2744330402_824240184c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/766099402_cdda6964f0.jpg: 480x640 2 persons, 1 kite, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 766099402_cdda6964f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2569643552_23696a9ba5.jpg: 640x448 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2569643552_23696a9ba5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667318593_fa1816b346.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3667318593_fa1816b346.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2720215226_5a98ff2bd3.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2720215226_5a98ff2bd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342271377_446ffc34c0.jpg: 480x640 7 persons, 1 handbag, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3342271377_446ffc34c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345721_3f3724a7fc.jpg: 448x640 11 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345721_3f3724a7fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423856014_8df0e7f656.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2423856014_8df0e7f656.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393264648_a280744f97.jpg: 256x640 2 persons, 8.1ms\n",
      "Speed: 0.9ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2393264648_a280744f97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2802337003_56e555cd30.jpg: 640x480 1 fire hydrant, 14.9ms\n",
      "Speed: 3.3ms preprocess, 14.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2802337003_56e555cd30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2952141476_fc9a48a60a.jpg: 384x640 1 person, 1 surfboard, 13.1ms\n",
      "Speed: 2.2ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2952141476_fc9a48a60a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2178095150_436b035741.jpg: 448x640 1 person, 1 dog, 11.8ms\n",
      "Speed: 2.2ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2178095150_436b035741.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2832033116_1677ea1e2e.jpg: 448x640 1 person, 1 scissors, 12.8ms\n",
      "Speed: 3.5ms preprocess, 12.8ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2832033116_1677ea1e2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3017656907_c3b137e070.jpg: 448x640 3 persons, 1 skateboard, 10.5ms\n",
      "Speed: 2.7ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3017656907_c3b137e070.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3711030008_3872d0b03f.jpg: 448x640 11 persons, 1 umbrella, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3711030008_3872d0b03f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2952320230_26601173be.jpg: 640x480 3 persons, 2 tennis rackets, 1 chair, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2952320230_26601173be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3594029059_cee1f4c59a.jpg: 448x640 1 bird, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3594029059_cee1f4c59a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2226587791_66e29dd01d.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2226587791_66e29dd01d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430779304_43a2146f4b.jpg: 512x640 2 persons, 1 dog, 3 horses, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3430779304_43a2146f4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887171449_f54a2b9f39.jpg: 416x640 4 persons, 1 handbag, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2887171449_f54a2b9f39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583065748_7d149a865c.jpg: 480x640 2 dogs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3583065748_7d149a865c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2343879696_59a82f496f.jpg: 640x480 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2343879696_59a82f496f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2580215443_4e64afe3d5.jpg: 512x640 15 persons, 1 handbag, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2580215443_4e64afe3d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3435233065_3411f2d29d.jpg: 640x448 1 person, 1 bicycle, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3435233065_3411f2d29d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2893374123_087f98d58a.jpg: 448x640 1 person, 1 motorcycle, 1 boat, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2893374123_087f98d58a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3184891327_8785194e3c.jpg: 448x640 10 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3184891327_8785194e3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/102351840_323e3de834.jpg: 256x640 3 persons, 1 boat, 1 skis, 10.1ms\n",
      "Speed: 1.3ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 102351840_323e3de834.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2045562030_654ddea5e5.jpg: 640x448 4 persons, 1 handbag, 1 sports ball, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2045562030_654ddea5e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3464708890_3cab754998.jpg: 640x448 1 person, 2 skateboards, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3464708890_3cab754998.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2437266971_b91a8f9a00.jpg: 640x576 2 persons, 1 tie, 1 book, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2437266971_b91a8f9a00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2642475077_69d19deb74.jpg: 448x640 2 persons, 1 cell phone, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2642475077_69d19deb74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2355880294_8f78a6fea6.jpg: 448x640 2 persons, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2355880294_8f78a6fea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/427936315_0b8f7b8d23.jpg: 448x640 1 dog, 1 horse, 1 frisbee, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 427936315_0b8f7b8d23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/430964917_022995afb6.jpg: 512x640 1 person, 1 bicycle, 3 cars, 1 backpack, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 430964917_022995afb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/404850242_3a55a4c874.jpg: 448x640 3 persons, 8.6ms\n",
      "Speed: 2.4ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 404850242_3a55a4c874.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/799486353_f665d7b0f0.jpg: 640x448 1 person, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 799486353_f665d7b0f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2971431335_e192613db4.jpg: 640x448 1 person, 1 kite, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2971431335_e192613db4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2399219552_bbba0a9a59.jpg: 448x640 1 person, 1 sports ball, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2399219552_bbba0a9a59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2600867924_cd502fc911.jpg: 448x640 2 dogs, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2600867924_cd502fc911.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2199083344_3aa77f4879.jpg: 640x384 7 persons, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2199083344_3aa77f4879.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3488837187_0c7264a16c.jpg: 640x448 1 person, 1 teddy bear, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3488837187_0c7264a16c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2277081067_d2b4c98bce.jpg: 640x480 2 persons, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2277081067_d2b4c98bce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2465441099_a1761a1757.jpg: 480x640 8 persons, 1 handbag, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2465441099_a1761a1757.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3451345621_fe470d4cf8.jpg: 640x448 4 persons, 1 remote, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3451345621_fe470d4cf8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3608663656_9192a21eeb.jpg: 640x448 1 person, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3608663656_9192a21eeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/937559727_ae2613cee5.jpg: 640x448 1 person, 1 cell phone, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 937559727_ae2613cee5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1597557856_30640e0b43.jpg: 640x448 1 person, 2 dogs, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1597557856_30640e0b43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3178489390_13a6ae7524.jpg: 448x640 3 persons, 4 birds, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3178489390_13a6ae7524.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/225909073_25c3c33a29.jpg: 448x640 1 dog, 9.8ms\n",
      "Speed: 2.5ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 225909073_25c3c33a29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/130211457_be3f6b335d.jpg: 480x640 6 persons, 1 motorcycle, 4 umbrellas, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 130211457_be3f6b335d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2716251485_d6113f4c8a.jpg: 480x640 1 person, 2 cars, 1 traffic light, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2716251485_d6113f4c8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726076549_0efb38854b.jpg: 448x640 1 person, 1 bicycle, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3726076549_0efb38854b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506738508_327efdf9c3.jpg: 640x480 3 persons, 1 bottle, 1 cup, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 506738508_327efdf9c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/360723732_23199af4bf.jpg: 608x640 2 dogs, 7.8ms\n",
      "Speed: 2.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 360723732_23199af4bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/448916362_17f3f1d0e1.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 448916362_17f3f1d0e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273028514_d7b584f73d.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2273028514_d7b584f73d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566794036_60f7acdf35.jpg: 640x480 1 person, 1 sports ball, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 566794036_60f7acdf35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1412832223_99e8b4701a.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1412832223_99e8b4701a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3435653630_3b6cca2c40.jpg: 640x448 5 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3435653630_3b6cca2c40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3257107194_f235c8f7ab.jpg: 512x640 4 persons, 1 elephant, 8.4ms\n",
      "Speed: 1.8ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3257107194_f235c8f7ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3718007650_e5930b4509.jpg: 544x640 13 persons, 1 teddy bear, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3718007650_e5930b4509.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3308997740_91765ecdcc.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3308997740_91765ecdcc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517466790_17c7753a1a.jpg: 448x640 15 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3517466790_17c7753a1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3067885047_f69d90c35b.jpg: 448x640 4 persons, 2 frisbees, 1 sports ball, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3067885047_f69d90c35b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3027399066_ca85495775.jpg: 448x640 (no detections), 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3027399066_ca85495775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458559770_12cf9f134e.jpg: 448x640 5 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3458559770_12cf9f134e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/235065283_1f9a3c79db.jpg: 480x640 1 person, 1 backpack, 13.0ms\n",
      "Speed: 3.7ms preprocess, 13.0ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 235065283_1f9a3c79db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3391209042_d2de8a8978.jpg: 448x640 2 persons, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3391209042_d2de8a8978.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/358607894_5abb1250d3.jpg: 640x608 2 persons, 11.2ms\n",
      "Speed: 3.2ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 358607894_5abb1250d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3392293702_ccb0599857.jpg: 640x480 1 dog, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3392293702_ccb0599857.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2805822564_6dee48e506.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2805822564_6dee48e506.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2527713011_b0ec25aa54.jpg: 448x640 9 persons, 1 car, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2527713011_b0ec25aa54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/523692399_d2e261a302.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 523692399_d2e261a302.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/989754491_7e53fb4586.jpg: 480x640 4 persons, 1 sports ball, 1 cell phone, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 989754491_7e53fb4586.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264397357_72f084cac1.jpg: 384x640 1 person, 1 bear, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3264397357_72f084cac1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3250695024_93e8ab7305.jpg: 480x640 4 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3250695024_93e8ab7305.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/403523132_73b9a1a4b3.jpg: 480x640 1 dog, 1 sports ball, 1 bed, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 403523132_73b9a1a4b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348384389_73b6647017.jpg: 448x640 2 dogs, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3348384389_73b6647017.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978810122_183e60ff2d.jpg: 448x640 1 person, 2 surfboards, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2978810122_183e60ff2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2043427251_83b746da8e.jpg: 448x640 3 persons, 2 cars, 10.5ms\n",
      "Speed: 2.8ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2043427251_83b746da8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1787222774_d5c68cce53.jpg: 448x640 2 persons, 2 cars, 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1787222774_d5c68cce53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3110614694_fecc23ca65.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3110614694_fecc23ca65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2918653119_f535fc25c4.jpg: 640x448 1 person, 1 skateboard, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2918653119_f535fc25c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122497129_d08f5729b8.jpg: 448x640 2 dogs, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3122497129_d08f5729b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2588625139_fdf6610218.jpg: 480x640 2 persons, 1 frisbee, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2588625139_fdf6610218.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3243591844_791cfa62eb.jpg: 512x640 14 persons, 9.2ms\n",
      "Speed: 2.5ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3243591844_791cfa62eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/938162709_21443d352f.jpg: 640x480 1 person, 1 backpack, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 938162709_21443d352f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229282764_a4a515f4e2.jpg: 448x640 2 persons, 1 tennis racket, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3229282764_a4a515f4e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2712974062_6d5b6aa7f0.jpg: 640x512 1 person, 2 cups, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2712974062_6d5b6aa7f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2101808682_0d66ef4a08.jpg: 512x640 1 person, 5 birds, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2101808682_0d66ef4a08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1469358746_2a879abaf3.jpg: 480x640 2 persons, 1 dog, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1469358746_2a879abaf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1026685415_0431cbf574.jpg: 448x640 1 elephant, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1026685415_0431cbf574.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3071676551_a65741e372.jpg: 448x640 1 person, 1 surfboard, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3071676551_a65741e372.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1454678644_7e5a371301.jpg: 480x640 4 dogs, 1 frisbee, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1454678644_7e5a371301.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3018304737_0a46fc5f1d.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3018304737_0a46fc5f1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/427082246_5bf1c3676f.jpg: 576x640 3 persons, 9.8ms\n",
      "Speed: 2.7ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 427082246_5bf1c3676f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042405316_ba3a01926b.jpg: 640x480 1 person, 1 truck, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3042405316_ba3a01926b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3492734013_e6b177ed99.jpg: 448x640 10 persons, 13 birds, 1 handbag, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3492734013_e6b177ed99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3357708906_fb3a54dd78.jpg: 480x640 4 persons, 2 dogs, 3 sheeps, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3357708906_fb3a54dd78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656206975_09e6ce58bd.jpg: 448x640 1 person, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3656206975_09e6ce58bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3469585782_e708496552.jpg: 448x640 9 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3469585782_e708496552.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3730457171_e66dde8c91.jpg: 640x480 1 person, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3730457171_e66dde8c91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1454841725_4b6e6199e2.jpg: 448x640 14 persons, 1 chair, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1454841725_4b6e6199e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1515025681_999199cb79.jpg: 640x480 3 persons, 1 umbrella, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1515025681_999199cb79.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373243733_9aba7740ed.jpg: 448x640 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3373243733_9aba7740ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2496236371_61dec88113.jpg: 448x640 1 person, 1 motorcycle, 1 bus, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2496236371_61dec88113.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2458862292_466a920ee2.jpg: 448x640 1 person, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2458862292_466a920ee2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3684680947_f1c460242f.jpg: 416x640 1 person, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3684680947_f1c460242f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3310551665_15b79ef4ea.jpg: 640x480 2 persons, 1 kite, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3310551665_15b79ef4ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2378149488_648e5deeac.jpg: 640x576 1 person, 1 tv, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2378149488_648e5deeac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/108899015_bf36131a57.jpg: 480x640 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 108899015_bf36131a57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2129430111_338a94f8fb.jpg: 448x640 1 person, 1 backpack, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2129430111_338a94f8fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2665461736_595c87f0a3.jpg: 480x640 5 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2665461736_595c87f0a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/661546153_9d30db6984.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 661546153_9d30db6984.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2696866120_254a0345bc.jpg: 480x640 4 sheeps, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2696866120_254a0345bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3557148230_7fc843e5de.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3557148230_7fc843e5de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2666179615_f05a9d8331.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2666179615_f05a9d8331.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892413015_5ecd9d972a.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2892413015_5ecd9d972a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/837893113_81854e94e3.jpg: 480x640 1 person, 1 frisbee, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 837893113_81854e94e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2385871165_9438c9fe84.jpg: 448x640 1 person, 1 tie, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2385871165_9438c9fe84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3509575615_653cbf01fc.jpg: 512x640 7 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3509575615_653cbf01fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430526230_234b3550f6.jpg: 480x640 2 cows, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3430526230_234b3550f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2647049174_0fb47cee2e.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2647049174_0fb47cee2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3675685612_3987d91d92.jpg: 448x640 1 person, 1 cell phone, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3675685612_3987d91d92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2470493181_2efbbf17bd.jpg: 480x640 13 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2470493181_2efbbf17bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/242064301_a9d12f1754.jpg: 608x640 1 person, 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 242064301_a9d12f1754.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3353278454_2f3a4d0bbc.jpg: 640x448 4 persons, 4 cars, 1 traffic light, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3353278454_2f3a4d0bbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/414773731_c3f5bf43d5.jpg: 512x640 1 dog, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 414773731_c3f5bf43d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3704431444_f337ec2b90.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3704431444_f337ec2b90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293596075_973b0bfd08.jpg: 480x640 1 cat, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3293596075_973b0bfd08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3671851846_60c25269df.jpg: 448x640 1 person, 1 cow, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3671851846_60c25269df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2266144051_614b2d62b0.jpg: 416x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2266144051_614b2d62b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3473534758_1ae3847781.jpg: 448x640 2 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3473534758_1ae3847781.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604384157_99241be16e.jpg: 448x640 6 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3604384157_99241be16e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3085667767_66041b202e.jpg: 448x640 1 person, 1 surfboard, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3085667767_66041b202e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/97731718_eb7ba71fd3.jpg: 640x640 1 person, 1 car, 1 motorcycle, 1 horse, 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 97731718_eb7ba71fd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3049649128_d83d847168.jpg: 640x480 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3049649128_d83d847168.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397815951_3b02090324.jpg: 480x640 3 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 397815951_3b02090324.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3236677456_75821e3583.jpg: 448x640 6 persons, 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3236677456_75821e3583.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2973272684_4d63cbc241.jpg: 448x640 2 persons, 1 baseball glove, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2973272684_4d63cbc241.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2162469360_ff777edc95.jpg: 640x512 2 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2162469360_ff777edc95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3515904775_f8acc5909e.jpg: 448x640 1 person, 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3515904775_f8acc5909e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3231237864_8cb1c6d863.jpg: 640x640 1 person, 1 sports ball, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3231237864_8cb1c6d863.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/511844627_0ec78e01e9.jpg: 640x480 1 person, 1 backpack, 1 skis, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 511844627_0ec78e01e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1303550623_cb43ac044a.jpg: 640x480 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1303550623_cb43ac044a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2350400382_ced2b6c91e.jpg: 512x640 1 dog, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2350400382_ced2b6c91e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329254388_27017bab30.jpg: 480x640 2 persons, 2 skiss, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3329254388_27017bab30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3462454965_a481809cea.jpg: 448x640 1 bench, 2 dogs, 1 horse, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3462454965_a481809cea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/848293676_98e73c52c1.jpg: 480x640 1 person, 1 sports ball, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 848293676_98e73c52c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/859620561_de417cac1e.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 859620561_de417cac1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260231029_966e2f1727.jpg: 640x512 1 dog, 1 bear, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 260231029_966e2f1727.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3403370354_5d266873b4.jpg: 448x640 1 person, 1 carrot, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3403370354_5d266873b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/517094985_4b9e926936.jpg: 640x480 2 persons, 1 bicycle, 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 517094985_4b9e926936.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3362805914_72f60ee8cb.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3362805914_72f60ee8cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262760716_1e9734f5ba.jpg: 256x640 2 persons, 8.7ms\n",
      "Speed: 1.0ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 3262760716_1e9734f5ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341477531_4e37450f35.jpg: 640x448 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3341477531_4e37450f35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3186863842_5832573c5e.jpg: 480x640 2 persons, 2 handbags, 1 book, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3186863842_5832573c5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640407952_bb38fb9d55.jpg: 640x448 2 persons, 2 skateboards, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3640407952_bb38fb9d55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597924257_d0da3c5fe6.jpg: 640x544 1 dog, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3597924257_d0da3c5fe6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2088910854_c6f8d4f5f9.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2088910854_c6f8d4f5f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2988439935_7cea05bc48.jpg: 640x448 6 persons, 1 sports ball, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2988439935_7cea05bc48.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085557551_7a88d01d4e.jpg: 448x640 1 person, 1 tennis racket, 2 cell phones, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2085557551_7a88d01d4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2659554389_ed3d15093f.jpg: 448x640 3 persons, 1 dog, 1 cow, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2659554389_ed3d15093f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2618322793_5fb164d86a.jpg: 640x448 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2618322793_5fb164d86a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2888732432_7e907a3df1.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2888732432_7e907a3df1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/788126442_086334f0cf.jpg: 640x448 2 persons, 1 teddy bear, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 788126442_086334f0cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2857558098_98e9249284.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2857558098_98e9249284.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487015378_2e90a79f4b.jpg: 640x512 13 persons, 1 umbrella, 1 handbag, 2 skateboards, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3487015378_2e90a79f4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3367851138_757d6bd2ef.jpg: 640x448 1 person, 1 boat, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3367851138_757d6bd2ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424416723_19c56cb365.jpg: 448x640 1 dog, 1 horse, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 424416723_19c56cb365.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623560640_0445c9a138.jpg: 640x480 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2623560640_0445c9a138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2470486377_c3a39ccb7b.jpg: 480x640 11 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2470486377_c3a39ccb7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/619169586_0a13ee7c21.jpg: 480x640 1 person, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 619169586_0a13ee7c21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521878609_146143708e.jpg: 640x448 2 persons, 1 spoon, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2521878609_146143708e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3094568845_d0b56c5651.jpg: 448x640 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3094568845_d0b56c5651.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/159712188_d530dd478c.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 159712188_d530dd478c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3053916979_848d32261b.jpg: 448x640 15 persons, 1 tie, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3053916979_848d32261b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2315807231_6948b3f3a5.jpg: 448x640 1 dog, 1 bed, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2315807231_6948b3f3a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2443380641_7b38d18f5b.jpg: 416x640 5 persons, 1 sports ball, 1 baseball bat, 7.6ms\n",
      "Speed: 1.4ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2443380641_7b38d18f5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/242324909_06d5a6c44b.jpg: 640x448 1 dog, 1 cow, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 242324909_06d5a6c44b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3103264875_2a8d534abc.jpg: 480x640 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3103264875_2a8d534abc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259992164_94600858b3.jpg: 480x640 19 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3259992164_94600858b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2833560457_24aedf3bef.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2833560457_24aedf3bef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3438981089_2ef1a6353c.jpg: 448x640 1 person, 1 kite, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3438981089_2ef1a6353c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2691966747_cfa154982b.jpg: 448x640 2 dogs, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2691966747_cfa154982b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2355093195_87fb7f82cb.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2355093195_87fb7f82cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2909875716_25c8652614.jpg: 480x640 1 person, 1 surfboard, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2909875716_25c8652614.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534668485_6887629ff0.jpg: 448x640 5 persons, 1 tie, 2 cell phones, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3534668485_6887629ff0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014169370_fc4059352e.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3014169370_fc4059352e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3124838157_7ef96745b7.jpg: 480x640 3 persons, 1 handbag, 2 bottles, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3124838157_7ef96745b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259002340_707ce96858.jpg: 640x352 1 dog, 8.9ms\n",
      "Speed: 1.3ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3259002340_707ce96858.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640870001_acbd1d5ceb.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3640870001_acbd1d5ceb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629027962_9cc3b46527.jpg: 448x640 8 persons, 1 bicycle, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2629027962_9cc3b46527.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/782017931_75d92bb7a4.jpg: 480x640 2 persons, 1 bicycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 782017931_75d92bb7a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2315418282_80bd0bb1c0.jpg: 448x640 1 bird, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2315418282_80bd0bb1c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899594400_61b4f6c114.jpg: 480x640 2 persons, 5 boats, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2899594400_61b4f6c114.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2426215757_e008a91fcb.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2426215757_e008a91fcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1457762320_7fe121b285.jpg: 448x640 6 persons, 4 bicycles, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1457762320_7fe121b285.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425071001_e7c9809ef2.jpg: 640x544 1 person, 1 sports ball, 1 tennis racket, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3425071001_e7c9809ef2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484365373_98d5304935.jpg: 480x640 3 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3484365373_98d5304935.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2587818583_4aa8e7b174.jpg: 640x448 6 persons, 2 sports balls, 1 apple, 2 oranges, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2587818583_4aa8e7b174.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3050264832_4215f2b398.jpg: 640x608 4 persons, 7.0ms\n",
      "Speed: 2.2ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3050264832_4215f2b398.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2135502491_a15c6b5eae.jpg: 480x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2135502491_a15c6b5eae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3675825945_96b2916959.jpg: 416x640 14 persons, 1 skateboard, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3675825945_96b2916959.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3248220732_0f173fc197.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3248220732_0f173fc197.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2733659177_d74a00995b.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2733659177_d74a00995b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3282434895_1c1efc1475.jpg: 448x640 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3282434895_1c1efc1475.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/298920219_9a3f80acc5.jpg: 448x640 1 bird, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 298920219_9a3f80acc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/501699433_f8df386cf9.jpg: 448x640 2 persons, 1 kite, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 501699433_f8df386cf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3539767254_c598b8e6c7.jpg: 448x640 4 persons, 3 boats, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3539767254_c598b8e6c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502782508_2c8211cd6b.jpg: 480x640 1 person, 1 bench, 1 book, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2502782508_2c8211cd6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443161359_65544fd732.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3443161359_65544fd732.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3266399073_40820596d5.jpg: 480x640 4 persons, 2 wine glasss, 2 cups, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3266399073_40820596d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439414478_8038ba9409.jpg: 640x480 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3439414478_8038ba9409.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695093520_5cfeb0729d.jpg: 416x640 5 persons, 1 car, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2695093520_5cfeb0729d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431101934_99a6c55914.jpg: 640x448 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3431101934_99a6c55914.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244368383_e90b6b2f20.jpg: 480x640 14 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244368383_e90b6b2f20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2579266054_1ec58aa92f.jpg: 448x640 (no detections), 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2579266054_1ec58aa92f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2714878018_1593c38d69.jpg: 640x448 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2714878018_1593c38d69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004291093_35d6fd8548.jpg: 576x640 1 person, 1 cell phone, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3004291093_35d6fd8548.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2511019188_ca71775f2d.jpg: 480x640 2 dogs, 1 frisbee, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2511019188_ca71775f2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744705147_acd767d3eb.jpg: 480x640 5 persons, 1 tie, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2744705147_acd767d3eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/407678652_1f475acd65.jpg: 448x640 8 persons, 1 backpack, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 407678652_1f475acd65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3567214106_6ece483f8b.jpg: 480x640 1 dog, 1 elephant, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3567214106_6ece483f8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3186556417_b2d6921a20.jpg: 480x640 3 persons, 16 birds, 1 handbag, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3186556417_b2d6921a20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710520638_866d542a80.jpg: 448x640 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3710520638_866d542a80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/308956341_642589e9cc.jpg: 448x640 1 bear, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 308956341_642589e9cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3669564923_8fcb1a6eff.jpg: 448x640 3 persons, 1 toothbrush, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3669564923_8fcb1a6eff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126773489_7ae425af17.jpg: 640x512 11 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3126773489_7ae425af17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1032460886_4a598ed535.jpg: 640x480 1 person, 5 clocks, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1032460886_4a598ed535.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1417941060_2a0f7908bc.jpg: 640x480 2 persons, 11.2ms\n",
      "Speed: 3.4ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1417941060_2a0f7908bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342487512_fd33971dea.jpg: 480x640 2 persons, 1 snowboard, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3342487512_fd33971dea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/515797344_4ae75cb9b1.jpg: 480x640 5 persons, 10 cars, 3 trucks, 1 traffic light, 11.0ms\n",
      "Speed: 2.1ms preprocess, 11.0ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 515797344_4ae75cb9b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/697490420_67d8d2a859.jpg: 640x480 1 person, 12.8ms\n",
      "Speed: 3.7ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 697490420_67d8d2a859.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3380072636_4cd59385fd.jpg: 640x480 10 persons, 1 sports ball, 9.5ms\n",
      "Speed: 2.8ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3380072636_4cd59385fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3309578722_1765d7d1af.jpg: 448x640 1 motorcycle, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3309578722_1765d7d1af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/772403830_08b72c7da9.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 772403830_08b72c7da9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/480607352_65614ab348.jpg: 640x448 2 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 480607352_65614ab348.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2206403470_8c25aa3cf8.jpg: 480x640 16 persons, 2 motorcycles, 2 backpacks, 1 suitcase, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2206403470_8c25aa3cf8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2906802485_dfebf09173.jpg: 480x640 3 persons, 2 boats, 1 surfboard, 12.8ms\n",
      "Speed: 3.9ms preprocess, 12.8ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2906802485_dfebf09173.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3224560800_8fefd52510.jpg: 448x640 1 person, 10.6ms\n",
      "Speed: 2.8ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3224560800_8fefd52510.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1897025969_0c41688fa6.jpg: 480x640 1 cat, 9.6ms\n",
      "Speed: 2.6ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1897025969_0c41688fa6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3678098428_40c1b74cc2.jpg: 448x640 5 persons, 1 kite, 12.3ms\n",
      "Speed: 3.2ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3678098428_40c1b74cc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326204251_2f9e446a2f.jpg: 448x640 6 persons, 10.0ms\n",
      "Speed: 2.7ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3326204251_2f9e446a2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2701895972_8605c4e038.jpg: 480x640 11 persons, 1 bottle, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2701895972_8605c4e038.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1067675215_7336a694d6.jpg: 480x640 1 person, 5 cars, 2 trucks, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1067675215_7336a694d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347496_1a35fec8dc.jpg: 640x448 7 persons, 1 sports ball, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347496_1a35fec8dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/124195430_d14028660f.jpg: 448x640 2 horses, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 124195430_d14028660f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325910784_5ecb88310c.jpg: 640x448 1 dog, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3325910784_5ecb88310c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3618525295_d32d634b2e.jpg: 480x640 7 persons, 1 bicycle, 4 cars, 1 bench, 2 handbags, 1 bottle, 4 cups, 1 chair, 1 dining table, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3618525295_d32d634b2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/179829865_095b040377.jpg: 640x608 1 person, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 179829865_095b040377.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339916063_63b960ed46.jpg: 640x448 6 persons, 1 car, 1 sports ball, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3339916063_63b960ed46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/295729735_8360f2e64c.jpg: 448x640 2 persons, 1 bench, 1 handbag, 1 book, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 295729735_8360f2e64c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410320522_d967f0b75c.jpg: 448x640 3 dogs, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2410320522_d967f0b75c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2525270674_4ab536e7ec.jpg: 640x448 2 persons, 3 cars, 2 trucks, 1 dog, 1 frisbee, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2525270674_4ab536e7ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544312930_3a0b8d70c1.jpg: 640x448 4 persons, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3544312930_3a0b8d70c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2069279767_fb32bfb2de.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2069279767_fb32bfb2de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2600883097_aca38cc146.jpg: 384x640 2 dogs, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2600883097_aca38cc146.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/207731022_988f6afb35.jpg: 640x480 1 airplane, 1 umbrella, 1 surfboard, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 207731022_988f6afb35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/707941195_4386109029.jpg: 640x448 1 dog, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 707941195_4386109029.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2501595799_6316001e89.jpg: 640x640 1 person, 2 surfboards, 9.3ms\n",
      "Speed: 3.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2501595799_6316001e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315616181_15dd137e27.jpg: 640x448 1 person, 10.6ms\n",
      "Speed: 2.5ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3315616181_15dd137e27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1095590286_c654f7e5a9.jpg: 640x512 3 dogs, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1095590286_c654f7e5a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1775029934_e1e96038a8.jpg: 448x640 1 zebra, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1775029934_e1e96038a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150659152_2ace03690b.jpg: 480x640 1 person, 1 boat, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3150659152_2ace03690b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2938747424_64e64784f0.jpg: 480x640 1 dog, 11.5ms\n",
      "Speed: 3.5ms preprocess, 11.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2938747424_64e64784f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1470061031_4cb59c12a8.jpg: 640x640 1 dog, 11.3ms\n",
      "Speed: 3.7ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1470061031_4cb59c12a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2755952680_68a0a1fa42.jpg: 640x448 9 persons, 3 bicycles, 10 cars, 2 motorcycles, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2755952680_68a0a1fa42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3696698390_989f1488e7.jpg: 544x640 1 person, 1 bicycle, 1 motorcycle, 9.6ms\n",
      "Speed: 2.5ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3696698390_989f1488e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/957682378_46c3b07bcd.jpg: 640x480 2 dogs, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 957682378_46c3b07bcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2505056124_1276e8dbcb.jpg: 640x480 1 person, 2 kites, 1 cell phone, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2505056124_1276e8dbcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3113682377_14fc7b62b0.jpg: 512x640 2 persons, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3113682377_14fc7b62b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867699650_e6ddb540de.jpg: 448x640 1 dog, 9.5ms\n",
      "Speed: 4.1ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2867699650_e6ddb540de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616508003_fa5ca5780d.jpg: 640x448 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2616508003_fa5ca5780d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697675767_97796334e4.jpg: 416x640 4 persons, 2 motorcycles, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3697675767_97796334e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2881468095_d4ce8c0c52.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2881468095_d4ce8c0c52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2754271176_4a2cda8c15.jpg: 640x448 9 persons, 2 handbags, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2754271176_4a2cda8c15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2718027742_70a72f99ae.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2718027742_70a72f99ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766854400_640e2abe08.jpg: 640x576 1 person, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2766854400_640e2abe08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2919459517_b8b858afa3.jpg: 448x640 1 person, 1 motorcycle, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2919459517_b8b858afa3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/656260720_a7db4ce48b.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 656260720_a7db4ce48b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3175849727_bf30b892cb.jpg: 448x640 1 person, 1 boat, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3175849727_bf30b892cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1803631090_05e07cc159.jpg: 640x640 1 person, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1803631090_05e07cc159.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249891874_6a090ef097.jpg: 640x640 5 persons, 2 handbags, 8.4ms\n",
      "Speed: 2.9ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3249891874_6a090ef097.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2607130765_97833d6ce1.jpg: 640x640 4 persons, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2607130765_97833d6ce1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273591668_069dcb4641.jpg: 448x640 2 dogs, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2273591668_069dcb4641.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1606988704_fe330878a3.jpg: 448x640 2 persons, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1606988704_fe330878a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2902269566_419d9f1d8e.jpg: 640x448 1 person, 1 motorcycle, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2902269566_419d9f1d8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2557129157_074a5a3128.jpg: 640x448 2 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2557129157_074a5a3128.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3460458114_35037d4d4c.jpg: 448x640 2 persons, 1 frisbee, 1 surfboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3460458114_35037d4d4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1184967930_9e29ce380d.jpg: 384x640 1 cow, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1184967930_9e29ce380d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3449114979_6cdc3e8da8.jpg: 640x448 6 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3449114979_6cdc3e8da8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/492802403_ba5246cfea.jpg: 480x640 1 horse, 1 sheep, 1 cow, 1 sports ball, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 492802403_ba5246cfea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724487641_d2096f10e5.jpg: 640x448 4 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3724487641_d2096f10e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2370481277_a3085614c9.jpg: 448x640 8 persons, 1 sports ball, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2370481277_a3085614c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404978479_8a81843e17.jpg: 480x640 4 persons, 1 baseball bat, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3404978479_8a81843e17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1507563902_6ec8d5d822.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1507563902_6ec8d5d822.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3479050296_65bcea69a0.jpg: 480x640 3 persons, 1 tie, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3479050296_65bcea69a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/399212516_d68046b277.jpg: 448x640 8 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 399212516_d68046b277.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477369101_8e0c61d8f4.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3477369101_8e0c61d8f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2109370875_05241bdda7.jpg: 384x640 2 persons, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2109370875_05241bdda7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1343426964_cde3fb54e8.jpg: 448x640 1 person, 1 kite, 3 surfboards, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1343426964_cde3fb54e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301854980_233cc2f896.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3301854980_233cc2f896.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/268654674_d29e00b3d0.jpg: 320x640 2 persons, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 268654674_d29e00b3d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/347186933_880caaf53b.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 347186933_880caaf53b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143155555_32b6d24f34.jpg: 480x640 2 persons, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143155555_32b6d24f34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1428641354_f7453afbea.jpg: 544x640 2 persons, 4 bottles, 2 bowls, 4 sandwichs, 1 chair, 1 microwave, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1428641354_f7453afbea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2876993733_cb26107d18.jpg: 352x640 2 persons, 3 chairs, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2876993733_cb26107d18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046286572_d2050ab0d9.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3046286572_d2050ab0d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501313414_ae865b6fdf.jpg: 640x448 1 horse, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3501313414_ae865b6fdf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1301140633_046e4e8010.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1301140633_046e4e8010.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/69710415_5c2bfb1058.jpg: 480x640 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 69710415_5c2bfb1058.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2999638340_75bc8b165d.jpg: 448x640 1 dog, 1 surfboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2999638340_75bc8b165d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2136992638_098d62a3c5.jpg: 480x640 2 dogs, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2136992638_098d62a3c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2519812011_f85c3b5cb5.jpg: 448x640 12 persons, 1 car, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2519812011_f85c3b5cb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2578395598_6982734d46.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2578395598_6982734d46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347271_a39a5a0070.jpg: 448x640 12 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347271_a39a5a0070.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375534917_62350bd06b.jpg: 448x640 4 persons, 4 horses, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3375534917_62350bd06b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2648310638_d6c576b5e4.jpg: 640x448 1 person, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2648310638_d6c576b5e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/279901198_e7a88c855a.jpg: 480x640 2 dogs, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 279901198_e7a88c855a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207775692_bb897d9afd.jpg: 448x640 3 dogs, 1 chair, 1 potted plant, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3207775692_bb897d9afd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390992102_67fa31b22f.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 390992102_67fa31b22f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873522522_829ea62491.jpg: 640x576 7 persons, 1 sports ball, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2873522522_829ea62491.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256456935_664a7a5bba.jpg: 576x640 12 persons, 2 cell phones, 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3256456935_664a7a5bba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3528902357_be2357a906.jpg: 640x288 1 person, 2 bicycles, 8.9ms\n",
      "Speed: 1.2ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 288)\n",
      "Cropped images saved for 3528902357_be2357a906.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3493000349_81c540e828.jpg: 352x640 13 persons, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3493000349_81c540e828.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1561658940_a947f2446a.jpg: 640x512 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1561658940_a947f2446a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2621771656_09a620da6d.jpg: 480x640 17 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2621771656_09a620da6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1015584366_dfcec3c85a.jpg: 480x640 1 dog, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1015584366_dfcec3c85a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245504245_27931f5ec1.jpg: 640x448 5 persons, 1 car, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3245504245_27931f5ec1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2896640216_761a47f006.jpg: 320x640 6 persons, 1 car, 1 truck, 1 dog, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2896640216_761a47f006.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249062399_0dafe5e4f5.jpg: 640x576 3 persons, 2 handbags, 1 sports ball, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3249062399_0dafe5e4f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482062809_3b694322c4.jpg: 448x640 7 persons, 1 backpack, 1 bowl, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3482062809_3b694322c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301859683_2d5e4b40a3.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3301859683_2d5e4b40a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3408130183_f038bdaa4f.jpg: 448x640 2 dogs, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3408130183_f038bdaa4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273625566_2454f1556b.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3273625566_2454f1556b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2525232298_cf42d415ab.jpg: 640x448 1 person, 1 sheep, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2525232298_cf42d415ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/357725852_6f55cb9abc.jpg: 480x640 2 persons, 1 boat, 13.2ms\n",
      "Speed: 3.8ms preprocess, 13.2ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 357725852_6f55cb9abc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359530430_249f51972c.jpg: 448x640 5 persons, 3 snowboards, 10.3ms\n",
      "Speed: 2.6ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3359530430_249f51972c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506560025_8d0f4f9ac4.jpg: 640x480 6 persons, 1 cell phone, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3506560025_8d0f4f9ac4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2798880731_4f51634374.jpg: 480x640 4 persons, 1 bowl, 1 cake, 1 couch, 1 dining table, 1 laptop, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2798880731_4f51634374.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346352_c5a0ea43c6.jpg: 448x640 11 persons, 1 sports ball, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346352_c5a0ea43c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2880874989_a33b632924.jpg: 640x448 1 person, 1 bottle, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2880874989_a33b632924.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460973814_5eacd1ced4.jpg: 480x640 3 persons, 3 wine glasss, 4 cups, 5 chairs, 3 potted plants, 1 dining table, 14.0ms\n",
      "Speed: 4.0ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 460973814_5eacd1ced4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3274879561_74997bbfff.jpg: 448x640 1 bird, 11.5ms\n",
      "Speed: 2.7ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3274879561_74997bbfff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3243020805_2bafc36c45.jpg: 480x640 12 persons, 1 baseball bat, 10.0ms\n",
      "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3243020805_2bafc36c45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934379210_4e399e3cac.jpg: 640x480 1 person, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2934379210_4e399e3cac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1474474514_b3eb492722.jpg: 480x640 1 person, 1 motorcycle, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1474474514_b3eb492722.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/332045444_583acaefc3.jpg: 640x544 1 dog, 9.5ms\n",
      "Speed: 2.6ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 332045444_583acaefc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2944952557_8484f0da8f.jpg: 640x640 2 persons, 2 baseball gloves, 9.4ms\n",
      "Speed: 3.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2944952557_8484f0da8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1499581619_a5f65a882c.jpg: 640x416 2 persons, 10.0ms\n",
      "Speed: 1.9ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 1499581619_a5f65a882c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537218226_478d2e4f26.jpg: 640x480 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3537218226_478d2e4f26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2637904605_fc355816fc.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2637904605_fc355816fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3328646934_5cca4cebce.jpg: 640x640 1 person, 1 dog, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3328646934_5cca4cebce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3029472296_d429b1586c.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3029472296_d429b1586c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2815256108_fc1302117d.jpg: 640x480 3 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2815256108_fc1302117d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/359837950_9e22ffe6c2.jpg: 640x608 1 dog, 9.2ms\n",
      "Speed: 2.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 359837950_9e22ffe6c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3691800116_6a7b315e46.jpg: 384x640 3 persons, 2 baseball bats, 2 baseball gloves, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3691800116_6a7b315e46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041689520_c481bdb20e.jpg: 448x640 3 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3041689520_c481bdb20e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/985067019_705fe4a4cc.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 985067019_705fe4a4cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1191338263_a4fa073154.jpg: 512x640 1 person, 1 tv, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1191338263_a4fa073154.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3085226474_62aba51179.jpg: 448x640 7 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3085226474_62aba51179.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088399255_1bd9a6aa04.jpg: 448x640 8 persons, 6 horses, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3088399255_1bd9a6aa04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2328104318_5a43ca170c.jpg: 512x640 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2328104318_5a43ca170c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3287549827_04dec6fb6e.jpg: 640x480 1 person, 1 skis, 10.9ms\n",
      "Speed: 3.0ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3287549827_04dec6fb6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1536597926_c2e1bc2379.jpg: 448x640 2 persons, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1536597926_c2e1bc2379.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530843182_35af2c821c.jpg: 384x640 2 persons, 3 cars, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3530843182_35af2c821c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/526661994_21838fc72c.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 526661994_21838fc72c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3158680604_c1f99b3946.jpg: 480x640 4 persons, 1 cup, 3 chairs, 1 potted plant, 1 microwave, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3158680604_c1f99b3946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3237760601_5334f3f3b5.jpg: 384x640 1 person, 2 skiss, 2 snowboards, 14.0ms\n",
      "Speed: 3.2ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3237760601_5334f3f3b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1277743944_f4e8c78403.jpg: 448x640 1 person, 1 suitcase, 1 sports ball, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1277743944_f4e8c78403.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2372763106_ddea79d36e.jpg: 448x640 (no detections), 12.6ms\n",
      "Speed: 3.6ms preprocess, 12.6ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2372763106_ddea79d36e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3593392955_a4125087f6.jpg: 480x640 2 dogs, 11.3ms\n",
      "Speed: 2.9ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3593392955_a4125087f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390992388_d74daee638.jpg: 448x640 1 person, 10.2ms\n",
      "Speed: 2.3ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 390992388_d74daee638.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247500085_c4f641aa84.jpg: 640x480 3 persons, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3247500085_c4f641aa84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568219100_dfbffddccd.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3568219100_dfbffddccd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2261169495_98254e2e66.jpg: 480x640 4 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2261169495_98254e2e66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3577235421_69e4efb8d1.jpg: 448x640 7 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3577235421_69e4efb8d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247704641_d883902277.jpg: 480x640 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 247704641_d883902277.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2679926555_b11cf45595.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2679926555_b11cf45595.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2385146732_d1c67c790e.jpg: 544x640 1 person, 2 cars, 9.1ms\n",
      "Speed: 2.7ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2385146732_d1c67c790e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3267644370_f2728d6c7a.jpg: 448x640 1 person, 1 skis, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3267644370_f2728d6c7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3036641436_d6594fc45f.jpg: 448x640 3 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3036641436_d6594fc45f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2367318629_b60cf4c4b3.jpg: 288x640 8 persons, 10.4ms\n",
      "Speed: 1.5ms preprocess, 10.4ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2367318629_b60cf4c4b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2070831281_dc04b3e15d.jpg: 480x640 1 person, 1 handbag, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2070831281_dc04b3e15d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376809186_4e26d880b7.jpg: 448x640 15 persons, 3 umbrellas, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376809186_4e26d880b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921430836_3b4d062238.jpg: 448x640 5 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2921430836_3b4d062238.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2452334314_a7c443a787.jpg: 448x640 1 person, 1 horse, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2452334314_a7c443a787.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3080891382_edf83dde18.jpg: 448x640 4 persons, 1 backpack, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3080891382_edf83dde18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/251958970_fa6b423f23.jpg: 480x640 15 persons, 1 tie, 1 chair, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 251958970_fa6b423f23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/758921886_55a351dd67.jpg: 480x640 1 person, 1 cow, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 758921886_55a351dd67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3683185795_704f445bf4.jpg: 416x640 2 persons, 1 dog, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3683185795_704f445bf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2319175397_3e586cfaf8.jpg: 448x640 1 dog, 1 sheep, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2319175397_3e586cfaf8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2445654384_4ee3e486e1.jpg: 480x640 2 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2445654384_4ee3e486e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3325157569_8084ab3293.jpg: 640x448 3 persons, 1 chair, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3325157569_8084ab3293.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490597800_8f94f7d353.jpg: 448x640 1 dog, 2 cows, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3490597800_8f94f7d353.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2400958566_4e09424046.jpg: 640x448 1 person, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2400958566_4e09424046.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3728695560_00ec1ca492.jpg: 448x640 3 persons, 1 cup, 2 cakes, 2 chairs, 1 dining table, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3728695560_00ec1ca492.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3413571342_b9855795e2.jpg: 480x640 1 person, 1 surfboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3413571342_b9855795e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752923489_d3bceebacb.jpg: 448x640 15 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2752923489_d3bceebacb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1570723692_3a2b064d43.jpg: 416x640 1 cat, 1 horse, 1 cow, 7.8ms\n",
      "Speed: 1.5ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1570723692_3a2b064d43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2448270671_5e0e391a80.jpg: 640x480 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2448270671_5e0e391a80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203742047_6a55065411.jpg: 480x640 2 persons, 1 snowboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3203742047_6a55065411.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3703035378_c6034cac51.jpg: 448x640 5 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3703035378_c6034cac51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/391324644_d23fdf06cb.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 391324644_d23fdf06cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3428386573_670f5362f0.jpg: 448x640 2 persons, 1 sports ball, 1 tennis racket, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3428386573_670f5362f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656225270_cdf89e3240.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3656225270_cdf89e3240.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3218861747_5c5c547b66.jpg: 480x640 14 persons, 1 car, 3 dogs, 3 skateboards, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3218861747_5c5c547b66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/408627152_1feaa4b94e.jpg: 448x640 1 person, 1 dog, 1 keyboard, 11.3ms\n",
      "Speed: 2.8ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 408627152_1feaa4b94e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2201222219_8d656b0633.jpg: 448x640 3 persons, 1 suitcase, 1 couch, 1 bed, 1 book, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2201222219_8d656b0633.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319405494_58dee86b21.jpg: 448x640 4 persons, 2 skiss, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3319405494_58dee86b21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2912476706_9a0dbd3a67.jpg: 448x640 2 dogs, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2912476706_9a0dbd3a67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/457875937_982588d918.jpg: 448x640 2 dogs, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 457875937_982588d918.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2354829523_9542fc74ba.jpg: 320x640 9 persons, 10.2ms\n",
      "Speed: 1.6ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2354829523_9542fc74ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2853205396_4fbe8d7a73.jpg: 480x640 1 dog, 1 bear, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2853205396_4fbe8d7a73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527524436_a54aca78a9.jpg: 640x448 9 persons, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3527524436_a54aca78a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/233327292_3bcbc3783f.jpg: 512x640 13 persons, 3 chairs, 1 remote, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 233327292_3bcbc3783f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2975018306_0e8da316f5.jpg: 448x640 1 dog, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2975018306_0e8da316f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/818340833_7b963c0ee3.jpg: 448x640 1 bird, 1 sheep, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 818340833_7b963c0ee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2095078658_c14ba89bc2.jpg: 480x640 1 dog, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2095078658_c14ba89bc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/835415474_7b7f2a9768.jpg: 640x448 2 persons, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 835415474_7b7f2a9768.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2624457062_89efc497a8.jpg: 480x640 1 person, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2624457062_89efc497a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1077931201_1e0bb83105.jpg: 640x480 2 persons, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1077931201_1e0bb83105.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2304374703_555195d8d5.jpg: 416x640 1 dog, 1 frisbee, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2304374703_555195d8d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2701603045_6cbdc4ce7c.jpg: 480x640 1 person, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2701603045_6cbdc4ce7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532028205_9ddd7599f8.jpg: 448x640 6 persons, 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532028205_9ddd7599f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1429814475_0b592b9995.jpg: 480x640 2 persons, 1 car, 1 skateboard, 1 potted plant, 11.1ms\n",
      "Speed: 2.8ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1429814475_0b592b9995.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345811_46b5f157d4.jpg: 640x448 7 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241345811_46b5f157d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1419385780_1383ec7ba9.jpg: 640x448 3 persons, 1 car, 13.4ms\n",
      "Speed: 3.9ms preprocess, 13.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1419385780_1383ec7ba9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204081508_0e7f408097.jpg: 640x480 14 persons, 1 boat, 1 chair, 11.6ms\n",
      "Speed: 2.9ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3204081508_0e7f408097.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2204277704_f1c8c741ed.jpg: 448x640 1 person, 2 couchs, 1 tv, 10.2ms\n",
      "Speed: 2.4ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2204277704_f1c8c741ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/618292739_0fdc2ccab0.jpg: 448x640 1 person, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 618292739_0fdc2ccab0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2429212017_77fc107699.jpg: 640x480 2 persons, 1 dog, 2 sheeps, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2429212017_77fc107699.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331900249_5872e90b25.jpg: 448x640 1 person, 1 surfboard, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3331900249_5872e90b25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2205088706_d7e633e00d.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2205088706_d7e633e00d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3724113279_99b6e5bf41.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3724113279_99b6e5bf41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862004252_53894bb28b.jpg: 448x640 1 dog, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2862004252_53894bb28b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301822808_f2ccff86f4.jpg: 512x640 7 persons, 1 horse, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3301822808_f2ccff86f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2526585002_10987a63f3.jpg: 640x480 6 persons, 3 boats, 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2526585002_10987a63f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595408539_a7d8aabc24.jpg: 480x640 3 persons, 1 remote, 1 cell phone, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3595408539_a7d8aabc24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/125319704_49ead3463c.jpg: 448x640 2 persons, 1 bicycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 125319704_49ead3463c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2312984882_bec7849e09.jpg: 480x640 6 persons, 1 tie, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2312984882_bec7849e09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/330325191_63e11d9c93.jpg: 224x640 2 persons, 10.7ms\n",
      "Speed: 1.2ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Cropped images saved for 330325191_63e11d9c93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/428483413_b9370baf72.jpg: 608x640 2 persons, 2 clocks, 14.6ms\n",
      "Speed: 4.9ms preprocess, 14.6ms inference, 2.0ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 428483413_b9370baf72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3017373346_3a34c3fe9d.jpg: 576x640 1 dog, 12.2ms\n",
      "Speed: 4.0ms preprocess, 12.2ms inference, 1.7ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3017373346_3a34c3fe9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/783994497_4f6885454d.jpg: 608x640 4 persons, 1 bottle, 1 cup, 9.5ms\n",
      "Speed: 2.8ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 783994497_4f6885454d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042483842_beb23828b9.jpg: 640x480 2 persons, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3042483842_beb23828b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2993049054_611f900644.jpg: 448x640 8 persons, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2993049054_611f900644.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3602676311_824b2c04ba.jpg: 416x640 6 persons, 1 skateboard, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3602676311_824b2c04ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/237953705_cfe6999307.jpg: 448x640 3 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 237953705_cfe6999307.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3201666946_04fe837aff.jpg: 480x640 3 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3201666946_04fe837aff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/316833109_6500b526dc.jpg: 512x640 1 dog, 9.8ms\n",
      "Speed: 2.3ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 316833109_6500b526dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3531811969_49af4c22f0.jpg: 640x480 1 person, 1 bicycle, 1 car, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3531811969_49af4c22f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2242863004_3a9f82a31f.jpg: 512x640 1 dog, 1 frisbee, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2242863004_3a9f82a31f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2197275664_fabcf3424b.jpg: 640x448 1 person, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2197275664_fabcf3424b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3272071680_648a99f7d2.jpg: 512x640 4 persons, 1 dog, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3272071680_648a99f7d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384147448_c1869070d3.jpg: 640x448 1 dog, 1 sheep, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2384147448_c1869070d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566794440_f9ec673a2f.jpg: 640x480 2 persons, 1 car, 2 baseball gloves, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 566794440_f9ec673a2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222041930_f642f49d28.jpg: 448x640 3 persons, 1 tie, 1 book, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3222041930_f642f49d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1434005938_ad75c8598c.jpg: 640x480 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1434005938_ad75c8598c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3358621566_12bac2e9d2.jpg: 480x640 2 persons, 1 bottle, 1 refrigerator, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3358621566_12bac2e9d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3110174991_a4b05f8a46.jpg: 448x640 5 persons, 1 tie, 1 remote, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3110174991_a4b05f8a46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3455757720_7aeba57056.jpg: 352x640 1 person, 1 dog, 1 sheep, 9.2ms\n",
      "Speed: 1.5ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3455757720_7aeba57056.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262446581_06ef7d9445.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 262446581_06ef7d9445.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2194806429_ca4c3770c1.jpg: 448x640 3 persons, 1 bicycle, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2194806429_ca4c3770c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2427558437_3e839056d7.jpg: 448x640 2 dogs, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2427558437_3e839056d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2209888959_d636b1be0b.jpg: 640x448 1 person, 1 baseball bat, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2209888959_d636b1be0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621652774_fd9634bd5b.jpg: 512x640 1 person, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3621652774_fd9634bd5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2245618207_fa486ba2b7.jpg: 448x640 12 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2245618207_fa486ba2b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2322327298_7948338390.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2322327298_7948338390.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2296166785_292a4e9f4c.jpg: 448x640 2 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2296166785_292a4e9f4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694991841_141804da1f.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3694991841_141804da1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2885382946_f541ea5722.jpg: 640x640 4 persons, 1 car, 5 boats, 7.8ms\n",
      "Speed: 2.5ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2885382946_f541ea5722.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/358114269_96fdb5f7c3.jpg: 448x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 358114269_96fdb5f7c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2979914158_5906470b8f.jpg: 640x416 1 person, 1 bench, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2979914158_5906470b8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/177302997_5b2d770a0a.jpg: 640x480 5 persons, 1 motorcycle, 1 backpack, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 177302997_5b2d770a0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1794818900_e0ffdd268e.jpg: 640x480 2 persons, 1 motorcycle, 1 bench, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1794818900_e0ffdd268e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2783620390_02c166c733.jpg: 448x640 1 person, 1 motorcycle, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2783620390_02c166c733.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521938720_911ac092f7.jpg: 544x640 2 persons, 1 sports ball, 2 baseball gloves, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2521938720_911ac092f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3654338683_13b2f95a9a.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3654338683_13b2f95a9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3365783912_e12c3510d8.jpg: 480x640 12 persons, 1 bus, 1 handbag, 1 tie, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3365783912_e12c3510d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3380519003_9f91d5a7fb.jpg: 512x640 1 dog, 1 frisbee, 1 sports ball, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3380519003_9f91d5a7fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457856049_2de173e818.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3457856049_2de173e818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3458577912_67db47209d.jpg: 480x640 2 dogs, 1 laptop, 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3458577912_67db47209d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2893238950_8a027be110.jpg: 640x480 7 persons, 1 kite, 3 skateboards, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2893238950_8a027be110.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1280147517_98767ca3b3.jpg: 448x640 1 person, 1 boat, 10.5ms\n",
      "Speed: 2.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1280147517_98767ca3b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275527950_41aca690a1.jpg: 448x640 1 person, 1 cow, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3275527950_41aca690a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2105756457_a100d8434e.jpg: 640x448 2 persons, 1 chair, 1 dining table, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2105756457_a100d8434e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3702436188_2c26192fd0.jpg: 640x512 1 person, 1 car, 1 truck, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3702436188_2c26192fd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549997413_01388dece0.jpg: 480x640 1 person, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3549997413_01388dece0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033257301_e2c8a39b04.jpg: 448x640 4 persons, 1 potted plant, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3033257301_e2c8a39b04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2650568697_ffb79bf2ea.jpg: 480x640 4 persons, 6 cars, 1 bus, 1 truck, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2650568697_ffb79bf2ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2444821454_22a346c996.jpg: 448x640 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2444821454_22a346c996.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2054308369_f9c6ec7815.jpg: 416x640 2 persons, 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2054308369_f9c6ec7815.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482314155_bd1e668b4e.jpg: 480x640 10 persons, 1 cow, 1 tennis racket, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3482314155_bd1e668b4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2858439751_daa3a30ab8.jpg: 448x640 2 persons, 1 motorcycle, 11.9ms\n",
      "Speed: 2.0ms preprocess, 11.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2858439751_daa3a30ab8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/581419370_30485f3580.jpg: 480x640 2 persons, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 581419370_30485f3580.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3707990914_843e8f15f1.jpg: 640x448 1 person, 2 surfboards, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3707990914_843e8f15f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2683963310_20dcd5e566.jpg: 640x640 1 person, 1 bowl, 9.4ms\n",
      "Speed: 3.1ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2683963310_20dcd5e566.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/483039719_75181b2726.jpg: 448x640 11 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 483039719_75181b2726.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2719102611_fef453bf30.jpg: 640x448 1 person, 1 motorcycle, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2719102611_fef453bf30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2825483885_3f7c54db3e.jpg: 448x640 1 person, 1 bicycle, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2825483885_3f7c54db3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2532294586_4cd76a837d.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2532294586_4cd76a837d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342822192_082f932ef2.jpg: 448x640 1 person, 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3342822192_082f932ef2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088572348_264c47f78c.jpg: 448x640 1 person, 1 surfboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3088572348_264c47f78c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265162450_5b4e3c5f1b.jpg: 512x640 3 dogs, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3265162450_5b4e3c5f1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099091086_f75f0ce09d.jpg: 448x640 10 persons, 1 tie, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3099091086_f75f0ce09d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3219210794_4324df188b.jpg: 640x640 1 bicycle, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3219210794_4324df188b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2995461857_dd26188dcf.jpg: 640x448 1 car, 1 truck, 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2995461857_dd26188dcf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2690702549_cf81da8cf6.jpg: 480x640 4 persons, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2690702549_cf81da8cf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3571193625_835da90c5e.jpg: 448x640 1 bear, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3571193625_835da90c5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1363843090_9425d93064.jpg: 480x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1363843090_9425d93064.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176498130_52ab9460b2.jpg: 480x640 5 persons, 1 bench, 1 skateboard, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3176498130_52ab9460b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3538213870_9856a76b2a.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3538213870_9856a76b2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1776981714_5b224d0f7a.jpg: 416x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1776981714_5b224d0f7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649307685_60c1294d2a.jpg: 640x320 1 person, 1 baseball bat, 9.4ms\n",
      "Speed: 1.3ms preprocess, 9.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 3649307685_60c1294d2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2712787899_d85048eb6a.jpg: 640x416 1 person, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2712787899_d85048eb6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374103776_0de490c1b0.jpg: 480x640 12 persons, 1 cup, 1 bowl, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 374103776_0de490c1b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/233270519_d60d4518fa.jpg: 640x480 1 person, 1 surfboard, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 233270519_d60d4518fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/379006645_b9a2886b51.jpg: 640x480 1 dog, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 379006645_b9a2886b51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/436013859_793d870b6f.jpg: 640x448 6 persons, 1 dog, 1 frisbee, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 436013859_793d870b6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2531942624_c3c072064e.jpg: 448x640 3 persons, 1 car, 1 truck, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2531942624_c3c072064e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/432490118_54a9c0e500.jpg: 448x640 2 dogs, 1 sheep, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 432490118_54a9c0e500.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/492493570_c27237a396.jpg: 416x640 1 person, 1 bird, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 492493570_c27237a396.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155279929_d1cddbe7cf.jpg: 640x640 1 dog, 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3155279929_d1cddbe7cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2738077433_10e6264b6f.jpg: 576x640 1 dog, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2738077433_10e6264b6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3360930596_1e75164ce6.jpg: 640x448 6 persons, 1 sports ball, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3360930596_1e75164ce6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2298946012_22de913532.jpg: 480x640 5 persons, 4 horses, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2298946012_22de913532.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/454691853_cc1e0fa6a1.jpg: 480x640 1 dog, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 454691853_cc1e0fa6a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3199895624_4f01798c6f.jpg: 480x640 3 persons, 1 potted plant, 1 tv, 1 laptop, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3199895624_4f01798c6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2942133798_e57c862a90.jpg: 448x640 2 dogs, 1 frisbee, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2942133798_e57c862a90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3166969425_b5ace2f9c2.jpg: 448x640 1 dog, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3166969425_b5ace2f9c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1918573100_d31cbb6b77.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1918573100_d31cbb6b77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537479916_c033897fac.jpg: 480x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 537479916_c033897fac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2303016989_0deb96c8d9.jpg: 480x640 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2303016989_0deb96c8d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1122944218_8eb3607403.jpg: 640x480 2 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1122944218_8eb3607403.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351667846_ac43118ae5.jpg: 448x640 6 persons, 3 pizzas, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3351667846_ac43118ae5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2687229880_97cfd8148e.jpg: 448x640 7 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2687229880_97cfd8148e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584603849_6cfd9af7dd.jpg: 448x640 1 airplane, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584603849_6cfd9af7dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2817847072_5eb3bc30ac.jpg: 640x448 7 persons, 1 car, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2817847072_5eb3bc30ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2693425189_47740c22ed.jpg: 640x480 1 person, 1 boat, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2693425189_47740c22ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/433855742_c2a6fda763.jpg: 608x640 1 person, 1 dog, 1 frisbee, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 433855742_c2a6fda763.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1167669558_87a8a467d6.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1167669558_87a8a467d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1975171469_84e425f61b.jpg: 448x640 3 persons, 1 sports ball, 1 cake, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1975171469_84e425f61b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860040276_eac0aca4fc.jpg: 448x640 1 person, 1 cell phone, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2860040276_eac0aca4fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2390369143_6523253a73.jpg: 448x640 1 bird, 2 dogs, 1 horse, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2390369143_6523253a73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2491343114_a3e35a2a3a.jpg: 384x640 1 dog, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2491343114_a3e35a2a3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3633396324_c4b24b1f51.jpg: 448x640 6 persons, 2 handbags, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3633396324_c4b24b1f51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2187720319_112d00f07d.jpg: 448x640 1 bear, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2187720319_112d00f07d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460159430_71ab1aacfa.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2460159430_71ab1aacfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576536763_3c8c4f232e.jpg: 512x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3576536763_3c8c4f232e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1397923690_d3bf1f799e.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1397923690_d3bf1f799e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3504479370_ff2d89a043.jpg: 480x640 10 persons, 1 stop sign, 1 umbrella, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3504479370_ff2d89a043.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2406591500_403f145905.jpg: 448x640 3 persons, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2406591500_403f145905.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3524436870_7670df68e8.jpg: 480x640 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3524436870_7670df68e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1931690777_897a7d8ab6.jpg: 512x640 4 persons, 1 potted plant, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1931690777_897a7d8ab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3362189985_fbae8f860a.jpg: 576x640 1 person, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3362189985_fbae8f860a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2339946012_06bd480ab8.jpg: 480x640 1 bird, 1 kite, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2339946012_06bd480ab8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3599568766_9e96def0ef.jpg: 480x640 1 dog, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3599568766_9e96def0ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2734669176_c272b42597.jpg: 640x576 3 persons, 1 dog, 3 frisbees, 1 chair, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2734669176_c272b42597.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2574230252_f5a1382dd4.jpg: 448x640 1 sheep, 1 bear, 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2574230252_f5a1382dd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569329986_1f468729b2.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3569329986_1f468729b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099694681_19a72c8bdc.jpg: 480x640 4 persons, 2 bottles, 2 wine glasss, 3 cups, 1 potted plant, 1 dining table, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3099694681_19a72c8bdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2165677531_e1d5e086f7.jpg: 480x640 15 persons, 1 car, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2165677531_e1d5e086f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2824401212_8da8ab99d6.jpg: 448x640 6 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2824401212_8da8ab99d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/229862312_1a0ba19dab.jpg: 480x640 1 dog, 1 sheep, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 229862312_1a0ba19dab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3133825703_359a0c414d.jpg: 448x640 4 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3133825703_359a0c414d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441701164_6dbbdf1bce.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3441701164_6dbbdf1bce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2351762979_0941aecced.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2351762979_0941aecced.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/930748509_8ca5cf5c24.jpg: 480x640 3 persons, 1 dog, 1 sheep, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 930748509_8ca5cf5c24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/518789868_8895ef8792.jpg: 480x640 2 persons, 3 chairs, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 518789868_8895ef8792.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3147913471_322ea231d9.jpg: 640x512 1 person, 1 baseball glove, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3147913471_322ea231d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649384501_f1e06c58c0.jpg: 640x448 2 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3649384501_f1e06c58c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2737233999_34d60dc5c3.jpg: 448x640 3 persons, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2737233999_34d60dc5c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3485816074_363cab4bff.jpg: 448x640 6 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3485816074_363cab4bff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2946016853_ceca4f5a07.jpg: 576x640 2 dogs, 4 frisbees, 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2946016853_ceca4f5a07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2101457132_69c950bc45.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2101457132_69c950bc45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/457945610_1a23b9ced0.jpg: 640x448 1 person, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 457945610_1a23b9ced0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3682428916_69ce66d375.jpg: 448x640 1 car, 1 airplane, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3682428916_69ce66d375.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2230067846_74046b89d3.jpg: 640x448 3 persons, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2230067846_74046b89d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2755314937_1e974bf2b5.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2755314937_1e974bf2b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2518853257_02f30e282e.jpg: 640x544 2 persons, 1 handbag, 3 chairs, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2518853257_02f30e282e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293753378_7a8ddb98b2.jpg: 448x640 3 persons, 1 handbag, 1 laptop, 1 remote, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3293753378_7a8ddb98b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498327617_d2e3db3ee3.jpg: 448x640 1 elephant, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3498327617_d2e3db3ee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3098824948_23c31df031.jpg: 640x448 5 persons, 2 ties, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3098824948_23c31df031.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3234890865_bb3c316968.jpg: 480x640 4 persons, 1 train, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3234890865_bb3c316968.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606355203_1260f43ec0.jpg: 448x640 5 persons, 1 handbag, 1 surfboard, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3606355203_1260f43ec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2572101672_4d699c8713.jpg: 480x640 1 person, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2572101672_4d699c8713.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3565598162_56044bc2f7.jpg: 448x640 9 persons, 1 handbag, 1 bottle, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3565598162_56044bc2f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2290589734_b588471345.jpg: 512x640 2 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2290589734_b588471345.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1461653394_8ab96aae63.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1461653394_8ab96aae63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3036033157_522a43a550.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3036033157_522a43a550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/418357172_bdddf71d32.jpg: 480x640 1 dog, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 418357172_bdddf71d32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150742439_b8a352e1e0.jpg: 448x640 2 persons, 1 umbrella, 1 suitcase, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3150742439_b8a352e1e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3028969146_26929ae0e8.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3028969146_26929ae0e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561212119_1af8cb9b5d.jpg: 640x448 1 dog, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2561212119_1af8cb9b5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2065875490_a46b58c12b.jpg: 480x640 2 persons, 2 cars, 2 buss, 2 trucks, 1 zebra, 1 handbag, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2065875490_a46b58c12b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251906388_c09d44340e.jpg: 512x640 1 dog, 1 sheep, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3251906388_c09d44340e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1358089136_976e3d2e30.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1358089136_976e3d2e30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2950637275_98f1e30cca.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2950637275_98f1e30cca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2705793985_007cc703fb.jpg: 640x448 1 dog, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2705793985_007cc703fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2677656448_6b7e7702af.jpg: 480x640 2 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2677656448_6b7e7702af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495033548_bd320405d8.jpg: 576x640 3 persons, 1 sports ball, 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 495033548_bd320405d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183195653_11b66acb34.jpg: 448x640 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3183195653_11b66acb34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638783120_f600ceb19d.jpg: 448x640 4 persons, 1 sports ball, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3638783120_f600ceb19d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3104400277_1524e4f758.jpg: 448x640 4 persons, 5 motorcycles, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3104400277_1524e4f758.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2289096282_4ef120f189.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2289096282_4ef120f189.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/542317719_ed4dd95dc2.jpg: 448x640 1 person, 1 bed, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 542317719_ed4dd95dc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998277360_9b4c0192f1.jpg: 416x640 1 person, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2998277360_9b4c0192f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3614881872_ccf9739b0e.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3614881872_ccf9739b0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/431410325_f4916b5460.jpg: 480x640 1 bird, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 431410325_f4916b5460.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2696636252_91ef1491ea.jpg: 448x640 5 persons, 8 cars, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2696636252_91ef1491ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415287719_3c776f370e.jpg: 448x640 2 persons, 1 baseball bat, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415287719_3c776f370e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2380765956_6313d8cae3.jpg: 544x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2380765956_6313d8cae3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3281078518_630a7a7f4f.jpg: 544x640 2 cats, 2 sheeps, 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3281078518_630a7a7f4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2436398074_8737f40869.jpg: 480x640 1 dog, 1 sports ball, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2436398074_8737f40869.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286222970_1fa445e38f.jpg: 480x640 1 person, 1 bench, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3286222970_1fa445e38f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143552697_af27e9acf5.jpg: 448x640 1 person, 1 motorcycle, 1 backpack, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 143552697_af27e9acf5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1810651611_35aae644fb.jpg: 480x640 1 person, 1 chair, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1810651611_35aae644fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2984174290_a915748d77.jpg: 448x640 1 person, 2 surfboards, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2984174290_a915748d77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/271770120_880e8d8e52.jpg: 320x640 2 dogs, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 271770120_880e8d8e52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444872454_9f51e07f88.jpg: 448x640 11 persons, 3 handbags, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 444872454_9f51e07f88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256439287_990ac4a761.jpg: 640x480 7 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 256439287_990ac4a761.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139876823_859c7d7c23.jpg: 480x640 1 person, 2 skiss, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3139876823_859c7d7c23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3652150541_8fb5a3a5d1.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3652150541_8fb5a3a5d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2417623030_afdc1024b5.jpg: 480x640 4 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2417623030_afdc1024b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2633082074_32c85f532c.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2633082074_32c85f532c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143980056_7a64a94b58.jpg: 480x640 3 persons, 1 couch, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3143980056_7a64a94b58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498423815_5b8fc097f4.jpg: 640x448 2 persons, 1 bicycle, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3498423815_5b8fc097f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3720366614_dfa8fe1088.jpg: 640x576 2 dogs, 2 beds, 7.8ms\n",
      "Speed: 2.4ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3720366614_dfa8fe1088.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2883099128_0b056eed9e.jpg: 448x640 1 person, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2883099128_0b056eed9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171250845_5ae0d2a8bc.jpg: 640x448 2 persons, 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3171250845_5ae0d2a8bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2341254813_c53a5ef27a.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2341254813_c53a5ef27a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1689658980_0074d81d28.jpg: 416x640 1 person, 2 dogs, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1689658980_0074d81d28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3714551959_66ece78f27.jpg: 640x480 1 person, 1 bench, 1 snowboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3714551959_66ece78f27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477683327_d9e6a2a64f.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3477683327_d9e6a2a64f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2267819545_446c5a3e18.jpg: 448x640 8 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2267819545_446c5a3e18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2847859796_4d9cb0d31f.jpg: 448x640 2 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2847859796_4d9cb0d31f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2139519215_8ca16dd192.jpg: 416x640 1 person, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2139519215_8ca16dd192.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2070831523_5035d5537e.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2070831523_5035d5537e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3175446111_681a89f873.jpg: 384x640 8 persons, 1 tv, 1 clock, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3175446111_681a89f873.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/96978713_775d66a18d.jpg: 640x384 3 persons, 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 96978713_775d66a18d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2375924666_fee50f1cba.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2375924666_fee50f1cba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667157255_4e66d11dc2.jpg: 640x512 3 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3667157255_4e66d11dc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1765164972_92dac06fa9.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1765164972_92dac06fa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3444974984_963fb441a0.jpg: 608x640 1 person, 1 book, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3444974984_963fb441a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2293424366_7b5fcd2398.jpg: 640x448 3 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2293424366_7b5fcd2398.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/340425915_490293058f.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 340425915_490293058f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2321865325_79b0954a5d.jpg: 544x640 2 dogs, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2321865325_79b0954a5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/497122685_a51b29dc46.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 497122685_a51b29dc46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2598979962_c01811cfca.jpg: 640x480 2 persons, 1 sports ball, 1 tennis racket, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2598979962_c01811cfca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472574160_8ce233f396.jpg: 512x640 (no detections), 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2472574160_8ce233f396.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323988406_e3c8fce690.jpg: 640x448 4 persons, 3 skateboards, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3323988406_e3c8fce690.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3651971126_309e6a5e22.jpg: 640x608 2 dogs, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3651971126_309e6a5e22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539493431_744eb1abaa.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 539493431_744eb1abaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2890075175_4bd32b201a.jpg: 512x640 1 dog, 7.8ms\n",
      "Speed: 3.2ms preprocess, 7.8ms inference, 2.0ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2890075175_4bd32b201a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/689776124_07f560a920.jpg: 512x640 1 dog, 1 frisbee, 6.7ms\n",
      "Speed: 1.9ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 689776124_07f560a920.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978024878_a45b282bf4.jpg: 640x448 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2978024878_a45b282bf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3232252882_05db7c2216.jpg: 448x640 1 person, 4 dogs, 1 sheep, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3232252882_05db7c2216.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2774554310_007e980a90.jpg: 640x256 1 person, 9.1ms\n",
      "Speed: 1.0ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 256)\n",
      "Cropped images saved for 2774554310_007e980a90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244734844_c318c29c23.jpg: 448x640 1 person, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3244734844_c318c29c23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/270263570_3160f360d3.jpg: 608x640 1 horse, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 270263570_3160f360d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2126950128_74a4882658.jpg: 640x480 1 person, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2126950128_74a4882658.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2869765795_21a398cb24.jpg: 416x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2869765795_21a398cb24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583321426_f373c52161.jpg: 480x640 1 person, 1 suitcase, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3583321426_f373c52161.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/505944126_f9a6ab8944.jpg: 480x640 1 dog, 1 sheep, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 505944126_f9a6ab8944.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1359101233_16c2c150e3.jpg: 448x640 1 dog, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1359101233_16c2c150e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2313230479_13f87c6bf3.jpg: 480x640 1 dog, 1 bear, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2313230479_13f87c6bf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667822570_d39850e217.jpg: 608x640 9 persons, 1 sports ball, 8.0ms\n",
      "Speed: 2.3ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3667822570_d39850e217.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2364096157_eb7970a69a.jpg: 416x640 1 dog, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2364096157_eb7970a69a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3083016677_5782bc337c.jpg: 608x640 4 persons, 1 car, 1 motorcycle, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3083016677_5782bc337c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2140182410_8e2a06fbda.jpg: 384x640 1 person, 1 car, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2140182410_8e2a06fbda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2906054175_e33af79522.jpg: 640x352 6 persons, 1 elephant, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2906054175_e33af79522.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2837640996_0183db8d93.jpg: 608x640 1 person, 1 cell phone, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2837640996_0183db8d93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244760301_5809214866.jpg: 480x640 5 persons, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244760301_5809214866.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676184321_858eff416b.jpg: 320x640 1 person, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2676184321_858eff416b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3669069522_555c97fbfb.jpg: 544x640 3 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3669069522_555c97fbfb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460935487_75b2da7854.jpg: 640x448 1 dog, 4 potted plants, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 460935487_75b2da7854.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260520547_944f9f4c91.jpg: 448x640 6 persons, 1 car, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 260520547_944f9f4c91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640109324_3ce89e4d1a.jpg: 384x640 6 persons, 1 tie, 1 book, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3640109324_3ce89e4d1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752329719_868545b7d2.jpg: 448x640 4 persons, 2 chairs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2752329719_868545b7d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2053777548_108e54c826.jpg: 640x480 1 dog, 1 sports ball, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2053777548_108e54c826.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924870944_90ff9eca1a.jpg: 640x512 1 dog, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2924870944_90ff9eca1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2481490320_7978c76271.jpg: 256x640 1 person, 1 boat, 9.6ms\n",
      "Speed: 1.0ms preprocess, 9.6ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2481490320_7978c76271.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3015898903_70bebb8903.jpg: 448x640 5 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3015898903_70bebb8903.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139238055_2817a0c7d8.jpg: 448x640 4 persons, 1 bottle, 2 wine glasss, 1 dining table, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3139238055_2817a0c7d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766148353_70b2e8070f.jpg: 640x512 5 persons, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2766148353_70b2e8070f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2429272699_8a9699775e.jpg: 576x640 1 person, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2429272699_8a9699775e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3287963317_186491ee78.jpg: 480x640 4 persons, 1 snowboard, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3287963317_186491ee78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/508432819_3d055f395d.jpg: 480x640 1 motorcycle, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 508432819_3d055f395d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/264141937_585320617a.jpg: 640x480 3 persons, 12.9ms\n",
      "Speed: 2.7ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 264141937_585320617a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3111208043_dbe8e87fa1.jpg: 640x448 1 person, 12.0ms\n",
      "Speed: 2.2ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3111208043_dbe8e87fa1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375070563_3c290a7991.jpg: 448x640 1 bird, 11.8ms\n",
      "Speed: 2.2ms preprocess, 11.8ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3375070563_3c290a7991.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856080862_95d793fa9d.jpg: 448x640 9 persons, 1 kite, 10.5ms\n",
      "Speed: 2.1ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2856080862_95d793fa9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1232148178_4f45cc3284.jpg: 480x640 2 persons, 11.7ms\n",
      "Speed: 2.2ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1232148178_4f45cc3284.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293751136_b0ce285dc3.jpg: 480x640 2 persons, 6 cars, 1 traffic light, 1 parking meter, 1 tie, 1 sports ball, 10.5ms\n",
      "Speed: 2.4ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3293751136_b0ce285dc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496971341_22782195f0.jpg: 480x640 2 dogs, 1 frisbee, 10.5ms\n",
      "Speed: 2.2ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 496971341_22782195f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3248352729_ab264b2222.jpg: 640x512 1 person, 1 snowboard, 11.5ms\n",
      "Speed: 2.4ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3248352729_ab264b2222.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535830521_aa971319fc.jpg: 480x640 1 person, 1 car, 3 parking meters, 1 book, 11.4ms\n",
      "Speed: 2.2ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 535830521_aa971319fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3065560742_f6e266ccd9.jpg: 512x640 2 persons, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3065560742_f6e266ccd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106340185_80d0cb770a.jpg: 480x640 1 person, 11.6ms\n",
      "Speed: 2.2ms preprocess, 11.6ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3106340185_80d0cb770a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2732625904_4fbb653434.jpg: 448x640 1 person, 1 bicycle, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2732625904_4fbb653434.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676764246_c58205a365.jpg: 448x640 3 dogs, 10.8ms\n",
      "Speed: 2.2ms preprocess, 10.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2676764246_c58205a365.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1425485485_d7c97a5470.jpg: 448x640 3 persons, 10.6ms\n",
      "Speed: 2.1ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1425485485_d7c97a5470.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474958471_9106beb07f.jpg: 640x448 1 person, 1 snowboard, 12.2ms\n",
      "Speed: 2.1ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3474958471_9106beb07f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3456862740_7550bcddc2.jpg: 448x640 1 person, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3456862740_7550bcddc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2790909995_8b7a03d9d1.jpg: 352x640 1 dog, 12.1ms\n",
      "Speed: 1.8ms preprocess, 12.1ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2790909995_8b7a03d9d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/459284240_5a4167bf92.jpg: 480x640 8 persons, 9.6ms\n",
      "Speed: 1.8ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 459284240_5a4167bf92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1357689954_72588dfdc4.jpg: 640x448 1 person, 8.9ms\n",
      "Speed: 1.5ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1357689954_72588dfdc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2330062180_355ccbceb5.jpg: 448x640 1 dog, 8.8ms\n",
      "Speed: 1.6ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2330062180_355ccbceb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/528498076_43f0ef36b5.jpg: 640x448 1 person, 1 bed, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 528498076_43f0ef36b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3198237818_cb5eb302f0.jpg: 448x640 6 persons, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3198237818_cb5eb302f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2587106431_1cc0e719c6.jpg: 448x640 3 persons, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2587106431_1cc0e719c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/382090166_be2c2c63e1.jpg: 448x640 1 dog, 2 sheeps, 1 bear, 1 sports ball, 1 orange, 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 382090166_be2c2c63e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3226254560_2f8ac147ea.jpg: 448x640 2 dogs, 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3226254560_2f8ac147ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3141293960_74459f0a24.jpg: 640x512 2 persons, 1 skateboard, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3141293960_74459f0a24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892395757_0a1b0eedd2.jpg: 640x448 3 persons, 1 bicycle, 2 cars, 1 truck, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2892395757_0a1b0eedd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2753531542_ace2c870b7.jpg: 448x640 3 persons, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2753531542_ace2c870b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2190137367_746335f707.jpg: 480x640 1 person, 1 dog, 1 bed, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2190137367_746335f707.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3650111717_346804ec2f.jpg: 640x448 2 giraffes, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3650111717_346804ec2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2687539673_d54a8dc613.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2687539673_d54a8dc613.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3081182021_22cfa18dd4.jpg: 640x416 1 person, 1 bench, 2 skateboards, 10.1ms\n",
      "Speed: 1.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3081182021_22cfa18dd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1509786421_f03158adfc.jpg: 448x640 1 person, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1509786421_f03158adfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3485425825_c2f3446e73.jpg: 640x608 2 persons, 1 bicycle, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3485425825_c2f3446e73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2056377805_e9a9b3bcf0.jpg: 544x640 6 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2056377805_e9a9b3bcf0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1307635496_94442dc21a.jpg: 544x640 1 person, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1307635496_94442dc21a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425756814_13909354d4.jpg: 640x512 8 persons, 2 tennis rackets, 1 chair, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425756814_13909354d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3661072592_2e693cd5a0.jpg: 640x448 3 persons, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3661072592_2e693cd5a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3313232606_4ce7e16b87.jpg: 448x640 10 persons, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3313232606_4ce7e16b87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2952751562_ff1c138286.jpg: 416x640 6 persons, 1 train, 1 handbag, 9.3ms\n",
      "Speed: 1.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2952751562_ff1c138286.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3227423095_5049951eab.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3227423095_5049951eab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/976392326_082dafc3c5.jpg: 640x448 3 persons, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 976392326_082dafc3c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/894928353_002a3d5f06.jpg: 480x640 3 persons, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 894928353_002a3d5f06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2297744130_f571f3a239.jpg: 640x480 2 persons, 8.9ms\n",
      "Speed: 1.6ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2297744130_f571f3a239.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2222498879_9e82a100ab.jpg: 448x640 1 dog, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2222498879_9e82a100ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998945968_86f236d1e8.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2998945968_86f236d1e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/291952021_f111b0fb3d.jpg: 480x640 1 dog, 9.1ms\n",
      "Speed: 1.9ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 291952021_f111b0fb3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345446_2e47ae8ddc.jpg: 640x448 6 persons, 9.2ms\n",
      "Speed: 1.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241345446_2e47ae8ddc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2719101587_4ba70dee14.jpg: 640x448 4 persons, 1 motorcycle, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2719101587_4ba70dee14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547000169_40191e02ca.jpg: 416x640 7 persons, 1 tv, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3547000169_40191e02ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326454455_960e5442e9.jpg: 640x448 1 person, 9.1ms\n",
      "Speed: 1.7ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3326454455_960e5442e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2637510448_4521cf6f29.jpg: 640x480 1 person, 1 skis, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2637510448_4521cf6f29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2058091220_2087270068.jpg: 640x480 1 kite, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2058091220_2087270068.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2715337869_e4fe36db50.jpg: 480x640 1 person, 1 surfboard, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2715337869_e4fe36db50.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638459638_ec74e3ff89.jpg: 384x640 1 dog, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3638459638_ec74e3ff89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/523249012_a0a25f487e.jpg: 480x640 1 person, 2 chairs, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 523249012_a0a25f487e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427023324_f1f6504bf4.jpg: 640x448 2 persons, 9.0ms\n",
      "Speed: 1.7ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3427023324_f1f6504bf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3303787342_b258b377b6.jpg: 448x640 1 person, 1 skis, 1 snowboard, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3303787342_b258b377b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/503090187_8758ab5680.jpg: 480x640 3 persons, 1 cell phone, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 503090187_8758ab5680.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2337377811_8c81b40a64.jpg: 480x640 1 person, 1 car, 1 skis, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2337377811_8c81b40a64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549673305_4dfd44e04a.jpg: 480x640 7 persons, 1 sports ball, 8.9ms\n",
      "Speed: 1.8ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3549673305_4dfd44e04a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/107582366_d86f2d3347.jpg: 448x640 7 persons, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 107582366_d86f2d3347.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3080056515_3013830309.jpg: 448x640 2 persons, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3080056515_3013830309.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638631362_af29bbff01.jpg: 416x640 1 person, 1 kite, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3638631362_af29bbff01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3453544202_3855ab34b6.jpg: 384x640 1 person, 1 motorcycle, 9.5ms\n",
      "Speed: 1.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3453544202_3855ab34b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3081734118_6f2090215c.jpg: 544x640 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3081734118_6f2090215c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/275002371_5b200e6a92.jpg: 512x640 3 dogs, 8.8ms\n",
      "Speed: 1.8ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 275002371_5b200e6a92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119076670_64b5340530.jpg: 640x448 1 person, 1 snowboard, 1 skateboard, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3119076670_64b5340530.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/947969010_f1ea572e89.jpg: 480x640 1 dog, 1 elephant, 1 bear, 8.9ms\n",
      "Speed: 1.8ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 947969010_f1ea572e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2525666287_638ab5e784.jpg: 480x640 4 persons, 1 train, 2 benchs, 1 cup, 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2525666287_638ab5e784.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155501473_510f9c9f6b.jpg: 480x640 2 persons, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3155501473_510f9c9f6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3191135894_2b4bdabb6d.jpg: 224x640 6 persons, 1 umbrella, 2 kites, 9.7ms\n",
      "Speed: 0.9ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Cropped images saved for 3191135894_2b4bdabb6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/201682811_105241dee3.jpg: 640x640 3 persons, 1 bottle, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 201682811_105241dee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/607339469_af851c4119.jpg: 448x640 1 person, 1 bird, 8.7ms\n",
      "Speed: 1.5ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 607339469_af851c4119.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569126684_a68b29a57f.jpg: 640x448 1 dog, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3569126684_a68b29a57f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2851304910_b5721199bc.jpg: 480x640 1 person, 1 motorcycle, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2851304910_b5721199bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2056930414_d2b0f1395a.jpg: 480x640 1 person, 1 bottle, 1 remote, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2056930414_d2b0f1395a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601508034_5a3bfc905e.jpg: 480x640 2 persons, 1 frisbee, 1 sports ball, 1 baseball bat, 8.4ms\n",
      "Speed: 1.8ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3601508034_5a3bfc905e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/375171241_0302ad8481.jpg: 512x640 3 persons, 3 bicycles, 3 cars, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 375171241_0302ad8481.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2150564996_d173a506d7.jpg: 544x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2150564996_d173a506d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523819210_99782628fc.jpg: 480x640 4 persons, 1 chair, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3523819210_99782628fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/270864951_1737ae5479.jpg: 480x640 1 person, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 270864951_1737ae5479.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441145615_b4fcd9eea0.jpg: 448x640 9 persons, 6 bicycles, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3441145615_b4fcd9eea0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/462080147_ca088e6541.jpg: 640x480 4 persons, 1 bottle, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 462080147_ca088e6541.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647693147_0d0434351b.jpg: 640x448 1 person, 1 bird, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3647693147_0d0434351b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2399114095_c3196ff456.jpg: 448x640 3 dogs, 8.6ms\n",
      "Speed: 1.6ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2399114095_c3196ff456.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2844963839_ff09cdb81f.jpg: 512x640 2 persons, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2844963839_ff09cdb81f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/195084264_72fb347b0f.jpg: 640x480 2 persons, 1 umbrella, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 195084264_72fb347b0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3069037969_bb7319e0dc.jpg: 448x640 1 dog, 1 sports ball, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3069037969_bb7319e0dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2955099064_1815b00825.jpg: 640x448 2 persons, 1 frisbee, 9.0ms\n",
      "Speed: 1.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2955099064_1815b00825.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3304556387_203b9d4db0.jpg: 640x448 2 persons, 2 dogs, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3304556387_203b9d4db0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420469425_2980b4cd30.jpg: 448x640 5 persons, 1 tie, 2 potted plants, 8.8ms\n",
      "Speed: 1.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3420469425_2980b4cd30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2935986346_29df6cf692.jpg: 640x544 1 bear, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2935986346_29df6cf692.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2226440063_c085b30558.jpg: 480x640 1 dog, 8.7ms\n",
      "Speed: 1.7ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2226440063_c085b30558.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2367317953_503317493e.jpg: 640x544 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2367317953_503317493e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1433088025_bce2cb69f8.jpg: 448x640 18 persons, 8.7ms\n",
      "Speed: 1.6ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1433088025_bce2cb69f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3525453732_f74a38f111.jpg: 640x640 2 birds, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3525453732_f74a38f111.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2840344516_8e15fe2668.jpg: 384x640 3 persons, 1 truck, 2 boats, 9.8ms\n",
      "Speed: 1.4ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2840344516_8e15fe2668.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3590557969_d0270d518b.jpg: 640x384 1 person, 1 bicycle, 9.9ms\n",
      "Speed: 1.4ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3590557969_d0270d518b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1529044279_4922ead27c.jpg: 480x640 5 persons, 8.8ms\n",
      "Speed: 1.7ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1529044279_4922ead27c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375134059_7e9eb2ef01.jpg: 640x448 2 dogs, 8.8ms\n",
      "Speed: 1.6ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3375134059_7e9eb2ef01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1383698008_8ac53ed7ec.jpg: 352x640 1 person, 1 snowboard, 9.6ms\n",
      "Speed: 1.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 1383698008_8ac53ed7ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3023178539_836b50cd43.jpg: 448x640 4 persons, 7 cars, 1 frisbee, 8.6ms\n",
      "Speed: 1.8ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3023178539_836b50cd43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3406116788_c8f62e32d1.jpg: 640x480 1 person, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3406116788_c8f62e32d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/406901451_7eafd7568a.jpg: 320x640 1 dog, 1 sports ball, 9.7ms\n",
      "Speed: 1.2ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 406901451_7eafd7568a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3053813297_7ce5f87710.jpg: 480x640 7 persons, 1 dog, 3 chairs, 9.4ms\n",
      "Speed: 1.7ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3053813297_7ce5f87710.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347689_d0b1ac297d.jpg: 640x448 14 persons, 9.1ms\n",
      "Speed: 1.6ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347689_d0b1ac297d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/325005410_e1ff5041b5.jpg: 640x448 15 persons, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 325005410_e1ff5041b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/452363869_cad37e609f.jpg: 448x640 1 dog, 1 sports ball, 11.0ms\n",
      "Speed: 1.6ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 452363869_cad37e609f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426724811_137855b4f7.jpg: 640x512 5 persons, 1 bottle, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3426724811_137855b4f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/410042380_517ccee020.jpg: 480x640 6 persons, 1 chair, 1 potted plant, 1 dining table, 15.3ms\n",
      "Speed: 1.8ms preprocess, 15.3ms inference, 2.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 410042380_517ccee020.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2595186208_9b16fa0ee3.jpg: 480x640 1 bench, 1 bird, 10.7ms\n",
      "Speed: 3.2ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2595186208_9b16fa0ee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1095980313_3c94799968.jpg: 640x480 1 person, 1 surfboard, 10.4ms\n",
      "Speed: 2.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1095980313_3c94799968.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3524975665_7bec41578b.jpg: 640x448 6 persons, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3524975665_7bec41578b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1236964638_1808784a3c.jpg: 640x448 3 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1236964638_1808784a3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2656890977_7a9f0e4138.jpg: 640x448 1 person, 12.1ms\n",
      "Speed: 3.4ms preprocess, 12.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2656890977_7a9f0e4138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2648165716_02e2e74fd6.jpg: 640x448 1 person, 9.6ms\n",
      "Speed: 2.6ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2648165716_02e2e74fd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/437054333_5c2761b8cd.jpg: 640x640 3 persons, 1 bus, 9.6ms\n",
      "Speed: 3.4ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 437054333_5c2761b8cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535664885_c848c0faee.jpg: 480x640 2 persons, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3535664885_c848c0faee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569420080_72fbe84751.jpg: 640x416 3 persons, 1 tie, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3569420080_72fbe84751.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119887391_212f379797.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3119887391_212f379797.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2352414953_10f3cd0f1f.jpg: 512x640 1 person, 1 bowl, 1 pizza, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2352414953_10f3cd0f1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3169591322_d0b6d0cd04.jpg: 640x448 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3169591322_d0b6d0cd04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/436015762_8d0bae90c3.jpg: 640x448 1 person, 1 train, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 436015762_8d0bae90c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3716272233_7845ec5b3e.jpg: 448x640 2 birds, 13.2ms\n",
      "Speed: 3.4ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3716272233_7845ec5b3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/697778778_b52090709d.jpg: 640x448 3 persons, 11.9ms\n",
      "Speed: 2.7ms preprocess, 11.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 697778778_b52090709d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1579206585_5ca6a24db0.jpg: 480x640 1 person, 10.6ms\n",
      "Speed: 2.5ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1579206585_5ca6a24db0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606846822_28c40b933a.jpg: 640x448 3 persons, 2 skateboards, 9.8ms\n",
      "Speed: 2.2ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3606846822_28c40b933a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3266261886_36e1323d2f.jpg: 448x640 2 persons, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3266261886_36e1323d2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3186412658_2ab2ebd397.jpg: 512x640 5 persons, 1 frisbee, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3186412658_2ab2ebd397.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3188319076_71724fcc07.jpg: 448x640 1 dog, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3188319076_71724fcc07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3269661567_faf190885a.jpg: 448x640 8 persons, 1 handbag, 2 ties, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3269661567_faf190885a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3751594676_edfbfa0688.jpg: 640x448 3 persons, 1 surfboard, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3751594676_edfbfa0688.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3662406028_29b9e46a6f.jpg: 640x480 3 persons, 1 backpack, 1 tie, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3662406028_29b9e46a6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482668767_66004ce736.jpg: 448x640 6 persons, 1 bottle, 1 chair, 2 dining tables, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3482668767_66004ce736.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2415265825_fbfe0c8556.jpg: 448x640 1 dog, 1 bear, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2415265825_fbfe0c8556.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/745966757_6d16dfad8f.jpg: 640x544 1 person, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 745966757_6d16dfad8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512306469_1392697d32.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512306469_1392697d32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3360730513_211e1a4db6.jpg: 608x640 16 persons, 1 dog, 9.2ms\n",
      "Speed: 2.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3360730513_211e1a4db6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578914491_36019ba703.jpg: 480x640 2 dogs, 2 cows, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3578914491_36019ba703.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477715432_79d82487bb.jpg: 480x640 5 persons, 1 cup, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3477715432_79d82487bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3455920874_6fbec43194.jpg: 512x640 3 persons, 3 dogs, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3455920874_6fbec43194.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537806062_c50d814aba.jpg: 448x640 2 dogs, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3537806062_c50d814aba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070274658_fc39fd4f84.jpg: 448x640 3 persons, 1 potted plant, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3070274658_fc39fd4f84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042381160_ffe2b16808.jpg: 480x640 8 persons, 4 chairs, 1 tv, 2 laptops, 1 mouse, 2 keyboards, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3042381160_ffe2b16808.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3079073247_97e88e2eb7.jpg: 448x640 3 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3079073247_97e88e2eb7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2657484970_610e18144f.jpg: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2657484970_610e18144f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2203615439_3c7cdc39dc.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2203615439_3c7cdc39dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2875658507_c0d9ceae90.jpg: 576x640 2 birds, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2875658507_c0d9ceae90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2162564553_96de62c7e6.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2162564553_96de62c7e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1350811702_2ce7cfd0c5.jpg: 448x640 6 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1350811702_2ce7cfd0c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/542179694_e170e9e465.jpg: 416x640 5 persons, 2 trains, 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 542179694_e170e9e465.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/509123893_07b8ea82a9.jpg: 640x480 3 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 509123893_07b8ea82a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/312156254_ef31dca5ed.jpg: 480x640 1 dog, 1 sports ball, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 312156254_ef31dca5ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/61209225_8512e1dad5.jpg: 480x640 4 persons, 1 car, 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 61209225_8512e1dad5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2322601965_748d59dc57.jpg: 448x640 2 persons, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2322601965_748d59dc57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831217847_555b2f95ca.jpg: 448x640 1 person, 1 car, 1 truck, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2831217847_555b2f95ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3406930103_4db7b4dde0.jpg: 448x640 1 dog, 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3406930103_4db7b4dde0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2853811730_fbb8ab0878.jpg: 480x640 2 persons, 1 bicycle, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2853811730_fbb8ab0878.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2279945145_8815c59217.jpg: 448x640 1 car, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2279945145_8815c59217.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3117562746_62f57a02b5.jpg: 608x640 2 dogs, 14.2ms\n",
      "Speed: 5.5ms preprocess, 14.2ms inference, 5.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3117562746_62f57a02b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2845246160_d0d1bbd6f0.jpg: 512x640 6 persons, 1 bicycle, 1 sandwich, 12.3ms\n",
      "Speed: 2.8ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2845246160_d0d1bbd6f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219843860_332e5ca7d4.jpg: 640x480 1 person, 10.4ms\n",
      "Speed: 2.2ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 219843860_332e5ca7d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1984936420_3f3102132b.jpg: 640x480 2 persons, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1984936420_3f3102132b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2616284322_b13e7c344e.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2616284322_b13e7c344e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256283122_a4ef4a17cb.jpg: 480x640 13 persons, 2 bicycles, 1 dog, 1 frisbee, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 256283122_a4ef4a17cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2198494923_8159551be4.jpg: 640x448 4 persons, 1 surfboard, 10.1ms\n",
      "Speed: 2.1ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2198494923_8159551be4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3245939062_8ffe1d2be5.jpg: 448x640 3 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3245939062_8ffe1d2be5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514036362_5f2b9b7314.jpg: 448x640 1 person, 1 car, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 514036362_5f2b9b7314.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262075846_5695021d84.jpg: 448x640 7 persons, 2 sports balls, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3262075846_5695021d84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3212465975_b657f40eed.jpg: 640x448 4 persons, 1 skateboard, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3212465975_b657f40eed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394824046_51cec8e5e7.jpg: 640x448 1 person, 1 bottle, 1 bowl, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2394824046_51cec8e5e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2571096893_694ce79768.jpg: 480x640 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2571096893_694ce79768.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1449692616_60507875fb.jpg: 480x640 1 person, 1 cell phone, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1449692616_60507875fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2644430445_47c985a2ee.jpg: 640x448 7 persons, 2 cell phones, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2644430445_47c985a2ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247619370_a01fb21dd3.jpg: 448x640 1 person, 2 benchs, 1 handbag, 9.6ms\n",
      "Speed: 2.0ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 247619370_a01fb21dd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2238166082_140f8b01b8.jpg: 480x640 1 person, 1 boat, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2238166082_140f8b01b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612484827_0e479f9ee8.jpg: 448x640 1 person, 1 bench, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3612484827_0e479f9ee8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522749949_fb615cee47.jpg: 640x480 2 persons, 1 sports ball, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3522749949_fb615cee47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3164415865_612f9fd8bc.jpg: 448x640 4 persons, 3 cars, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3164415865_612f9fd8bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860372882_e0ef4131d4.jpg: 640x448 4 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2860372882_e0ef4131d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2342478660_faef1afea8.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2342478660_faef1afea8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/410422753_de506155fa.jpg: 512x640 1 dog, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 410422753_de506155fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647750811_395fbd397e.jpg: 320x640 1 dog, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 3.3ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3647750811_395fbd397e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3703413486_3c682732a0.jpg: 480x640 2 persons, 1 train, 1 truck, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3703413486_3c682732a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/386470686_1ae9242878.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 386470686_1ae9242878.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3572942419_16ebdc3d46.jpg: 448x640 4 persons, 1 cup, 1 chair, 1 dining table, 2 cell phones, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3572942419_16ebdc3d46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1394599090_fe0ba238f0.jpg: 448x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1394599090_fe0ba238f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/430173345_86388d8822.jpg: 448x640 1 dog, 1 frisbee, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 430173345_86388d8822.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1542033433_5453d4c466.jpg: 640x480 3 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1542033433_5453d4c466.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3559425864_0462d7613f.jpg: 448x640 2 persons, 2 bicycles, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3559425864_0462d7613f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3047264346_e24601bfbf.jpg: 640x512 3 persons, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3047264346_e24601bfbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640417354_b0b3e4aec9.jpg: 640x640 1 person, 1 bird, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3640417354_b0b3e4aec9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2918880895_e61f74f2f0.jpg: 448x640 2 persons, 1 tie, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2918880895_e61f74f2f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640020134_367941f5ec.jpg: 480x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3640020134_367941f5ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/299181827_8dc714101b.jpg: 448x640 2 persons, 2 frisbees, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 299181827_8dc714101b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3470951932_27ed74eb0b.jpg: 448x640 2 persons, 9 cars, 2 trucks, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3470951932_27ed74eb0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326376344_3306bf439e.jpg: 480x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3326376344_3306bf439e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2353102255_67d9d2e40a.jpg: 512x640 1 person, 2 dogs, 1 frisbee, 1 tv, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2353102255_67d9d2e40a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2333816000_7105d0ffac.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2333816000_7105d0ffac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/441212506_fcc321ac28.jpg: 480x640 3 dogs, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 441212506_fcc321ac28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/508958120_afe274f726.jpg: 448x640 3 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 508958120_afe274f726.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3510219078_670b6b3157.jpg: 448x640 3 dogs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3510219078_670b6b3157.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949880800_ca9a1bb7e6.jpg: 640x480 2 persons, 2 frisbees, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2949880800_ca9a1bb7e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2120469056_7a738413be.jpg: 640x480 1 person, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2120469056_7a738413be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3385956569_a849218e34.jpg: 448x640 1 person, 1 snowboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3385956569_a849218e34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2843695880_eeea6c67db.jpg: 640x448 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2843695880_eeea6c67db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2353088412_5e5804c6f5.jpg: 448x640 2 dogs, 1 frisbee, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2353088412_5e5804c6f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/533602654_9edc74385d.jpg: 480x640 4 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 533602654_9edc74385d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3515358125_9e1d796244.jpg: 448x640 2 cars, 1 dog, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3515358125_9e1d796244.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3187364311_4c2a87083b.jpg: 640x640 1 person, 1 dog, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3187364311_4c2a87083b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/347543966_b2053ae78c.jpg: 640x480 3 persons, 2 cars, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 347543966_b2053ae78c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2679565682_91ecd283ff.jpg: 480x640 2 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2679565682_91ecd283ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1785138090_76a56aaabc.jpg: 640x448 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1785138090_76a56aaabc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016178284_ec50a09e8c.jpg: 448x640 1 person, 1 backpack, 1 tennis racket, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3016178284_ec50a09e8c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/500308355_f0c19067c0.jpg: 640x480 1 person, 1 dog, 1 teddy bear, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 500308355_f0c19067c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102732029_9ae520914d.jpg: 448x640 2 persons, 1 bicycle, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2102732029_9ae520914d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2601612082_4b9be27426.jpg: 480x640 1 person, 2 chairs, 1 couch, 2 remotes, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2601612082_4b9be27426.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3512033659_7e8a0c2ffa.jpg: 448x640 8 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3512033659_7e8a0c2ffa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/245442617_407eba1e98.jpg: 480x640 1 person, 1 bird, 1 handbag, 1 banana, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 245442617_407eba1e98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2344898759_5674382bcd.jpg: 512x640 1 person, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2344898759_5674382bcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3646927481_5e0af1efab.jpg: 448x640 10 persons, 2 cars, 2 motorcycles, 1 kite, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3646927481_5e0af1efab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393035454_2d2370ffd4.jpg: 640x480 1 person, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3393035454_2d2370ffd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3485599424_94de8ede51.jpg: 640x480 1 bear, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3485599424_94de8ede51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2552816307_c7c8e7f6b4.jpg: 480x640 (no detections), 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2552816307_c7c8e7f6b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2318659263_c24005a5cb.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2318659263_c24005a5cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3308171165_20f93d2fba.jpg: 544x640 1 person, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3308171165_20f93d2fba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3009018821_ba47396e24.jpg: 480x640 2 persons, 1 chair, 2 potted plants, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3009018821_ba47396e24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2288315705_5f4c37d932.jpg: 640x480 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2288315705_5f4c37d932.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2832453252_a06f7826a8.jpg: 384x640 4 persons, 9.1ms\n",
      "Speed: 1.5ms preprocess, 9.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2832453252_a06f7826a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3370308329_7f2de5bb58.jpg: 416x640 5 persons, 8.5ms\n",
      "Speed: 1.6ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3370308329_7f2de5bb58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2294688426_96c8614f1d.jpg: 640x448 1 person, 7 cars, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2294688426_96c8614f1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/523991446_65dbc5a4a5.jpg: 640x448 6 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 523991446_65dbc5a4a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3704995657_e2e114083d.jpg: 640x448 (no detections), 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3704995657_e2e114083d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520321387_710ab74cda.jpg: 448x640 2 persons, 8.2ms\n",
      "Speed: 3.5ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3520321387_710ab74cda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2403544744_cba152f5c1.jpg: 448x640 1 dog, 1 sheep, 2 cows, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2403544744_cba152f5c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256275785_9c3af57576.jpg: 512x640 1 person, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3256275785_9c3af57576.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3711664623_ef87105ea7.jpg: 448x640 4 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3711664623_ef87105ea7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2358898017_24496b80e8.jpg: 512x640 1 bench, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2358898017_24496b80e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3660826540_481d25fbb0.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3660826540_481d25fbb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259119085_21613b69df.jpg: 448x640 3 persons, 1 surfboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259119085_21613b69df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2335619125_2e2034f2c3.jpg: 640x448 1 dog, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2335619125_2e2034f2c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2656987333_80dcc82c05.jpg: 640x448 4 persons, 1 horse, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2656987333_80dcc82c05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640743904_d14eea0a0b.jpg: 512x640 1 person, 1 car, 1 dog, 1 backpack, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3640743904_d14eea0a0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2762702644_2aa3bf9680.jpg: 640x480 1 person, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2762702644_2aa3bf9680.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2904601886_39e9d317b1.jpg: 448x640 3 dogs, 1 sheep, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2904601886_39e9d317b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987328689_96a2d814f1.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2987328689_96a2d814f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2512683710_991c9d466d.jpg: 480x640 4 persons, 2 boats, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2512683710_991c9d466d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2407091303_931c918490.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2407091303_931c918490.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3596428453_8cfdec4869.jpg: 448x640 8 persons, 1 cell phone, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3596428453_8cfdec4869.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3232030272_b2480a5fe7.jpg: 480x640 3 persons, 1 car, 1 hot dog, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3232030272_b2480a5fe7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/582788646_dc40748639.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 582788646_dc40748639.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/230016181_0c52b95304.jpg: 480x640 1 person, 2 dogs, 1 tennis racket, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 230016181_0c52b95304.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/33108590_d685bfe51c.jpg: 448x640 3 persons, 1 bottle, 1 bowl, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 33108590_d685bfe51c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1248357227_2b4175fc39.jpg: 448x640 2 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1248357227_2b4175fc39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2332986053_864db84971.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2332986053_864db84971.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1819261140_6c022f4b1d.jpg: 640x480 1 person, 1 bottle, 2 cups, 1 dining table, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1819261140_6c022f4b1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2209496328_2a34fd201d.jpg: 480x640 2 bears, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2209496328_2a34fd201d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488352274_9a22064cb3.jpg: 480x640 1 dog, 1 cow, 1 frisbee, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 488352274_9a22064cb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/494907021_321e82877a.jpg: 480x640 6 persons, 1 bench, 10.7ms\n",
      "Speed: 2.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 494907021_321e82877a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2988244398_5da7012fce.jpg: 640x512 1 person, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2988244398_5da7012fce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532999240_1409d073be.jpg: 448x640 1 dog, 1 sports ball, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 532999240_1409d073be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667404919_b273df57e4.jpg: 448x640 2 persons, 2 motorcycles, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3667404919_b273df57e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/191003287_2915c11d8e.jpg: 480x640 4 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 191003287_2915c11d8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836553263_b1a08c25ea.jpg: 448x640 2 persons, 2 bicycles, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2836553263_b1a08c25ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349528565_0bc013b70a.jpg: 448x640 1 person, 2 skateboards, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3349528565_0bc013b70a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2268729848_d418451226.jpg: 448x640 4 persons, 1 train, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2268729848_d418451226.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339751521_7a8768be27.jpg: 384x640 4 persons, 2 backpacks, 1 handbag, 9.5ms\n",
      "Speed: 1.5ms preprocess, 9.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3339751521_7a8768be27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3055716848_b253324afc.jpg: 448x640 4 persons, 1 dog, 1 skateboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3055716848_b253324afc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/933118213_b35b0b62a7.jpg: 416x640 1 person, 1 kite, 8.1ms\n",
      "Speed: 1.6ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 933118213_b35b0b62a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1402859872_0fc8cf8108.jpg: 448x640 2 persons, 1 banana, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1402859872_0fc8cf8108.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3166366760_e43cf66eda.jpg: 480x640 1 person, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3166366760_e43cf66eda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2744705574_519c171ca0.jpg: 448x640 1 dog, 1 frisbee, 1 surfboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2744705574_519c171ca0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/309687244_4bdf3b591f.jpg: 640x448 1 person, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 309687244_4bdf3b591f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656906086_7034f69ab6.jpg: 576x640 1 dog, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3656906086_7034f69ab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3610836023_3a972b10b0.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3610836023_3a972b10b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2380740486_8cd5d4601a.jpg: 576x640 1 person, 3 birds, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2380740486_8cd5d4601a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2563578471_9a4e4c2ecc.jpg: 480x640 1 person, 1 surfboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2563578471_9a4e4c2ecc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3581538034_783b7d0d09.jpg: 640x448 1 person, 1 surfboard, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3581538034_783b7d0d09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2553550034_5901aa9d6c.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2553550034_5901aa9d6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417662443_2eaea88977.jpg: 640x480 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3417662443_2eaea88977.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/284279868_2ca98e3dcd.jpg: 384x640 1 dog, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 284279868_2ca98e3dcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498444334_a680d318a1.jpg: 544x640 7 persons, 1 bottle, 2 potted plants, 1 remote, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 498444334_a680d318a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/448590900_db83c42006.jpg: 480x640 1 cow, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 448590900_db83c42006.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1396703063_e8c3687afe.jpg: 640x448 3 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1396703063_e8c3687afe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3732728142_86364a706e.jpg: 640x480 1 person, 1 teddy bear, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3732728142_86364a706e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346317_be3f07bd2e.jpg: 448x640 12 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346317_be3f07bd2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182121297_38c99b2769.jpg: 640x448 1 person, 1 snowboard, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3182121297_38c99b2769.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3164423279_9b27cb6a06.jpg: 448x640 1 person, 1 snowboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3164423279_9b27cb6a06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2080033499_6be742f483.jpg: 416x640 1 person, 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2080033499_6be742f483.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516825206_5750824874.jpg: 448x640 3 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3516825206_5750824874.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/123889082_d3751e0350.jpg: 480x640 2 dogs, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 123889082_d3751e0350.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1028205764_7e8df9a2ea.jpg: 384x640 2 persons, 2 boats, 8.1ms\n",
      "Speed: 1.4ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1028205764_7e8df9a2ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3231575742_31732664cf.jpg: 448x640 1 person, 1 skis, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3231575742_31732664cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2034553054_b00c166895.jpg: 480x640 1 person, 1 umbrella, 1 cup, 1 bowl, 3 apples, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2034553054_b00c166895.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2115631346_9585a479b0.jpg: 640x512 2 dogs, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2115631346_9585a479b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/917574521_74fab68514.jpg: 448x640 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 917574521_74fab68514.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3090386315_87ed417814.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3090386315_87ed417814.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371279606_c0d0cddab2.jpg: 544x640 4 persons, 1 cup, 1 bowl, 3 chairs, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3371279606_c0d0cddab2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2925242998_9e0db9b4a2.jpg: 448x640 1 person, 1 backpack, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2925242998_9e0db9b4a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2990471798_73c50c76fb.jpg: 640x448 1 person, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2990471798_73c50c76fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2138487671_5b89104043.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2138487671_5b89104043.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839807428_efe42423f2.jpg: 448x640 4 persons, 1 dog, 2 frisbees, 1 sports ball, 2 surfboards, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2839807428_efe42423f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/479807465_cf42f39d00.jpg: 640x448 2 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 479807465_cf42f39d00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004359992_f6b3617706.jpg: 640x384 4 persons, 1 sports ball, 1 tennis racket, 8.9ms\n",
      "Speed: 1.4ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3004359992_f6b3617706.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3297323827_f582356478.jpg: 640x640 1 person, 1 surfboard, 8.2ms\n",
      "Speed: 2.5ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3297323827_f582356478.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587941206_36769c3f1d.jpg: 640x512 8 persons, 1 baseball bat, 1 baseball glove, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3587941206_36769c3f1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/12830823_87d2654e31.jpg: 480x640 15 persons, 1 potted plant, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 12830823_87d2654e31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/388386075_9ac3a89ada.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 388386075_9ac3a89ada.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3644142276_caed26029e.jpg: 640x608 7 persons, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3644142276_caed26029e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441531010_8eebbb507e.jpg: 512x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3441531010_8eebbb507e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2574084102_f2be3f73cb.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2574084102_f2be3f73cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3044536048_e615466e7f.jpg: 384x640 1 person, 1 surfboard, 8.4ms\n",
      "Speed: 1.6ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3044536048_e615466e7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/399679638_d3036da331.jpg: 480x640 1 bird, 1 cow, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 399679638_d3036da331.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1334892555_1beff092c3.jpg: 480x640 3 persons, 2 cars, 1 truck, 2 baseball bats, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1334892555_1beff092c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2909955251_4b326a46a7.jpg: 480x640 1 person, 1 boat, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2909955251_4b326a46a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2694426634_118566f7ab.jpg: 640x448 3 persons, 1 boat, 1 handbag, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2694426634_118566f7ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2707933554_f6dc5e0e3c.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2707933554_f6dc5e0e3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3115354165_44dbeec6c1.jpg: 640x448 12 persons, 1 skateboard, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3115354165_44dbeec6c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978236380_fb24c43f1e.jpg: 416x640 2 persons, 1 chair, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2978236380_fb24c43f1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2049646140_d0de01e3c4.jpg: 480x640 3 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2049646140_d0de01e3c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2282043629_91b7831352.jpg: 480x640 5 persons, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2282043629_91b7831352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/103195344_5d2dc613a3.jpg: 640x480 3 persons, 2 backpacks, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 103195344_5d2dc613a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/343662720_39e4067cd1.jpg: 448x640 1 person, 5 books, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 343662720_39e4067cd1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372215826_b3e6403b2e.jpg: 448x640 2 persons, 1 cell phone, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3372215826_b3e6403b2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2553619107_d382a820f9.jpg: 448x640 1 surfboard, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2553619107_d382a820f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2381613738_d8d2012e3c.jpg: 640x640 1 person, 7.3ms\n",
      "Speed: 2.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2381613738_d8d2012e3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521213787_ca9b5a1758.jpg: 448x640 2 persons, 2 cars, 1 truck, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2521213787_ca9b5a1758.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2716457668_187a6d2b1c.jpg: 448x640 3 persons, 1 cell phone, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2716457668_187a6d2b1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/483136916_16976f4902.jpg: 512x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 483136916_16976f4902.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/510197538_0a11b94460.jpg: 640x448 1 person, 1 tennis racket, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 510197538_0a11b94460.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/395461421_c586b136de.jpg: 448x640 4 persons, 1 car, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 395461421_c586b136de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351370405_e417e38f52.jpg: 640x448 1 person, 1 snowboard, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3351370405_e417e38f52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373481779_511937e09d.jpg: 480x640 3 persons, 1 baseball glove, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3373481779_511937e09d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3024172109_a10198e1dd.jpg: 640x480 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3024172109_a10198e1dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2335428699_4eba9b6245.jpg: 448x640 5 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2335428699_4eba9b6245.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/533713007_bf9f3e25b4.jpg: 480x640 1 person, 1 bottle, 1 cup, 2 bowls, 4 potted plants, 2 dining tables, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 533713007_bf9f3e25b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/754852108_72f80d421f.jpg: 640x512 4 persons, 10.4ms\n",
      "Speed: 2.8ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 754852108_72f80d421f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632842482_482f29e712.jpg: 448x640 2 cats, 3 dogs, 1 couch, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3632842482_482f29e712.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399798295_a452963365.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3399798295_a452963365.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3041384194_04316bd416.jpg: 544x640 1 person, 9.6ms\n",
      "Speed: 2.6ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3041384194_04316bd416.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262963190_a78b799e89.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 262963190_a78b799e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3213992947_3f3f967a9f.jpg: 480x640 7 persons, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3213992947_3f3f967a9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3128514681_a51b415c31.jpg: 480x640 2 persons, 1 handbag, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3128514681_a51b415c31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589367895_5d3729e3ea.jpg: 640x448 2 persons, 2 ties, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3589367895_5d3729e3ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/142802798_962a4ec5ce.jpg: 448x640 1 person, 1 skateboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 142802798_962a4ec5ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/226607225_44d696db6b.jpg: 640x448 1 surfboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 226607225_44d696db6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561295656_4f21fba209.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2561295656_4f21fba209.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3528105511_12ff45dc9c.jpg: 448x640 1 person, 1 motorcycle, 9.5ms\n",
      "Speed: 2.8ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3528105511_12ff45dc9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014773357_f66bd09290.jpg: 192x640 2 birds, 1 sports ball, 10.6ms\n",
      "Speed: 1.1ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 192, 640)\n",
      "Cropped images saved for 3014773357_f66bd09290.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3422979565_e08cd77bfe.jpg: 480x640 16 persons, 2 boats, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3422979565_e08cd77bfe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286235203_af3cd8f243.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2286235203_af3cd8f243.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3646453252_5ebbbaa6cc.jpg: 448x640 1 person, 1 motorcycle, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3646453252_5ebbbaa6cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414390475_28a0107bb0.jpg: 640x448 3 persons, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2414390475_28a0107bb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627679667_0e3de9fc90.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3627679667_0e3de9fc90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/181157221_e12410ef0b.jpg: 480x640 2 persons, 2 dogs, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 181157221_e12410ef0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1303727828_d1052ee341.jpg: 448x640 3 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1303727828_d1052ee341.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2853743795_e90ebc669d.jpg: 640x512 1 person, 1 dog, 8.3ms\n",
      "Speed: 1.8ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2853743795_e90ebc669d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/103106960_e8a41d64f8.jpg: 480x640 1 person, 1 baseball bat, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 103106960_e8a41d64f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/292887910_f34ac101c8.jpg: 448x640 1 dog, 1 sheep, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 292887910_f34ac101c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1904112245_549e47c8aa.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1904112245_549e47c8aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2447289477_e888df561d.jpg: 480x640 4 persons, 1 bottle, 1 chair, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2447289477_e888df561d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3616638478_641d02183d.jpg: 640x480 3 persons, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3616638478_641d02183d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/173020287_230bfc4ffc.jpg: 640x448 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 173020287_230bfc4ffc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2053441349_a98b5fc742.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2053441349_a98b5fc742.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836360729_6500249fe6.jpg: 640x640 3 persons, 2 umbrellas, 1 surfboard, 1 chair, 7.7ms\n",
      "Speed: 2.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2836360729_6500249fe6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3383037991_8f07784b81.jpg: 480x640 1 person, 1 dog, 1 bottle, 1 couch, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3383037991_8f07784b81.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/540604040_bec822c144.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 540604040_bec822c144.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211581957_df2f7e2236.jpg: 448x640 2 persons, 1 horse, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3211581957_df2f7e2236.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3422146099_35ffc8680e.jpg: 480x640 1 car, 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3422146099_35ffc8680e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/654130822_4aeb1f1273.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 654130822_4aeb1f1273.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934359101_cdf57442dc.jpg: 640x448 4 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2934359101_cdf57442dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3419916411_72934edcdb.jpg: 608x640 1 dog, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3419916411_72934edcdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484019369_354e0b88c0.jpg: 448x640 21 persons, 1 tie, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3484019369_354e0b88c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3476381830_3751dd9339.jpg: 352x640 1 person, 1 bench, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3476381830_3751dd9339.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3651476768_2bae721a6b.jpg: 480x640 2 persons, 8.5ms\n",
      "Speed: 1.6ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3651476768_2bae721a6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3728256505_7f8db8270d.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3728256505_7f8db8270d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3238951136_2a99f1a1a8.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3238951136_2a99f1a1a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3110018626_307a123b59.jpg: 448x640 2 dogs, 1 cow, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3110018626_307a123b59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154293126_e52bd07524.jpg: 448x640 1 person, 2 snowboards, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3154293126_e52bd07524.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2201192417_d934730fea.jpg: 640x448 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2201192417_d934730fea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139389284_f01bd4c236.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3139389284_f01bd4c236.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2550011909_6b95f11330.jpg: 480x640 4 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2550011909_6b95f11330.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107513635_fe8a21f148.jpg: 480x640 13 persons, 1 train, 1 umbrella, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3107513635_fe8a21f148.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3656151153_b4ed5d94c4.jpg: 448x640 1 person, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3656151153_b4ed5d94c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2360194369_d2fd03b337.jpg: 480x640 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2360194369_d2fd03b337.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3542484764_77d8920ec9.jpg: 640x576 1 person, 1 car, 1 skis, 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3542484764_77d8920ec9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535284878_f90f10236e.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3535284878_f90f10236e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1112212364_0c48235fc2.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1112212364_0c48235fc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/458735196_176e7df6b3.jpg: 640x480 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 458735196_176e7df6b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329793486_afc16663cc.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3329793486_afc16663cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/191003283_992257f835.jpg: 480x640 11 persons, 2 umbrellas, 1 skateboard, 4 chairs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 191003283_992257f835.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1015118661_980735411b.jpg: 480x640 2 persons, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1015118661_980735411b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/410453140_5401bf659a.jpg: 448x640 3 persons, 1 umbrella, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 410453140_5401bf659a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244571201_0339d8e8d1.jpg: 544x640 6 persons, 1 bench, 5 dogs, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 244571201_0339d8e8d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405961988_fcfe97f31e.jpg: 640x480 1 person, 1 car, 9.7ms\n",
      "Speed: 2.0ms preprocess, 9.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 405961988_fcfe97f31e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/217583047_5e93e1e119.jpg: 640x416 1 person, 1 boat, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 217583047_5e93e1e119.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2481598514_05a65d1f72.jpg: 480x640 2 persons, 1 motorcycle, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2481598514_05a65d1f72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1148238960_f8cacec2fc.jpg: 640x448 4 persons, 4 bicycles, 2 cars, 2 traffic lights, 1 backpack, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1148238960_f8cacec2fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2288099178_41091aa00c.jpg: 640x448 8 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2288099178_41091aa00c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2308271254_27fb466eb4.jpg: 640x640 2 dogs, 7.6ms\n",
      "Speed: 2.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2308271254_27fb466eb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710073758_ac2b217f29.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3710073758_ac2b217f29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3670918456_68631d362a.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3670918456_68631d362a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2940366012_1ef8ab334e.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2940366012_1ef8ab334e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3269895626_7b253c82ed.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3269895626_7b253c82ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2784625888_71a421e171.jpg: 640x640 1 dog, 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2784625888_71a421e171.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877511986_c965ced502.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2877511986_c965ced502.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/385835044_4aa11f6990.jpg: 640x544 1 person, 1 sports ball, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 385835044_4aa11f6990.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2303426046_09cfd7bc4e.jpg: 640x448 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2303426046_09cfd7bc4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2874728371_ccd6db87f3.jpg: 448x640 1 dog, 1 horse, 1 cow, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2874728371_ccd6db87f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596474836_79468f23a0.jpg: 640x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2596474836_79468f23a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3047749814_621ed0786b.jpg: 480x640 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3047749814_621ed0786b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2756765580_9e57e10f0d.jpg: 448x640 2 persons, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2756765580_9e57e10f0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978394277_4572967b97.jpg: 640x448 7 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2978394277_4572967b97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3246190363_68d903bfcb.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3246190363_68d903bfcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3036596725_541bbe0955.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3036596725_541bbe0955.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1280320287_b2a4b9b7bd.jpg: 448x640 3 persons, 1 boat, 1 surfboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1280320287_b2a4b9b7bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2378127945_8dc9da82d7.jpg: 640x480 1 cow, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2378127945_8dc9da82d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2812125355_5e11a76533.jpg: 640x448 1 person, 1 frisbee, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2812125355_5e11a76533.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695962887_a1647c567b.jpg: 384x640 3 persons, 1 dog, 9.3ms\n",
      "Speed: 1.4ms preprocess, 9.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2695962887_a1647c567b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1164765687_7aca07bbe7.jpg: 480x640 1 person, 1 dog, 1 bed, 1 teddy bear, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1164765687_7aca07bbe7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495055747_a75872762a.jpg: 448x640 1 person, 9 cars, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 495055747_a75872762a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265964840_5374ed9c53.jpg: 640x448 1 person, 1 snowboard, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3265964840_5374ed9c53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089555297_95cf001fa7.jpg: 480x640 1 person, 1 backpack, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2089555297_95cf001fa7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278105206_df987b0ca0.jpg: 640x608 1 dog, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 278105206_df987b0ca0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109202756_b97fcdc62c.jpg: 640x480 1 person, 1 horse, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 109202756_b97fcdc62c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033612929_764d977bd5.jpg: 512x640 2 dogs, 1 frisbee, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3033612929_764d977bd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/334768700_51c439b9ee.jpg: 480x640 1 person, 1 frisbee, 3 cell phones, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 334768700_51c439b9ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3138562460_44227a35cf.jpg: 640x448 1 person, 1 horse, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3138562460_44227a35cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564385317_1bf5094068.jpg: 640x448 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3564385317_1bf5094068.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2121357310_f8235311da.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2121357310_f8235311da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3170110692_d1e0e66cee.jpg: 448x640 2 persons, 1 cell phone, 2 toothbrushs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3170110692_d1e0e66cee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2901880865_3fd7b66a45.jpg: 416x640 1 person, 2 surfboards, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2901880865_3fd7b66a45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2861413434_f0e2a10179.jpg: 448x640 1 person, 1 car, 1 sports ball, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2861413434_f0e2a10179.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3424934891_69f18da66e.jpg: 640x480 2 dogs, 1 frisbee, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3424934891_69f18da66e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3048821353_83d4c0cbb9.jpg: 448x640 3 persons, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3048821353_83d4c0cbb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372251830_baa3665928.jpg: 480x640 6 persons, 14.0ms\n",
      "Speed: 4.1ms preprocess, 14.0ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3372251830_baa3665928.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3053415073_5b667230ed.jpg: 448x640 1 person, 1 snowboard, 11.0ms\n",
      "Speed: 2.7ms preprocess, 11.0ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3053415073_5b667230ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2584020755_14e2b3e8fc.jpg: 448x640 11 persons, 1 remote, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2584020755_14e2b3e8fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154709407_9b0778cbeb.jpg: 640x448 9 persons, 1 sports ball, 1 surfboard, 4 chairs, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3154709407_9b0778cbeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3195188609_01afbe46e6.jpg: 288x640 16 persons, 1 horse, 10.7ms\n",
      "Speed: 1.4ms preprocess, 10.7ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3195188609_01afbe46e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3241892328_4ebf8b21ce.jpg: 448x640 1 bear, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3241892328_4ebf8b21ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3363836972_c87b58c948.jpg: 640x448 7 persons, 1 sports ball, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3363836972_c87b58c948.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374104006_7f32c8c5de.jpg: 640x480 4 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 374104006_7f32c8c5de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2786299623_a3c48bd318.jpg: 640x448 3 persons, 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2786299623_a3c48bd318.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1410193619_13fff6c875.jpg: 640x544 1 person, 2 chairs, 1 couch, 9.1ms\n",
      "Speed: 2.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1410193619_13fff6c875.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2531837969_6f28637811.jpg: 640x448 1 person, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2531837969_6f28637811.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203707977_cc9448fecb.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3203707977_cc9448fecb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2597737483_6518a230e4.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2597737483_6518a230e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766765386_4c0beb939d.jpg: 448x640 1 person, 2 surfboards, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2766765386_4c0beb939d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404408360_430f73b034.jpg: 448x640 7 persons, 1 handbag, 1 suitcase, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3404408360_430f73b034.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1569562856_eedb5a0a1f.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1569562856_eedb5a0a1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/765929807_de381cc764.jpg: 480x640 7 persons, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 765929807_de381cc764.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3511890331_6163612bb9.jpg: 448x640 1 person, 1 horse, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3511890331_6163612bb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2409597310_958f5d8aff.jpg: 448x640 4 persons, 1 bench, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2409597310_958f5d8aff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2107837987_ffecfc367a.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2107837987_ffecfc367a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1388970365_162edcceb4.jpg: 480x640 2 dogs, 1 cow, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1388970365_162edcceb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273799395_5072a5736d.jpg: 480x640 5 persons, 2 skiss, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2273799395_5072a5736d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/254901702_67ada9867c.jpg: 640x544 1 person, 1 frisbee, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 254901702_67ada9867c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601491447_a338875b51.jpg: 480x640 10 persons, 1 sports ball, 1 baseball glove, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3601491447_a338875b51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249738122_decde6c117.jpg: 640x544 15 persons, 5 dogs, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3249738122_decde6c117.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1527297882_dededc7891.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1527297882_dededc7891.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3471571540_b4ab77f20d.jpg: 448x640 1 person, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3471571540_b4ab77f20d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2704257993_d485058a5f.jpg: 640x544 6 persons, 1 skateboard, 6.8ms\n",
      "Speed: 2.0ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2704257993_d485058a5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1141739219_2c47195e4c.jpg: 576x640 6 persons, 1 truck, 1 backpack, 2 handbags, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 1141739219_2c47195e4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2337809114_899ba61330.jpg: 480x640 4 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2337809114_899ba61330.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1452361926_6d8c535e32.jpg: 640x480 1 person, 1 bottle, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1452361926_6d8c535e32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/293151893_ee7249eccb.jpg: 640x640 2 dogs, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 293151893_ee7249eccb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549408779_4d453db080.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3549408779_4d453db080.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286032269_8ba929709c.jpg: 480x640 3 persons, 1 surfboard, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2286032269_8ba929709c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163323414_d1ce127aa6.jpg: 448x640 6 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3163323414_d1ce127aa6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613323772_d15cef66d1.jpg: 640x448 1 person, 1 skateboard, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3613323772_d15cef66d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543326592_70bd4d8602.jpg: 512x640 19 persons, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 543326592_70bd4d8602.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246055693_ccb69ac5c6.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 246055693_ccb69ac5c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2678612999_893ed671f8.jpg: 640x448 6 persons, 2 cars, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2678612999_893ed671f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1806580620_a8fe0fb9f8.jpg: 640x544 1 person, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1806580620_a8fe0fb9f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/811662356_f9a632b63c.jpg: 640x448 1 person, 2 tennis rackets, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 811662356_f9a632b63c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197791645_601908f26b.jpg: 480x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3197791645_601908f26b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042679440_010b2c596c.jpg: 480x640 20 persons, 6.5ms\n",
      "Speed: 1.9ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3042679440_010b2c596c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3068407619_5207b26986.jpg: 448x640 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3068407619_5207b26986.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1445754124_647168f211.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1445754124_647168f211.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122888809_9ae9b4b9b2.jpg: 448x640 2 dogs, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3122888809_9ae9b4b9b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2978735290_7464b12270.jpg: 448x640 2 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2978735290_7464b12270.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2805101709_1c8916f63a.jpg: 640x480 3 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2805101709_1c8916f63a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/864290968_eccb46d5ab.jpg: 416x640 3 persons, 1 bus, 1 train, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 864290968_eccb46d5ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467073304_aefe553c4d.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3467073304_aefe553c4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033210806_3ffc0a231a.jpg: 416x640 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3033210806_3ffc0a231a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/94232465_a135df2711.jpg: 640x448 2 persons, 1 suitcase, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 94232465_a135df2711.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247652942_29ede19352.jpg: 640x448 1 person, 1 kite, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 247652942_29ede19352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2285741931_07159a21f2.jpg: 416x640 1 person, 1 baseball bat, 1 skateboard, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2285741931_07159a21f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2225864432_48a24f49a4.jpg: 608x640 2 persons, 7.5ms\n",
      "Speed: 2.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2225864432_48a24f49a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2881087519_ca0aa79b2b.jpg: 448x640 1 person, 1 bicycle, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2881087519_ca0aa79b2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2225231022_1632d0a5aa.jpg: 448x640 12 persons, 1 backpack, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2225231022_1632d0a5aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2479162876_a5ce3306af.jpg: 480x640 1 person, 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2479162876_a5ce3306af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482830610_13a0a6c924.jpg: 480x640 1 person, 1 chair, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 482830610_13a0a6c924.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752084369_52e7867da7.jpg: 640x480 1 person, 1 suitcase, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2752084369_52e7867da7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/531261613_f1a045cd75.jpg: 448x640 8 persons, 4 horses, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 531261613_f1a045cd75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3717309680_e5105afa6d.jpg: 512x640 5 persons, 4 motorcycles, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3717309680_e5105afa6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271084924_4778d556cc.jpg: 480x640 1 person, 1 bicycle, 4 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3271084924_4778d556cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2854234756_8c0e472f51.jpg: 480x640 1 person, 12.3ms\n",
      "Speed: 3.9ms preprocess, 12.3ms inference, 2.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2854234756_8c0e472f51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2260560631_09093be4c6.jpg: 448x640 1 bear, 11.0ms\n",
      "Speed: 2.7ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2260560631_09093be4c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3379839396_0cd84b55f1.jpg: 448x640 1 person, 1 cup, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3379839396_0cd84b55f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3632572264_577703b384.jpg: 480x640 11 persons, 1 sports ball, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3632572264_577703b384.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3328397409_092de2bd32.jpg: 448x640 1 person, 1 snowboard, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3328397409_092de2bd32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2833582518_074bef3ed6.jpg: 448x640 1 horse, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2833582518_074bef3ed6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676651833_3bb42bbb32.jpg: 448x640 5 persons, 1 surfboard, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2676651833_3bb42bbb32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/940973925_a2e6d7951c.jpg: 544x640 4 persons, 1 sports ball, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 940973925_a2e6d7951c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569979711_6507841268.jpg: 480x640 3 persons, 1 surfboard, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3569979711_6507841268.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2686432878_0697dbc048.jpg: 416x640 1 person, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2686432878_0697dbc048.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1342766791_1e72f92455.jpg: 640x448 5 persons, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1342766791_1e72f92455.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2319808437_bbbdc317c0.jpg: 480x640 4 persons, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2319808437_bbbdc317c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/500678178_26ce0f4417.jpg: 480x640 1 person, 9.6ms\n",
      "Speed: 2.7ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 500678178_26ce0f4417.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3605061440_1d08c80a57.jpg: 640x640 9 persons, 1 dog, 2 sheeps, 10.4ms\n",
      "Speed: 3.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3605061440_1d08c80a57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2182050469_1edac0bc60.jpg: 640x512 2 persons, 10.1ms\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2182050469_1edac0bc60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2937697444_2367ff0e28.jpg: 640x640 1 person, 9.3ms\n",
      "Speed: 3.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2937697444_2367ff0e28.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647283075_3005333222.jpg: 640x448 1 person, 1 skateboard, 10.1ms\n",
      "Speed: 2.1ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3647283075_3005333222.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2646540383_343e1ec9a4.jpg: 640x448 1 person, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2646540383_343e1ec9a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2289212650_69de7a20b2.jpg: 640x640 2 dogs, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2289212650_69de7a20b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1414779054_31946f9dfc.jpg: 576x640 3 persons, 2 cars, 8.8ms\n",
      "Speed: 2.6ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 1414779054_31946f9dfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2493469969_11b6190615.jpg: 640x480 1 dog, 1 horse, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2493469969_11b6190615.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107889179_106d223345.jpg: 544x640 4 persons, 1 bottle, 2 wine glasss, 2 cups, 4 chairs, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3107889179_106d223345.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3616525288_9c19223de6.jpg: 448x640 4 persons, 4 horses, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3616525288_9c19223de6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272823323_3b7291cd47.jpg: 480x640 2 persons, 3 backpacks, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2272823323_3b7291cd47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460126267_0deea8b645.jpg: 448x640 1 person, 1 sports ball, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2460126267_0deea8b645.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442272060_f9155194c2.jpg: 480x640 3 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3442272060_f9155194c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2281006675_fde04e93dd.jpg: 448x640 1 dog, 1 horse, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2281006675_fde04e93dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3213622536_31da7f6682.jpg: 448x640 10 persons, 1 kite, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3213622536_31da7f6682.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2504056718_25ded44ecb.jpg: 480x640 1 dog, 1 frisbee, 1 cup, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2504056718_25ded44ecb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3596459539_a47aa80612.jpg: 512x640 5 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3596459539_a47aa80612.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1480712062_32a61ad4b7.jpg: 640x480 (no detections), 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1480712062_32a61ad4b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2847514745_9a35493023.jpg: 480x640 9 persons, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2847514745_9a35493023.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2658009523_b49d611db8.jpg: 448x640 2 dogs, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2658009523_b49d611db8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3419238351_ac18b440c0.jpg: 480x640 5 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3419238351_ac18b440c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139393607_f0a54ca46d.jpg: 352x640 6 persons, 1 snowboard, 8.4ms\n",
      "Speed: 1.2ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3139393607_f0a54ca46d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089442007_6fc798548c.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2089442007_6fc798548c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991575785_bd4868e215.jpg: 448x640 3 persons, 5 cars, 2 umbrellas, 1 cup, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2991575785_bd4868e215.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480668859_6f9b46be6a.jpg: 480x640 9 persons, 3 boats, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2480668859_6f9b46be6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296584432_bef3c965a3.jpg: 480x640 1 person, 1 dog, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3296584432_bef3c965a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263395801_5e4cee2b9e.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3263395801_5e4cee2b9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399906919_bc8562b257.jpg: 640x416 2 persons, 4 cups, 1 dining table, 2 vases, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3399906919_bc8562b257.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/491405109_798222cfd0.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 491405109_798222cfd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/480505313_2dc686e5db.jpg: 480x640 4 persons, 1 handbag, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 480505313_2dc686e5db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109738763_90541ef30d.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 109738763_90541ef30d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534313000_4ad39c7ee0.jpg: 480x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 534313000_4ad39c7ee0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3334953664_a669038795.jpg: 640x608 1 person, 1 sports ball, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3334953664_a669038795.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516653997_98ec551a67.jpg: 448x640 14 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3516653997_98ec551a67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3702607829_2b8b3e65ab.jpg: 480x640 4 persons, 1 bench, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3702607829_2b8b3e65ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126681108_f88128699c.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3126681108_f88128699c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/878758390_dd2cdc42f6.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 878758390_dd2cdc42f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217240672_b99a682026.jpg: 448x640 8 persons, 1 bicycle, 1 motorcycle, 1 bus, 1 truck, 1 elephant, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3217240672_b99a682026.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457364788_3514a52091.jpg: 640x448 4 persons, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3457364788_3514a52091.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/536828916_b763b82949.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 536828916_b763b82949.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2785115802_137fa30000.jpg: 480x640 1 bear, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2785115802_137fa30000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2555638166_2f0847d57d.jpg: 640x480 3 persons, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2555638166_2f0847d57d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2826769554_85c90864c9.jpg: 640x640 1 bird, 1 dog, 8.3ms\n",
      "Speed: 2.6ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2826769554_85c90864c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2221818690_9003756d33.jpg: 480x640 3 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2221818690_9003756d33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2557507575_b247f145bc.jpg: 480x640 1 sheep, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2557507575_b247f145bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1115565519_d976d4b1f1.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1115565519_d976d4b1f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173928684_4ea0ee5114.jpg: 448x640 1 dog, 1 sheep, 1 bear, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3173928684_4ea0ee5114.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373069977_bc73e9e409.jpg: 416x640 4 persons, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3373069977_bc73e9e409.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3706356018_28f62290e8.jpg: 640x512 7 persons, 1 umbrella, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3706356018_28f62290e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339747039_1a8455c210.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3339747039_1a8455c210.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/387078972_514a38dc33.jpg: 480x640 1 dog, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 387078972_514a38dc33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2534194182_ac53035cf4.jpg: 416x640 10 persons, 2 sports balls, 7.4ms\n",
      "Speed: 1.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2534194182_ac53035cf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2714703706_d21c5cb8df.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2714703706_d21c5cb8df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/404702274_fa8b3fe378.jpg: 448x640 1 dog, 1 frisbee, 1 skis, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 404702274_fa8b3fe378.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1225443522_1633e7121f.jpg: 448x640 4 persons, 6 cars, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1225443522_1633e7121f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1088767354_2acee738cf.jpg: 640x640 5 persons, 1 car, 1 handbag, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1088767354_2acee738cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461677493_5bfb73038e.jpg: 640x448 2 persons, 1 cake, 2 potted plants, 1 laptop, 1 vase, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3461677493_5bfb73038e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2541701582_0a651c380f.jpg: 448x640 2 persons, 3 bicycles, 1 backpack, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2541701582_0a651c380f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3132903412_b4780d0ccf.jpg: 448x640 1 person, 12.1ms\n",
      "Speed: 3.6ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3132903412_b4780d0ccf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/136639119_6040b00946.jpg: 640x480 1 bird, 11.3ms\n",
      "Speed: 2.8ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 136639119_6040b00946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/128912885_8350d277a4.jpg: 640x608 1 dog, 1 frisbee, 1 sports ball, 10.1ms\n",
      "Speed: 3.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 128912885_8350d277a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091916691_b1c96669c6.jpg: 640x448 4 persons, 9.7ms\n",
      "Speed: 2.1ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3091916691_b1c96669c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2169951750_495820a215.jpg: 640x640 1 bird, 1 dog, 9.4ms\n",
      "Speed: 3.0ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2169951750_495820a215.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/542405691_0594b1ce72.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 542405691_0594b1ce72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537474810_cf676b3259.jpg: 448x640 5 persons, 1 sports ball, 1 baseball glove, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3537474810_cf676b3259.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2630507245_bea4804288.jpg: 448x640 1 horse, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2630507245_bea4804288.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214381315_b54f5c9be4.jpg: 448x640 11 persons, 8.3ms\n",
      "Speed: 1.9ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3214381315_b54f5c9be4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1067180831_a59dc64344.jpg: 640x640 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1067180831_a59dc64344.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/311196733_03966b4836.jpg: 352x640 4 persons, 1 car, 10.3ms\n",
      "Speed: 1.7ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 311196733_03966b4836.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/506478284_7cf8bdbe36.jpg: 640x608 5 persons, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 506478284_7cf8bdbe36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2210368267_0615754b48.jpg: 448x640 1 person, 1 dog, 1 couch, 2 potted plants, 1 tv, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2210368267_0615754b48.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921670682_6a77a6c3e9.jpg: 448x640 1 person, 1 bicycle, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2921670682_6a77a6c3e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336682980_1082a66878.jpg: 448x640 3 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3336682980_1082a66878.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1155138244_859fd6e079.jpg: 448x640 13 persons, 4 cups, 1 chair, 2 dining tables, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1155138244_859fd6e079.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2813992915_f732cf8539.jpg: 480x640 13 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2813992915_f732cf8539.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347114_6273736da8.jpg: 448x640 6 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347114_6273736da8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3089842255_359ccf5c40.jpg: 512x640 1 person, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3089842255_359ccf5c40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3546720729_38fff1bbd9.jpg: 640x640 2 persons, 3 remotes, 8.8ms\n",
      "Speed: 3.1ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3546720729_38fff1bbd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3431671749_e8e3a449ac.jpg: 448x640 1 car, 1 bus, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3431671749_e8e3a449ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004287781_c041c09c16.jpg: 544x640 3 persons, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3004287781_c041c09c16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/429270993_294ba8e64c.jpg: 640x448 4 persons, 1 car, 1 backpack, 2 suitcases, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 429270993_294ba8e64c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/839295615_bb9baf2f95.jpg: 480x640 1 person, 2 cars, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 839295615_bb9baf2f95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439560988_f001f96fc9.jpg: 640x512 1 cow, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3439560988_f001f96fc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3388094307_5a83be64a5.jpg: 640x512 1 person, 1 sports ball, 1 tennis racket, 6.3ms\n",
      "Speed: 1.9ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3388094307_5a83be64a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662890367_382eaf83bd.jpg: 480x640 1 person, 3 umbrellas, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2662890367_382eaf83bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3615239961_62b4dbc174.jpg: 640x640 1 person, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3615239961_62b4dbc174.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2257631407_1529b9db39.jpg: 640x480 4 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2257631407_1529b9db39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549614763_42f34f3d1e.jpg: 480x640 4 persons, 2 chairs, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3549614763_42f34f3d1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251088971_f4471048e3.jpg: 640x640 1 person, 10.1ms\n",
      "Speed: 3.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3251088971_f4471048e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/312427606_defa0dfaa8.jpg: 544x640 2 persons, 1 apple, 1 donut, 8.8ms\n",
      "Speed: 2.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 312427606_defa0dfaa8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3208553539_2bf6c6d162.jpg: 416x640 4 persons, 3 handbags, 1 skateboard, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3208553539_2bf6c6d162.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516935867_78cf63c69c.jpg: 448x640 1 person, 1 horse, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3516935867_78cf63c69c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/104136873_5b5d41be75.jpg: 480x640 3 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 104136873_5b5d41be75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2600442766_e750ec9a56.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2600442766_e750ec9a56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2761599088_8b39cc5f41.jpg: 448x640 1 dog, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2761599088_8b39cc5f41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/693164706_9624582e69.jpg: 480x640 2 persons, 1 baseball bat, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 693164706_9624582e69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/451081733_40218cec31.jpg: 480x640 10 persons, 1 suitcase, 1 kite, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 451081733_40218cec31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3721799573_2f470950e0.jpg: 480x640 2 persons, 2 cars, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3721799573_2f470950e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320032226_63390d74a6.jpg: 640x448 1 person, 1 backpack, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3320032226_63390d74a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606942887_1159d92548.jpg: 448x640 1 person, 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3606942887_1159d92548.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371567529_606fa3452b.jpg: 480x640 1 person, 2 surfboards, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3371567529_606fa3452b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2245916742_73af13c733.jpg: 480x640 1 person, 7.9ms\n",
      "Speed: 2.2ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2245916742_73af13c733.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2419591925_1038c6c570.jpg: 640x448 1 person, 3 skateboards, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2419591925_1038c6c570.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3147217787_ed21cd4990.jpg: 640x512 7 persons, 1 chair, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3147217787_ed21cd4990.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432550415_e7b77232de.jpg: 480x640 1 person, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3432550415_e7b77232de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1479513774_70c94cf9d3.jpg: 640x448 1 person, 1 traffic light, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1479513774_70c94cf9d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/111537217_082a4ba060.jpg: 640x448 1 person, 1 elephant, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 111537217_082a4ba060.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/429174232_ddd4ff5e0b.jpg: 512x640 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 429174232_ddd4ff5e0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495340319_705f2e63d6.jpg: 640x608 3 persons, 1 sports ball, 1 baseball glove, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 495340319_705f2e63d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3199460792_deef518c01.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3199460792_deef518c01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831846986_5534425cfa.jpg: 448x640 6 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2831846986_5534425cfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2930580341_d36eec8e3c.jpg: 448x640 6 persons, 1 bench, 1 frisbee, 1 sports ball, 1 potted plant, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2930580341_d36eec8e3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/369047365_35476becc9.jpg: 448x640 2 persons, 1 dog, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 369047365_35476becc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3162045919_c2decbb69b.jpg: 448x640 1 person, 1 surfboard, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3162045919_c2decbb69b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480832276_fa55480ecb.jpg: 640x448 (no detections), 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2480832276_fa55480ecb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2863349041_5eba6e3e21.jpg: 640x448 2 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2863349041_5eba6e3e21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2083778090_3aecaa11cc.jpg: 640x448 1 person, 2 ties, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2083778090_3aecaa11cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708177171_529bb4ff1d.jpg: 512x640 1 person, 1 car, 1 bench, 1 skateboard, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3708177171_529bb4ff1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3635177305_bfbe1fc348.jpg: 480x640 9 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3635177305_bfbe1fc348.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397803103_8a46d716f4.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3397803103_8a46d716f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263233914_d25004e4cd.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 263233914_d25004e4cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2266901263_4324af1f03.jpg: 512x640 6 persons, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2266901263_4324af1f03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3333017828_b930b9d41b.jpg: 512x640 1 dog, 6.7ms\n",
      "Speed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3333017828_b930b9d41b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/532396029_ce125bda3f.jpg: 640x640 1 person, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 532396029_ce125bda3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2244613488_4d1f9edb33.jpg: 352x640 4 persons, 2 bicycles, 2 motorcycles, 8.1ms\n",
      "Speed: 1.3ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2244613488_4d1f9edb33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3552435734_04da83b905.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3552435734_04da83b905.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1042590306_95dea0916c.jpg: 448x640 3 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1042590306_95dea0916c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/743571049_68080e8751.jpg: 640x640 2 persons, 7.1ms\n",
      "Speed: 2.3ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 743571049_68080e8751.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2140305708_591d10b54d.jpg: 640x384 1 person, 15.5ms\n",
      "Speed: 3.4ms preprocess, 15.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2140305708_591d10b54d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/326585030_e1dcca2562.jpg: 448x640 1 person, 1 boat, 11.9ms\n",
      "Speed: 3.0ms preprocess, 11.9ms inference, 3.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 326585030_e1dcca2562.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/101654506_8eb26cfb60.jpg: 576x640 1 dog, 13.7ms\n",
      "Speed: 4.4ms preprocess, 13.7ms inference, 2.0ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 101654506_8eb26cfb60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2908859957_e96c33c1e0.jpg: 640x448 1 person, 1 sports ball, 1 kite, 12.0ms\n",
      "Speed: 2.7ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2908859957_e96c33c1e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1363924449_487f0733df.jpg: 640x416 2 persons, 10.6ms\n",
      "Speed: 2.2ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 1363924449_487f0733df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/250892549_1e06a06a78.jpg: 480x640 2 dogs, 12.3ms\n",
      "Speed: 2.3ms preprocess, 12.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 250892549_1e06a06a78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/441398149_297146e38d.jpg: 448x640 3 persons, 1 stop sign, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 441398149_297146e38d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106562372_e349a27764.jpg: 640x448 2 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3106562372_e349a27764.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708266246_97a033fcc7.jpg: 448x640 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3708266246_97a033fcc7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/571507143_be346225b7.jpg: 448x640 5 persons, 1 cell phone, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 571507143_be346225b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2456615908_59cdac6605.jpg: 448x640 1 bear, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2456615908_59cdac6605.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3304511635_113beaf458.jpg: 640x640 5 persons, 2 chairs, 9.4ms\n",
      "Speed: 3.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3304511635_113beaf458.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420064875_0349a75d69.jpg: 384x640 1 airplane, 1 dog, 1 horse, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3420064875_0349a75d69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640329164_20cb245fd5.jpg: 448x640 2 persons, 4 surfboards, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3640329164_20cb245fd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3600221224_945df01247.jpg: 384x640 1 dog, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3600221224_945df01247.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3555729342_cc7a3b67fd.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3555729342_cc7a3b67fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2587017287_888c811b5a.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2587017287_888c811b5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2422018883_336519b5c6.jpg: 640x480 1 person, 1 surfboard, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2422018883_336519b5c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1167908324_8caab45e15.jpg: 448x640 1 person, 1 chair, 1 tv, 1 laptop, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1167908324_8caab45e15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2054869561_ff723e9eab.jpg: 448x640 1 dog, 1 sports ball, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2054869561_ff723e9eab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1479124077_17dcc0d5d7.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1479124077_17dcc0d5d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3527184455_1a9c074ff2.jpg: 448x640 6 persons, 1 car, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3527184455_1a9c074ff2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540155303_08225a4567.jpg: 640x448 1 person, 9.5ms\n",
      "Speed: 2.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3540155303_08225a4567.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1476002408_4256b7b2fa.jpg: 480x640 2 persons, 1 car, 1 backpack, 13.2ms\n",
      "Speed: 3.6ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1476002408_4256b7b2fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3289817083_4e78e1c05a.jpg: 448x640 4 persons, 1 horse, 11.4ms\n",
      "Speed: 2.6ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3289817083_4e78e1c05a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3405011838_f81117c99f.jpg: 384x640 8 persons, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3405011838_f81117c99f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2973638173_0dc21fd443.jpg: 640x544 1 person, 1 skateboard, 10.7ms\n",
      "Speed: 3.7ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2973638173_0dc21fd443.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500342526_393c739e2f.jpg: 480x640 1 bear, 10.0ms\n",
      "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3500342526_393c739e2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640422448_a0f42e4559.jpg: 640x640 1 person, 9.9ms\n",
      "Speed: 3.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3640422448_a0f42e4559.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3730011701_5352e02286.jpg: 640x448 3 persons, 2 umbrellas, 10.0ms\n",
      "Speed: 2.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3730011701_5352e02286.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3093970461_825b0cac2f.jpg: 416x640 7 persons, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3093970461_825b0cac2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3648097366_706c8a57a1.jpg: 448x640 7 persons, 1 skateboard, 9.9ms\n",
      "Speed: 2.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3648097366_706c8a57a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/352981175_16ff5c07e4.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 352981175_16ff5c07e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/150582765_bad8dec237.jpg: 640x640 2 persons, 2 cars, 9.6ms\n",
      "Speed: 2.9ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 150582765_bad8dec237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3051754615_3d6494c2ae.jpg: 448x640 11 persons, 10.4ms\n",
      "Speed: 2.1ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3051754615_3d6494c2ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1809758121_96026913bb.jpg: 448x640 1 person, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1809758121_96026913bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454199170_ae26917dcd.jpg: 640x576 2 persons, 2 cars, 2 horses, 9.9ms\n",
      "Speed: 2.6ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3454199170_ae26917dcd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2303951441_3c8080907a.jpg: 480x640 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2303951441_3c8080907a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2954525375_9d5ca97341.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2954525375_9d5ca97341.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347150_5ff37818c2.jpg: 448x640 4 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347150_5ff37818c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3278811919_d5a3432af6.jpg: 640x480 3 persons, 10.6ms\n",
      "Speed: 2.3ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3278811919_d5a3432af6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2830880811_d7f66dd2cf.jpg: 608x640 1 dog, 1 frisbee, 9.9ms\n",
      "Speed: 2.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2830880811_d7f66dd2cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3693297007_94512e861e.jpg: 480x640 12 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3693297007_94512e861e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2391269207_d1d2615b1d.jpg: 448x640 1 dog, 1 sports ball, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2391269207_d1d2615b1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576840040_9356b5b10a.jpg: 448x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3576840040_9356b5b10a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2623496164_68ffeb5067.jpg: 416x640 2 persons, 1 bicycle, 1 motorcycle, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2623496164_68ffeb5067.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2249141510_f534708374.jpg: 480x640 1 person, 2 sports balls, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2249141510_f534708374.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3550253365_27d4c303cf.jpg: 480x640 5 persons, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3550253365_27d4c303cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2937178897_ab3d1a941a.jpg: 448x640 4 persons, 1 backpack, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2937178897_ab3d1a941a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495014499_8fd065cfd9.jpg: 640x448 8 persons, 2 cars, 1 bus, 1 traffic light, 7.1ms\n",
      "Speed: 3.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 495014499_8fd065cfd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831578193_3c997ae330.jpg: 384x640 2 persons, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2831578193_3c997ae330.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348811097_0e09baa26f.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3348811097_0e09baa26f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3143765063_a7761b16d3.jpg: 608x640 6 persons, 2 umbrellas, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3143765063_a7761b16d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275627207_0b41e44597.jpg: 640x416 1 person, 1 sports ball, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3275627207_0b41e44597.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2665586311_9a5f4e3fbe.jpg: 640x480 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2665586311_9a5f4e3fbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583293892_c96af8cd98.jpg: 448x640 2 persons, 1 sports ball, 2 baseball gloves, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3583293892_c96af8cd98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3087095548_6df7c2a8ed.jpg: 512x640 7 persons, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3087095548_6df7c2a8ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3138433655_ea1d59e5b7.jpg: 448x640 5 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3138433655_ea1d59e5b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404906655_bc51c69c1e.jpg: 448x640 1 person, 3 dogs, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3404906655_bc51c69c1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2066241589_b80e9f676c.jpg: 640x448 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2066241589_b80e9f676c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1355450069_c0675b0706.jpg: 640x448 1 person, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1355450069_c0675b0706.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217893350_57be430d06.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3217893350_57be430d06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/69189650_6687da7280.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 69189650_6687da7280.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019199755_a984bc21b1.jpg: 448x640 3 persons, 1 car, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3019199755_a984bc21b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2681215810_00b0642f7b.jpg: 640x512 1 person, 8.4ms\n",
      "Speed: 1.8ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2681215810_00b0642f7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2909081008_9a7bfc599a.jpg: 448x640 2 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2909081008_9a7bfc599a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3204525212_d548c7fca7.jpg: 640x640 2 dogs, 1 cup, 1 bowl, 7.1ms\n",
      "Speed: 2.3ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3204525212_d548c7fca7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3277824093_299cbb3138.jpg: 640x448 1 person, 1 skateboard, 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3277824093_299cbb3138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3601533527_6c2439113c.jpg: 640x352 1 person, 10.4ms\n",
      "Speed: 1.3ms preprocess, 10.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3601533527_6c2439113c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/228949397_9e63bfa775.jpg: 640x640 3 persons, 1 bench, 1 surfboard, 7.0ms\n",
      "Speed: 2.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 228949397_9e63bfa775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/504904434_889f426c6e.jpg: 640x608 1 person, 1 bicycle, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 504904434_889f426c6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2857609295_16aaa85293.jpg: 640x480 10 persons, 1 dog, 2 umbrellas, 1 frisbee, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2857609295_16aaa85293.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3284887033_e2e48f1863.jpg: 448x640 1 person, 1 car, 1 dog, 1 frisbee, 1 sports ball, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3284887033_e2e48f1863.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891617125_f939f604c7.jpg: 448x640 1 person, 1 motorcycle, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2891617125_f939f604c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2801146217_03a0b59ccb.jpg: 480x640 2 persons, 1 remote, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2801146217_03a0b59ccb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004823335_9b82cbd8a7.jpg: 480x640 1 bird, 1 dog, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3004823335_9b82cbd8a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512031915_0dd03dcdf9.jpg: 640x480 1 dog, 1 orange, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 512031915_0dd03dcdf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2842609837_b3a0b383f7.jpg: 448x640 13 persons, 2 cups, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2842609837_b3a0b383f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3133403457_95dfe11da1.jpg: 640x544 1 person, 1 chair, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3133403457_95dfe11da1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427118504_93126c83e0.jpg: 448x640 2 persons, 1 backpack, 1 chair, 2 remotes, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3427118504_93126c83e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2774430374_fee1d793e7.jpg: 640x448 1 person, 1 baseball bat, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2774430374_fee1d793e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2966552760_e65b22cd26.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2966552760_e65b22cd26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2588927489_f4da2f11ec.jpg: 640x480 1 person, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2588927489_f4da2f11ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3574930742_9081bd2426.jpg: 448x640 8 persons, 1 cup, 3 chairs, 1 dining table, 2 laptops, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3574930742_9081bd2426.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3717809376_f97611ab84.jpg: 448x640 2 persons, 1 bicycle, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3717809376_f97611ab84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2051194177_fbeee211e3.jpg: 480x640 1 dog, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2051194177_fbeee211e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347700_ef2451d256.jpg: 640x448 1 person, 1 cell phone, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347700_ef2451d256.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2685788323_ceab14534a.jpg: 640x352 1 person, 1 bottle, 2 chairs, 1 tv, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2685788323_ceab14534a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/450596617_ed37ec0fe4.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 450596617_ed37ec0fe4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364861247_d590fa170d.jpg: 448x640 7 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3364861247_d590fa170d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393298349_e659308218.jpg: 448x640 2 persons, 1 dog, 1 sheep, 1 sports ball, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2393298349_e659308218.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3298175192_bbef524ddc.jpg: 640x448 2 dogs, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3298175192_bbef524ddc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3623650392_7b75d4de21.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3623650392_7b75d4de21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2423894412_d952d5d103.jpg: 448x640 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2423894412_d952d5d103.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2559638792_a803ff63d1.jpg: 448x640 3 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2559638792_a803ff63d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2067833088_04e84e5bf2.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2067833088_04e84e5bf2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3447007090_08d997833a.jpg: 448x640 7 persons, 3 cars, 1 truck, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3447007090_08d997833a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695961935_a2a6338f26.jpg: 640x576 1 dog, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2695961935_a2a6338f26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3098707588_5096d20397.jpg: 640x512 2 persons, 2 ties, 2 wine glasss, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3098707588_5096d20397.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3614595423_f9e0ab4fb0.jpg: 640x480 9 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3614595423_f9e0ab4fb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2902844125_4186bf3ab6.jpg: 640x448 1 person, 1 skateboard, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2902844125_4186bf3ab6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1087539207_9f77ab3aaf.jpg: 480x640 3 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1087539207_9f77ab3aaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2950905787_f2017d3e49.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2950905787_f2017d3e49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336374196_f6eaca542f.jpg: 640x448 1 person, 1 skateboard, 8.3ms\n",
      "Speed: 1.5ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3336374196_f6eaca542f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549583146_3e8bb2f7e9.jpg: 448x640 1 person, 1 skateboard, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3549583146_3e8bb2f7e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2441629086_52f68eb316.jpg: 448x640 1 person, 1 cup, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2441629086_52f68eb316.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/369802520_9825f2cd84.jpg: 640x480 1 bear, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 369802520_9825f2cd84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3323514651_3efdbd63ed.jpg: 640x576 21 persons, 1 tie, 1 tennis racket, 1 bottle, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3323514651_3efdbd63ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3346918203_986dca6641.jpg: 448x640 2 persons, 1 bicycle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3346918203_986dca6641.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/448252603_7d928c900e.jpg: 448x640 1 person, 1 frisbee, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 448252603_7d928c900e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/307994435_592f933a6d.jpg: 640x448 3 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 307994435_592f933a6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472634822_7d5d2858c0.jpg: 640x544 1 person, 1 dog, 1 frisbee, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2472634822_7d5d2858c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2823575468_15f6c345fc.jpg: 640x448 3 persons, 1 baseball glove, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2823575468_15f6c345fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3440104178_6871a24e13.jpg: 640x640 1 dog, 7.4ms\n",
      "Speed: 2.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3440104178_6871a24e13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217291172_10ef70af88.jpg: 448x640 5 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3217291172_10ef70af88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1478606153_a7163bf899.jpg: 480x640 1 dog, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1478606153_a7163bf899.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3545779287_8f52e06909.jpg: 448x640 1 dog, 10.9ms\n",
      "Speed: 2.6ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3545779287_8f52e06909.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2728486640_cc2a31d2b0.jpg: 448x640 1 dog, 1 frisbee, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2728486640_cc2a31d2b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3228069008_edb2961fc4.jpg: 480x640 1 dog, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3228069008_edb2961fc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2009636597_e3f4fe19fb.jpg: 480x640 1 person, 9.4ms\n",
      "Speed: 2.8ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2009636597_e3f4fe19fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1475046848_831245fc64.jpg: 640x480 1 person, 13.1ms\n",
      "Speed: 3.2ms preprocess, 13.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1475046848_831245fc64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3412450683_7da035f2de.jpg: 512x640 1 bird, 14.7ms\n",
      "Speed: 4.4ms preprocess, 14.7ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3412450683_7da035f2de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1079274291_9aaf896cc1.jpg: 480x640 3 persons, 11.7ms\n",
      "Speed: 2.6ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1079274291_9aaf896cc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564148252_aa4cb36a32.jpg: 448x640 1 person, 1 motorcycle, 11.2ms\n",
      "Speed: 2.3ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3564148252_aa4cb36a32.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2255338013_566127590b.jpg: 640x448 1 person, 14.0ms\n",
      "Speed: 2.1ms preprocess, 14.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2255338013_566127590b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3481884992_45770ec698.jpg: 448x640 1 person, 1 surfboard, 9.8ms\n",
      "Speed: 2.1ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3481884992_45770ec698.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/431282339_0aa60dd78e.jpg: 640x640 1 person, 1 dog, 10.0ms\n",
      "Speed: 3.6ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 431282339_0aa60dd78e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2290330500_e7bdaa58e1.jpg: 448x640 1 person, 2 cars, 1 sports ball, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2290330500_e7bdaa58e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2849194983_2968c72832.jpg: 640x640 1 person, 9.5ms\n",
      "Speed: 2.9ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2849194983_2968c72832.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561639055_5ac66ae92f.jpg: 640x448 1 person, 3 bicycles, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3561639055_5ac66ae92f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2880051254_e0ca96b6be.jpg: 480x640 1 person, 1 dog, 1 potted plant, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2880051254_e0ca96b6be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3006095077_1992b677f8.jpg: 512x640 2 persons, 1 car, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3006095077_1992b677f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/551664516_78a5131dc4.jpg: 640x416 1 person, 1 handbag, 13.7ms\n",
      "Speed: 3.0ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 551664516_78a5131dc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1095476286_87d4f8664e.jpg: 480x640 2 persons, 1 giraffe, 12.0ms\n",
      "Speed: 2.8ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1095476286_87d4f8664e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335773346_ac0d97efeb.jpg: 640x480 6 persons, 10.8ms\n",
      "Speed: 2.5ms preprocess, 10.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3335773346_ac0d97efeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442978981_53bf1f45f3.jpg: 448x640 2 persons, 10.1ms\n",
      "Speed: 2.3ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3442978981_53bf1f45f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/199463720_329a802206.jpg: 480x640 2 persons, 2 bicycles, 1 bottle, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 199463720_329a802206.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295680663_af21ea648b.jpg: 448x640 1 person, 1 bicycle, 1 train, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3295680663_af21ea648b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/451326127_2d95a2e1c2.jpg: 448x640 1 bird, 1 bear, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 451326127_2d95a2e1c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/179009558_69be522c63.jpg: 640x448 1 person, 1 bicycle, 1 dog, 12.3ms\n",
      "Speed: 2.0ms preprocess, 12.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 179009558_69be522c63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3073579130_7c95d16a7f.jpg: 640x480 4 persons, 1 handbag, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3073579130_7c95d16a7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564543247_05cdbc31cf.jpg: 640x640 3 persons, 8.8ms\n",
      "Speed: 3.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3564543247_05cdbc31cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/55135290_9bed5c4ca3.jpg: 512x640 5 persons, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 55135290_9bed5c4ca3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1299459550_1fd5594fa2.jpg: 448x640 2 persons, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1299459550_1fd5594fa2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3198247669_7493af04a7.jpg: 640x448 1 person, 1 surfboard, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3198247669_7493af04a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2056042552_f59e338533.jpg: 416x640 2 dogs, 9.4ms\n",
      "Speed: 1.9ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2056042552_f59e338533.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497224764_6e17544e0d.jpg: 448x640 6 persons, 1 cell phone, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3497224764_6e17544e0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437034427_6df5e9fbf9.jpg: 448x640 2 persons, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3437034427_6df5e9fbf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2021613437_d99731f986.jpg: 640x448 5 persons, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2021613437_d99731f986.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/270724499_107481c88f.jpg: 448x640 1 dog, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 270724499_107481c88f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2844846111_8c1cbfc75d.jpg: 448x640 10 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2844846111_8c1cbfc75d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2326879311_555ebef188.jpg: 448x640 2 persons, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2326879311_555ebef188.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2071309418_1d7580b0f0.jpg: 640x448 1 bird, 3 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2071309418_1d7580b0f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3602838407_bf13e49243.jpg: 640x448 1 dog, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3602838407_bf13e49243.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3618908551_7fd2de5710.jpg: 448x640 1 person, 1 skateboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3618908551_7fd2de5710.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673032164_6c6843de87.jpg: 448x640 4 persons, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3673032164_6c6843de87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3545652636_0746537307.jpg: 640x448 2 persons, 1 sports ball, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3545652636_0746537307.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2778290592_1910bb0431.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2778290592_1910bb0431.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2307118114_c258e3a47e.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2307118114_c258e3a47e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2519483556_2b1632a18c.jpg: 640x512 3 persons, 2 potted plants, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2519483556_2b1632a18c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585488964_1467945775.jpg: 512x640 2 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3585488964_1467945775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/49553964_cee950f3ba.jpg: 480x640 1 person, 1 surfboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 49553964_cee950f3ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3361411074_83f27d2a1c.jpg: 448x640 1 dog, 1 horse, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3361411074_83f27d2a1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2396746868_0727e06983.jpg: 640x448 2 persons, 2 surfboards, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2396746868_0727e06983.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514222303_cb98584536.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 514222303_cb98584536.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3048380686_732db55281.jpg: 480x640 11 persons, 1 chair, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3048380686_732db55281.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661489896_cc3425777e.jpg: 480x640 5 persons, 1 car, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2661489896_cc3425777e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2316097768_ef662f444b.jpg: 640x480 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2316097768_ef662f444b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/294098577_c10f32bcfa.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 294098577_c10f32bcfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417231408_6ce951c011.jpg: 640x448 1 person, 1 remote, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3417231408_6ce951c011.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/544576742_283b65fa0d.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 544576742_283b65fa0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/23445819_3a458716c1.jpg: 512x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 23445819_3a458716c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016726158_4d15b83b06.jpg: 448x640 3 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3016726158_4d15b83b06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/271177682_48da79ab33.jpg: 480x640 1 car, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 271177682_48da79ab33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1490213660_9ea45550cf.jpg: 640x640 1 dog, 6.9ms\n",
      "Speed: 2.3ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1490213660_9ea45550cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225998968_ef786d86e0.jpg: 480x640 1 person, 2 skiss, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3225998968_ef786d86e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1482960952_95f2d419cb.jpg: 448x640 3 persons, 2 handbags, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1482960952_95f2d419cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2217328285_b1d55c17ca.jpg: 480x640 1 cow, 1 bear, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2217328285_b1d55c17ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/443430496_3fea95a07d.jpg: 448x640 1 bear, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 2.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 443430496_3fea95a07d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1330645772_24f831ff8f.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1330645772_24f831ff8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3131160589_dc73c209b7.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3131160589_dc73c209b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3713177334_32f3245fd8.jpg: 320x640 13 persons, 1 car, 1 umbrella, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3713177334_32f3245fd8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3655155990_b0e201dd3c.jpg: 576x640 2 bears, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3655155990_b0e201dd3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3127142756_bf0bfcb571.jpg: 416x640 1 dog, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3127142756_bf0bfcb571.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582685410_05315a15b8.jpg: 640x512 2 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3582685410_05315a15b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3175712926_5a470d0886.jpg: 448x640 1 bird, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3175712926_5a470d0886.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3691592651_6e4e7f1da9.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3691592651_6e4e7f1da9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1189977786_4f5aaed773.jpg: 448x640 1 bird, 1 dog, 1 sheep, 1 frisbee, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1189977786_4f5aaed773.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3492383096_5bbc08f0da.jpg: 480x640 3 dogs, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3492383096_5bbc08f0da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578068665_87bdacef6a.jpg: 448x640 18 cars, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3578068665_87bdacef6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3224904543_679fe05c41.jpg: 512x640 7 persons, 3 bicycles, 5 cars, 2 motorcycles, 3 traffic lights, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3224904543_679fe05c41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3146630574_05d9ebbed1.jpg: 640x448 1 dog, 1 frisbee, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3146630574_05d9ebbed1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3120953244_b00b152246.jpg: 448x640 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3120953244_b00b152246.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1032122270_ea6f0beedb.jpg: 480x640 1 person, 1 dog, 3 sheeps, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1032122270_ea6f0beedb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134644844_493eec6cdc.jpg: 448x640 1 person, 1 tie, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3134644844_493eec6cdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2612949583_f45b3afe33.jpg: 448x640 1 person, 4 birds, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2612949583_f45b3afe33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3197891333_b1b0fd1702.jpg: 480x640 9 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3197891333_b1b0fd1702.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/368954110_821ccf005c.jpg: 480x640 3 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 368954110_821ccf005c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2070798293_6b9405e04d.jpg: 448x640 1 person, 1 potted plant, 1 tv, 1 laptop, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2070798293_6b9405e04d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258472448_75cfab5e6f.jpg: 640x448 2 persons, 1 suitcase, 1 chair, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3258472448_75cfab5e6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/437917001_ae1106f34e.jpg: 352x640 2 persons, 8.9ms\n",
      "Speed: 1.3ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 437917001_ae1106f34e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349307529_c1a516b9dc.jpg: 640x480 1 person, 1 backpack, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3349307529_c1a516b9dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2395967330_7e6ea404f6.jpg: 640x416 1 person, 1 car, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2395967330_7e6ea404f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390986651_c801db91a0.jpg: 448x640 2 persons, 1 umbrella, 1 kite, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 390986651_c801db91a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2706023395_ac9eba0e42.jpg: 480x640 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2706023395_ac9eba0e42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457210101_3533edebc8.jpg: 512x640 9 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3457210101_3533edebc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1316247213_1d2c726dd5.jpg: 640x480 3 persons, 13.6ms\n",
      "Speed: 3.6ms preprocess, 13.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1316247213_1d2c726dd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2957071266_1b40ec7d96.jpg: 480x640 1 dog, 2 sheeps, 11.6ms\n",
      "Speed: 3.0ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2957071266_1b40ec7d96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393911878_68afe6e6c1.jpg: 640x480 1 person, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2393911878_68afe6e6c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726019124_f302b3d48a.jpg: 480x640 1 person, 2 kites, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3726019124_f302b3d48a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218843713_cf28ea319e.jpg: 480x640 1 person, 1 elephant, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2218843713_cf28ea319e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2021671653_567395c7cf.jpg: 448x640 1 person, 1 bench, 3 potted plants, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2021671653_567395c7cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506869953_802f463178.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3506869953_802f463178.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1464120327_d90279ca3a.jpg: 480x640 1 person, 1 refrigerator, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1464120327_d90279ca3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2885912662_a3a2dfde45.jpg: 480x640 1 person, 2 trucks, 1 sports ball, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2885912662_a3a2dfde45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/836828001_af98d16256.jpg: 608x640 1 person, 9.9ms\n",
      "Speed: 2.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 836828001_af98d16256.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/881725588_efabbcd96a.jpg: 640x480 4 persons, 1 tie, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 881725588_efabbcd96a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3228960484_9aab98b91a.jpg: 640x640 2 persons, 1 backpack, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3228960484_9aab98b91a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/371522748_dc557bcd6c.jpg: 480x640 1 person, 1 umbrella, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 371522748_dc557bcd6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3373870185_f79163fa51.jpg: 448x640 1 bird, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3373870185_f79163fa51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3334057289_68ece38a85.jpg: 416x640 1 dog, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3334057289_68ece38a85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3463922449_f6040a2931.jpg: 448x640 1 person, 1 dog, 1 chair, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3463922449_f6040a2931.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2431470169_0eeba7d602.jpg: 512x640 1 person, 4 chairs, 2 dining tables, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2431470169_0eeba7d602.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242263536_a436f19257.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3242263536_a436f19257.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2647394564_4843800cff.jpg: 384x640 1 person, 3 beds, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2647394564_4843800cff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356901257_83811a19eb.jpg: 640x384 5 persons, 1 dog, 1 umbrella, 10.5ms\n",
      "Speed: 1.8ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3356901257_83811a19eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514297698_0512623955.jpg: 448x640 1 bird, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514297698_0512623955.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2599903773_0f724d8f63.jpg: 576x640 1 person, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2599903773_0f724d8f63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3192311620_99bda27fbd.jpg: 640x448 1 horse, 1 sports ball, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3192311620_99bda27fbd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561537309_e271d57492.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3561537309_e271d57492.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3002920707_5d2e6e6aac.jpg: 576x640 8 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3002920707_5d2e6e6aac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3502343542_f9b46688e5.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3502343542_f9b46688e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3038760935_9a713510eb.jpg: 448x640 2 dogs, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3038760935_9a713510eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3647446816_bd4383c828.jpg: 480x640 3 persons, 1 sports ball, 2 baseball gloves, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3647446816_bd4383c828.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3418504074_083f0bb68d.jpg: 416x640 1 dog, 1 sports ball, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3418504074_083f0bb68d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339319023_5dcc3ef81a.jpg: 640x640 2 persons, 1 sports ball, 1 baseball bat, 1 baseball glove, 7.9ms\n",
      "Speed: 2.5ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3339319023_5dcc3ef81a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3392851587_a638ff25e2.jpg: 448x640 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3392851587_a638ff25e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3052104757_d1cf646935.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3052104757_d1cf646935.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1131800850_89c7ffd477.jpg: 448x640 1 bear, 8.1ms\n",
      "Speed: 1.5ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1131800850_89c7ffd477.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2079152458_40712c3b40.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2079152458_40712c3b40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/116409198_0fe0c94f3b.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 116409198_0fe0c94f3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203908917_53e53c03d1.jpg: 480x640 5 persons, 1 cell phone, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3203908917_53e53c03d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1072153132_53d2bb1b60.jpg: 640x448 1 dog, 1 sports ball, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1072153132_53d2bb1b60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621329299_6fb3f2935c.jpg: 448x640 2 persons, 1 bicycle, 1 motorcycle, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3621329299_6fb3f2935c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2789350645_96a2535b4d.jpg: 512x640 (no detections), 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2789350645_96a2535b4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/977856234_0d9caee7b2.jpg: 352x640 1 dog, 8.4ms\n",
      "Speed: 1.3ms preprocess, 8.4ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 977856234_0d9caee7b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/464116251_1ac4bc91f8.jpg: 640x640 1 dog, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 464116251_1ac4bc91f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2473689180_e9d8fd656a.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2473689180_e9d8fd656a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/846085364_fc9d23df46.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 846085364_fc9d23df46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3610687607_895fdc94bd.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3610687607_895fdc94bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3401437960_7da856e004.jpg: 448x640 1 dog, 1 potted plant, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3401437960_7da856e004.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3394586927_eae7732b64.jpg: 640x480 1 person, 1 skateboard, 9.8ms\n",
      "Speed: 1.8ms preprocess, 9.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3394586927_eae7732b64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676648667_cb055b4fc6.jpg: 480x640 1 person, 8.3ms\n",
      "Speed: 2.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2676648667_cb055b4fc6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3221128704_d1205db79b.jpg: 448x640 4 persons, 1 sports ball, 1 baseball glove, 1 chair, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3221128704_d1205db79b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2354064281_10afa38206.jpg: 416x640 2 dogs, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2354064281_10afa38206.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2591110592_ef5f54f91c.jpg: 640x448 6 persons, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2591110592_ef5f54f91c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2872963574_52ab5182cb.jpg: 448x640 4 persons, 1 tie, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2872963574_52ab5182cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2680619645_ab6645218d.jpg: 640x480 1 person, 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2680619645_ab6645218d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2632381125_de32bdfdf6.jpg: 416x640 2 dogs, 1 frisbee, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2632381125_de32bdfdf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497238310_2abde3965d.jpg: 640x416 4 persons, 8.2ms\n",
      "Speed: 1.6ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3497238310_2abde3965d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390671130_09fdccd52f.jpg: 640x640 1 dog, 1 sports ball, 7.6ms\n",
      "Speed: 3.0ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 390671130_09fdccd52f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2954584849_3c2899f319.jpg: 640x544 13 persons, 1 baseball bat, 2 chairs, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2954584849_3c2899f319.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2057306459_2f52ce648e.jpg: 480x640 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2057306459_2f52ce648e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/799199774_142b1c3bb2.jpg: 448x640 1 person, 9.7ms\n",
      "Speed: 1.6ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 799199774_142b1c3bb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1459250022_bf1eddad11.jpg: 480x640 10 persons, 1 chair, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1459250022_bf1eddad11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1244485675_822e6efe60.jpg: 640x640 3 persons, 7.6ms\n",
      "Speed: 2.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1244485675_822e6efe60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1370615506_2b96105ca3.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1370615506_2b96105ca3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539801139_7258ee437f.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 539801139_7258ee437f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3382303178_69b6d1bdd2.jpg: 448x640 1 person, 1 skateboard, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3382303178_69b6d1bdd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/118309463_a532b75be9.jpg: 448x640 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 118309463_a532b75be9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911552402_5166bc173b.jpg: 640x416 1 person, 1 bicycle, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2911552402_5166bc173b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/948196883_e190a483b1.jpg: 480x640 2 persons, 1 handbag, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 948196883_e190a483b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3723903586_e98d3d8ec7.jpg: 448x640 2 horses, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3723903586_e98d3d8ec7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346105_c1c860db0d.jpg: 448x640 12 persons, 2 baseball bats, 2 baseball gloves, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346105_c1c860db0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315323307_bd148a8964.jpg: 640x480 1 dog, 10.6ms\n",
      "Speed: 2.8ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3315323307_bd148a8964.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1206506157_c7956accd5.jpg: 448x640 1 cow, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1206506157_c7956accd5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3216829599_366a43f05e.jpg: 448x640 1 person, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3216829599_366a43f05e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2992614450_b5a6692239.jpg: 640x448 (no detections), 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 0.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2992614450_b5a6692239.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544793763_b38546a5e8.jpg: 544x640 3 persons, 1 sports ball, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3544793763_b38546a5e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3383491811_fd9d3a891d.jpg: 448x640 1 dog, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3383491811_fd9d3a891d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2280525192_81911f2b00.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2280525192_81911f2b00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3026102616_3cf350af9e.jpg: 640x480 1 cat, 1 dog, 1 refrigerator, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3026102616_3cf350af9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2055646179_169807fed4.jpg: 448x640 5 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2055646179_169807fed4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2497420371_74788d7ba1.jpg: 480x640 1 person, 1 bicycle, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2497420371_74788d7ba1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/556568556_bc5124dc8e.jpg: 640x608 1 person, 1 motorcycle, 9.9ms\n",
      "Speed: 2.9ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 556568556_bc5124dc8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495116214_f1df479fb0.jpg: 480x640 2 dogs, 10.1ms\n",
      "Speed: 2.3ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 495116214_f1df479fb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3044500219_778f9f2b71.jpg: 448x640 7 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3044500219_778f9f2b71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582914739_bef2828a06.jpg: 448x640 10 persons, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3582914739_bef2828a06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2449552677_ee78f01bae.jpg: 512x640 1 person, 1 bicycle, 1 potted plant, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2449552677_ee78f01bae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/27782020_4dab210360.jpg: 480x640 12 persons, 2 bicycles, 2 cars, 1 truck, 4 traffic lights, 1 umbrella, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 3.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 27782020_4dab210360.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3500115252_9404c066a8.jpg: 480x640 2 dogs, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3500115252_9404c066a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2511762757_bd0ab0a017.jpg: 448x640 2 persons, 1 dog, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2511762757_bd0ab0a017.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/306315650_e064f5c677.jpg: 480x640 1 person, 1 fire hydrant, 1 chair, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 306315650_e064f5c677.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3555573680_41c1540a86.jpg: 640x576 1 person, 1 surfboard, 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3555573680_41c1540a86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346971_c100650320.jpg: 448x640 15 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346971_c100650320.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1400424834_1c76e700c4.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1400424834_1c76e700c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2339140905_9f625f140a.jpg: 640x416 1 person, 1 skis, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2339140905_9f625f140a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3674521435_89ff681074.jpg: 448x640 10 persons, 5 bicycles, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3674521435_89ff681074.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016521240_2ef20834b6.jpg: 448x640 3 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3016521240_2ef20834b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3384528359_e920154177.jpg: 512x640 1 bird, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3384528359_e920154177.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3543378438_47e2712486.jpg: 480x640 2 persons, 1 umbrella, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3543378438_47e2712486.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/181415975_2627aa6668.jpg: 640x448 1 person, 1 surfboard, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 181415975_2627aa6668.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296226598_1c892c4351.jpg: 448x640 9 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3296226598_1c892c4351.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/733964952_69f011a6c4.jpg: 480x640 5 persons, 1 handbag, 1 chair, 1 potted plant, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 733964952_69f011a6c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258391809_38fc6211f7.jpg: 448x640 4 persons, 1 car, 1 chair, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3258391809_38fc6211f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3218481970_1fa627b3da.jpg: 640x512 1 person, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3218481970_1fa627b3da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457460673_800d7f7dd9.jpg: 640x448 1 person, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3457460673_800d7f7dd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2322334640_d4d22619ff.jpg: 640x480 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2322334640_d4d22619ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3641456303_c50c33337b.jpg: 448x640 17 persons, 1 car, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3641456303_c50c33337b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2171891283_dedd9cf416.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2171891283_dedd9cf416.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3192266178_f9bf5d3dba.jpg: 448x640 1 person, 1 bench, 2 dogs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 3.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3192266178_f9bf5d3dba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3169777863_d745865784.jpg: 448x640 10 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3169777863_d745865784.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319058642_885d756295.jpg: 640x448 4 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3319058642_885d756295.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520922312_e58a6cfd9c.jpg: 640x448 2 persons, 4 handbags, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3520922312_e58a6cfd9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/766346887_a9a9d0637a.jpg: 544x640 3 persons, 1 boat, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 766346887_a9a9d0637a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2086513494_dbbcb583e7.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2086513494_dbbcb583e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3268407162_6274e0f74f.jpg: 512x640 1 person, 1 sports ball, 1 tennis racket, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3268407162_6274e0f74f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1685990174_09c4fb7df8.jpg: 448x640 12 persons, 1 baseball glove, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1685990174_09c4fb7df8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3250076419_eb3de15063.jpg: 448x640 2 cows, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3250076419_eb3de15063.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2295894587_2fd8faf550.jpg: 448x640 1 bear, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2295894587_2fd8faf550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2993167183_2bda95fa3d.jpg: 512x640 11 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2993167183_2bda95fa3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2087640654_1a84577a44.jpg: 480x640 1 bird, 1 bear, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2087640654_1a84577a44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3475092236_cf45d383c7.jpg: 480x640 14 persons, 1 backpack, 2 cell phones, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3475092236_cf45d383c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/836768303_d748df5546.jpg: 512x640 3 persons, 1 dog, 1 cup, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 836768303_d748df5546.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/114949897_490ca7eaec.jpg: 448x640 3 persons, 1 sports ball, 2 baseball bats, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 114949897_490ca7eaec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1252787177_4b08625897.jpg: 480x640 2 persons, 3 cars, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1252787177_4b08625897.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2650620212_0586016e0d.jpg: 480x640 1 dog, 1 bear, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2650620212_0586016e0d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2675397335_1dcdbd12f5.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2675397335_1dcdbd12f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/53043785_c468d6f931.jpg: 480x640 2 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 53043785_c468d6f931.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425061393_d093edb8da.jpg: 640x512 6 persons, 1 frisbee, 1 tennis racket, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3425061393_d093edb8da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3468275336_61936db92d.jpg: 448x640 6 persons, 1 bicycle, 1 laptop, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3468275336_61936db92d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2062607137_dac194ad02.jpg: 480x640 1 dog, 1 cow, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2062607137_dac194ad02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628059004_5c3529b120.jpg: 640x448 1 person, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3628059004_5c3529b120.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2399551242_c62a46dd5f.jpg: 640x480 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2399551242_c62a46dd5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3358558292_6ab14193ed.jpg: 320x640 5 persons, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3358558292_6ab14193ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3068945309_ff0973e859.jpg: 448x640 7 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3068945309_ff0973e859.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319586526_3994e9cd58.jpg: 448x640 7 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3319586526_3994e9cd58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3587449716_3bf1552c36.jpg: 448x640 6 persons, 1 cell phone, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3587449716_3bf1552c36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1356543628_c13ebe38fb.jpg: 640x416 2 persons, 1 teddy bear, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 1356543628_c13ebe38fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/861608773_bdafd5c996.jpg: 448x640 1 bench, 1 dog, 2 sheeps, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 861608773_bdafd5c996.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2356574282_5078f08b58.jpg: 448x640 4 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2356574282_5078f08b58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/57417274_d55d34e93e.jpg: 480x640 6 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 57417274_d55d34e93e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3665569615_9a71c4b6e4.jpg: 640x448 1 person, 2 baseball gloves, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3665569615_9a71c4b6e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/97406261_5eea044056.jpg: 480x640 1 cat, 1 dog, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 97406261_5eea044056.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220140234_e072856e6c.jpg: 640x512 5 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3220140234_e072856e6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2613889835_6f50a3b83b.jpg: 480x640 2 persons, 2 boats, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2613889835_6f50a3b83b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488356951_b3b77ad832.jpg: 640x640 3 persons, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 488356951_b3b77ad832.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2428959030_bdffc2812e.jpg: 640x448 1 person, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2428959030_bdffc2812e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019842612_8501c1791e.jpg: 608x640 1 person, 1 skateboard, 7.7ms\n",
      "Speed: 2.2ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3019842612_8501c1791e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891162278_fbf96be4f4.jpg: 448x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2891162278_fbf96be4f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556792157_d09d42bef7.jpg: 480x640 8 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3556792157_d09d42bef7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3462165890_c13ce13eff.jpg: 480x640 18 persons, 1 car, 4 backpacks, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3462165890_c13ce13eff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393446245_37dd3f3b59.jpg: 640x640 1 person, 7.0ms\n",
      "Speed: 2.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3393446245_37dd3f3b59.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3088074124_481139dc92.jpg: 640x448 4 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3088074124_481139dc92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/519059913_4906fe4050.jpg: 576x640 1 person, 1 chair, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 519059913_4906fe4050.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487820317_3728e7569e.jpg: 640x416 5 persons, 1 tie, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3487820317_3728e7569e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2247138288_7355861203.jpg: 480x640 8 persons, 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2247138288_7355861203.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/294353408_d459bdaa68.jpg: 480x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 294353408_d459bdaa68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701544312_b2e4e9813d.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3701544312_b2e4e9813d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2649705487_4605e879e9.jpg: 416x640 1 person, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2649705487_4605e879e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1433142189_cda8652603.jpg: 480x640 1 person, 1 backpack, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1433142189_cda8652603.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300550441_f44ec3701a.jpg: 448x640 7 persons, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300550441_f44ec3701a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2447035752_415f4bb152.jpg: 448x640 7 persons, 1 surfboard, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2447035752_415f4bb152.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532593368_be10432e92.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532593368_be10432e92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877424957_9beb1dc49a.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2877424957_9beb1dc49a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/157139628_5dc483e2e4.jpg: 640x512 2 persons, 2 umbrellas, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 157139628_5dc483e2e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/416106657_cab2a107a5.jpg: 480x640 2 sheeps, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 416106657_cab2a107a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2591603141_33d6397e0a.jpg: 480x640 1 dog, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2591603141_33d6397e0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/464527562_a18f095225.jpg: 480x640 2 dogs, 2 frisbees, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 464527562_a18f095225.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2387197355_237f6f41ee.jpg: 480x640 7 persons, 1 tie, 1 tennis racket, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2387197355_237f6f41ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1600208439_e94527b80f.jpg: 448x640 1 dog, 1 horse, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1600208439_e94527b80f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3107558821_f3b205d4ed.jpg: 448x640 2 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3107558821_f3b205d4ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2815788792_d226215d10.jpg: 640x512 2 persons, 1 tennis racket, 9.0ms\n",
      "Speed: 1.8ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2815788792_d226215d10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3031263767_2e3856130e.jpg: 448x640 1 person, 8.9ms\n",
      "Speed: 1.6ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3031263767_2e3856130e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1540631615_8b42c1b160.jpg: 480x640 1 person, 1 sports ball, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1540631615_8b42c1b160.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2585141045_b496a7b7c4.jpg: 480x640 3 persons, 1 bottle, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2585141045_b496a7b7c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/365759754_6cf7068c9a.jpg: 608x640 1 bear, 7.3ms\n",
      "Speed: 2.1ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 365759754_6cf7068c9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621623690_0095e330bc.jpg: 480x640 5 persons, 1 horse, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3621623690_0095e330bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1924234308_c9ddcf206d.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1924234308_c9ddcf206d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596475173_58f11fc583.jpg: 544x640 1 dog, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2596475173_58f11fc583.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3608567609_aae96d4a5e.jpg: 512x640 2 persons, 1 bench, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3608567609_aae96d4a5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425127583_611200619a.jpg: 512x640 2 persons, 1 tennis racket, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3425127583_611200619a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/501520507_c86f805ab8.jpg: 640x416 4 persons, 7.0ms\n",
      "Speed: 1.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 501520507_c86f805ab8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516648762_0cff84ea97.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 516648762_0cff84ea97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2344699642_4fae2f4e07.jpg: 448x640 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2344699642_4fae2f4e07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2074244690_82e30ff44b.jpg: 448x640 12 persons, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2074244690_82e30ff44b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2439031566_2e0c0d3550.jpg: 608x640 3 persons, 1 car, 1 motorcycle, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2439031566_2e0c0d3550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339263085_6db9fd0981.jpg: 480x640 1 dog, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3339263085_6db9fd0981.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2613209320_edf6a2b7e9.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2613209320_edf6a2b7e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346885_f519ece460.jpg: 448x640 7 persons, 1 baseball glove, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346885_f519ece460.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3365198533_539073002b.jpg: 448x640 (no detections), 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3365198533_539073002b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459570613_3932816d3f.jpg: 640x480 3 persons, 1 frisbee, 2 chairs, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3459570613_3932816d3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/944788251_a0bcd4b960.jpg: 480x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 944788251_a0bcd4b960.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2545363449_1985903f82.jpg: 640x384 1 person, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2545363449_1985903f82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241046599_28b0ca7b9f.jpg: 480x640 1 person, 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 241046599_28b0ca7b9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/481887827_f8975dabf1.jpg: 448x640 2 persons, 1 horse, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 481887827_f8975dabf1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1348957576_c4a78eb974.jpg: 480x640 2 persons, 6 boats, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1348957576_c4a78eb974.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1777816180_08d7e8063b.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1777816180_08d7e8063b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2344875609_8e172d682b.jpg: 640x416 1 person, 1 bed, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2344875609_8e172d682b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726170067_094cc1b7e5.jpg: 480x640 1 person, 2 bananas, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3726170067_094cc1b7e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3082474922_9c3533eaf6.jpg: 480x640 1 person, 1 surfboard, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3082474922_9c3533eaf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3495453699_1c9faedf3c.jpg: 640x448 2 persons, 1 clock, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3495453699_1c9faedf3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726700898_c50494b8bd.jpg: 640x448 2 persons, 1 cup, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3726700898_c50494b8bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2927878881_90b42fc444.jpg: 384x640 2 persons, 1 dog, 8.5ms\n",
      "Speed: 1.4ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2927878881_90b42fc444.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/534056823_0752303702.jpg: 480x640 3 persons, 1 elephant, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 534056823_0752303702.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3221036999_3f7b152d8a.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3221036999_3f7b152d8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/565605894_8f0bed0438.jpg: 640x448 1 person, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 565605894_8f0bed0438.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1187593464_ce862352c6.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1187593464_ce862352c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2986280913_13fb2d472e.jpg: 640x480 1 person, 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2986280913_13fb2d472e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2475300106_b8563111ba.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2475300106_b8563111ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3424605029_53078d3505.jpg: 480x640 3 persons, 1 skateboard, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3424605029_53078d3505.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3693961165_9d6c333d5b.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3693961165_9d6c333d5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2694890967_7c7a89de16.jpg: 416x640 4 persons, 2 umbrellas, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2694890967_7c7a89de16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/415118186_64defc96f3.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 415118186_64defc96f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278002800_3817135105.jpg: 480x640 3 persons, 1 sandwich, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278002800_3817135105.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3125309108_1011486589.jpg: 640x480 8 persons, 1 sports ball, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3125309108_1011486589.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/434792818_56375e203f.jpg: 448x640 2 persons, 1 car, 1 airplane, 1 kite, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 434792818_56375e203f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3626964430_cb5c7e5acc.jpg: 480x640 6 persons, 1 car, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3626964430_cb5c7e5acc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2765747519_2b851e01d6.jpg: 448x640 1 person, 1 umbrella, 1 bed, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2765747519_2b851e01d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3105691757_817083b0a6.jpg: 640x448 5 persons, 5 cars, 1 handbag, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3105691757_817083b0a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1215334959_b1970965f7.jpg: 416x640 4 persons, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1215334959_b1970965f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461114418_c27b4043a2.jpg: 640x448 1 person, 1 bicycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3461114418_c27b4043a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/180506881_de0f59770f.jpg: 448x640 2 persons, 1 skateboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 180506881_de0f59770f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3355756569_b430a29c2a.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3355756569_b430a29c2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3641999223_942f8198cc.jpg: 480x640 13 persons, 1 sports ball, 1 baseball bat, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3641999223_942f8198cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3181701312_70a379ab6e.jpg: 640x448 8 persons, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3181701312_70a379ab6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3340857141_85d97a7466.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3340857141_85d97a7466.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264464625_c711cc40c6.jpg: 448x640 3 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3264464625_c711cc40c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596514158_c516e57974.jpg: 640x448 1 person, 1 surfboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2596514158_c516e57974.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503544012_1771be9d3a.jpg: 576x640 4 persons, 1 backpack, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3503544012_1771be9d3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/444845904_a4531c811a.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 444845904_a4531c811a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3063544435_10516c6937.jpg: 448x640 1 person, 1 boat, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3063544435_10516c6937.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2883907436_82bf4a36b8.jpg: 640x416 1 person, 1 dog, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2883907436_82bf4a36b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482088914_e6ea4501e9.jpg: 416x640 1 person, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 482088914_e6ea4501e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432637363_3ba357e2da.jpg: 576x640 8 persons, 5 backpacks, 1 umbrella, 1 cup, 8.2ms\n",
      "Speed: 2.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3432637363_3ba357e2da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/229954612_a4438297ee.jpg: 448x640 6 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 229954612_a4438297ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3217909454_7baa0edbb2.jpg: 640x448 3 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3217909454_7baa0edbb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2303356248_65dd6aba6f.jpg: 640x448 1 person, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2303356248_65dd6aba6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1235681222_819231767a.jpg: 448x640 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1235681222_819231767a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437654963_c4fdc17e8b.jpg: 448x640 2 bears, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3437654963_c4fdc17e8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/70995350_75d0698839.jpg: 448x640 1 airplane, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 70995350_75d0698839.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405534993_5158644f98.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 405534993_5158644f98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497106366_d1a256e723.jpg: 480x640 1 person, 1 surfboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3497106366_d1a256e723.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3346711367_5e7b29e20f.jpg: 640x448 2 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3346711367_5e7b29e20f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/510531976_90bbee22a2.jpg: 640x640 5 persons, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 510531976_90bbee22a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535399240_0714a6e950.jpg: 448x640 8 persons, 1 umbrella, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 535399240_0714a6e950.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155657768_b83a7831e5.jpg: 640x480 6 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3155657768_b83a7831e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3716244806_97d5a1fb61.jpg: 448x640 5 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3716244806_97d5a1fb61.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3371266735_43150bce52.jpg: 544x640 7 persons, 1 skis, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3371266735_43150bce52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2094323311_27d58b1513.jpg: 480x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2094323311_27d58b1513.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2505988632_9541f15583.jpg: 640x480 4 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2505988632_9541f15583.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3332248667_617606714b.jpg: 512x640 6 persons, 1 handbag, 1 remote, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3332248667_617606714b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2282895743_f803f1cf01.jpg: 640x320 2 persons, 1 backpack, 8.9ms\n",
      "Speed: 1.2ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 320)\n",
      "Cropped images saved for 2282895743_f803f1cf01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2050067751_22d2763fd2.jpg: 512x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2050067751_22d2763fd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484649669_7bfe62080b.jpg: 640x640 1 person, 1 carrot, 13.7ms\n",
      "Speed: 5.5ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3484649669_7bfe62080b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2846785268_904c5fcf9f.jpg: 640x448 1 person, 12.8ms\n",
      "Speed: 2.9ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2846785268_904c5fcf9f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397633339_d1ae6d9a0e.jpg: 448x640 1 person, 1 surfboard, 10.5ms\n",
      "Speed: 2.1ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3397633339_d1ae6d9a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3610683688_bbe6d725ed.jpg: 480x640 1 dog, 10.5ms\n",
      "Speed: 2.2ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3610683688_bbe6d725ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3069937639_364fc11e99.jpg: 480x640 1 dog, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3069937639_364fc11e99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3100251515_c68027cc22.jpg: 448x640 8 persons, 1 handbag, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3100251515_c68027cc22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354474353_daf9e168cf.jpg: 448x640 2 dogs, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3354474353_daf9e168cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/293881927_ac62900fd4.jpg: 640x512 1 cow, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 293881927_ac62900fd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582689770_e57ab56671.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3582689770_e57ab56671.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2584957647_4f9235c150.jpg: 448x640 (no detections), 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2584957647_4f9235c150.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/261883591_3f2bca823c.jpg: 448x640 1 person, 1 car, 1 truck, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 261883591_3f2bca823c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520869880_2e8b7d7842.jpg: 640x448 2 persons, 1 umbrella, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3520869880_2e8b7d7842.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2976684095_ce7ccfd423.jpg: 480x640 1 person, 2 cars, 1 stop sign, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2976684095_ce7ccfd423.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2371749487_d80195a2e7.jpg: 448x640 2 persons, 1 truck, 1 sports ball, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2371749487_d80195a2e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354075558_3b67eaa502.jpg: 480x640 1 bear, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3354075558_3b67eaa502.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2589241160_3832440850.jpg: 480x640 2 dogs, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2589241160_3832440850.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/670609997_5c7fdb3f0b.jpg: 640x512 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 670609997_5c7fdb3f0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95151149_5ca6747df6.jpg: 448x640 (no detections), 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 95151149_5ca6747df6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673970325_4e025069e9.jpg: 448x640 7 persons, 6 cups, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3673970325_4e025069e9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3052038928_9f53aa2084.jpg: 448x640 4 persons, 2 couchs, 1 remote, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3052038928_9f53aa2084.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442844140_15aa45e9b8.jpg: 448x640 3 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3442844140_15aa45e9b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467941308_ae6989e29c.jpg: 480x640 1 person, 1 dog, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3467941308_ae6989e29c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2656039837_f46b29af92.jpg: 640x544 1 person, 15 sports balls, 3 tennis rackets, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2656039837_f46b29af92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262849619_0bc4f88ef9.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3262849619_0bc4f88ef9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701291852_373ea46bb6.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3701291852_373ea46bb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189251454_03b76c2e92.jpg: 448x640 1 dog, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3189251454_03b76c2e92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2863180332_372510aa49.jpg: 448x640 5 persons, 1 car, 2 skateboards, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2863180332_372510aa49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480664591_e6d22ed61c.jpg: 384x640 5 persons, 8.6ms\n",
      "Speed: 1.3ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2480664591_e6d22ed61c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2542662402_d781dd7f7c.jpg: 448x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2542662402_d781dd7f7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2562377955_8d670ccec6.jpg: 640x448 2 persons, 2 cars, 1 sports ball, 2 baseball gloves, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2562377955_8d670ccec6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/488196964_49159f11fd.jpg: 640x576 1 dog, 13.2ms\n",
      "Speed: 3.9ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 488196964_49159f11fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3631810528_7233c0f421.jpg: 448x640 3 persons, 1 car, 1 bus, 1 dog, 10.8ms\n",
      "Speed: 2.6ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3631810528_7233c0f421.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1801874841_4c12055e2f.jpg: 480x640 1 person, 1 dog, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1801874841_4c12055e2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384550175_e421d3a871.jpg: 640x448 3 persons, 9.6ms\n",
      "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2384550175_e421d3a871.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2878705136_609dfbf318.jpg: 640x416 1 person, 10.3ms\n",
      "Speed: 2.2ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2878705136_609dfbf318.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3087485737_cb09bc80b6.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3087485737_cb09bc80b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602085456_d1beebcb29.jpg: 448x640 2 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602085456_d1beebcb29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319185571_56162796da.jpg: 448x640 1 dog, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 319185571_56162796da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/261490838_2f3ac98b12.jpg: 448x640 1 bird, 1 bear, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 261490838_2f3ac98b12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319847657_2c40e14113.jpg: 640x448 2 persons, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 319847657_2c40e14113.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3226796100_08c49cfa5c.jpg: 640x480 2 persons, 4 cars, 1 sports ball, 1 baseball bat, 9.5ms\n",
      "Speed: 2.2ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3226796100_08c49cfa5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/640053014_549d2f23d2.jpg: 448x640 5 persons, 2 motorcycles, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 640053014_549d2f23d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2876994989_a4ebbd8491.jpg: 448x640 5 persons, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2876994989_a4ebbd8491.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3381161854_af8b05243c.jpg: 640x448 2 dogs, 12.8ms\n",
      "Speed: 3.3ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3381161854_af8b05243c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1129704496_4a61441f2c.jpg: 448x640 1 dog, 10.7ms\n",
      "Speed: 2.6ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1129704496_4a61441f2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3285993030_87b0f1d202.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3285993030_87b0f1d202.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3691729694_2b97f14c1e.jpg: 640x448 3 persons, 1 bicycle, 2 motorcycles, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3691729694_2b97f14c1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/490044494_d2d546be8d.jpg: 448x640 3 dogs, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 490044494_d2d546be8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3209350613_eb86579ee8.jpg: 640x448 1 person, 1 bench, 1 skateboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3209350613_eb86579ee8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3513265399_a32e8cfd18.jpg: 480x640 5 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3513265399_a32e8cfd18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3009035153_715e39b440.jpg: 448x640 1 person, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3009035153_715e39b440.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106782647_b078830a9e.jpg: 640x448 1 person, 1 bicycle, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3106782647_b078830a9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/248994078_a9257f448b.jpg: 480x640 1 dog, 2 frisbees, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 248994078_a9257f448b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1659396176_ced00a549f.jpg: 448x640 2 dogs, 1 cow, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1659396176_ced00a549f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3048597471_5697538daf.jpg: 448x640 1 dog, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3048597471_5697538daf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/369244499_752f0c1018.jpg: 448x640 6 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 369244499_752f0c1018.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2824922268_3fafb64683.jpg: 448x640 1 dog, 1 bear, 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2824922268_3fafb64683.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3318995586_c2bc50b92e.jpg: 480x640 3 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3318995586_c2bc50b92e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/373219198_149af371d9.jpg: 640x640 1 car, 1 dog, 1 cow, 10.1ms\n",
      "Speed: 3.0ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 373219198_149af371d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726168984_1fa2c8965b.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3726168984_1fa2c8965b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3695949492_27ca3892fd.jpg: 480x640 1 dog, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3695949492_27ca3892fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240094420_a9eea11d39.jpg: 448x640 4 persons, 1 bottle, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3240094420_a9eea11d39.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2098418613_85a0c9afea.jpg: 480x640 1 car, 1 bench, 1 dog, 3 potted plants, 1 toilet, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2098418613_85a0c9afea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3120648767_812c72eabe.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3120648767_812c72eabe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613955682_3860e116cf.jpg: 448x640 12 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3613955682_3860e116cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2870426310_4d59795032.jpg: 480x640 3 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2870426310_4d59795032.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3338291921_fe7ae0c8f8.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3338291921_fe7ae0c8f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3423802527_94bd2b23b0.jpg: 448x640 11 persons, 2 handbags, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3423802527_94bd2b23b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286823363_7d554ea740.jpg: 448x640 1 person, 2 chairs, 1 couch, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2286823363_7d554ea740.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3480379024_545e8ec818.jpg: 448x640 2 persons, 2 motorcycles, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3480379024_545e8ec818.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1473080948_bae2925dc8.jpg: 480x640 1 person, 2 surfboards, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1473080948_bae2925dc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/242109387_e497277e07.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 242109387_e497277e07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2207244634_1db1a1890b.jpg: 448x640 6 persons, 3 skateboards, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2207244634_1db1a1890b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329777647_5e1fd503ac.jpg: 480x640 1 car, 2 dogs, 1 horse, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3329777647_5e1fd503ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3432656291_a6c7981f6e.jpg: 576x640 7 persons, 2 trucks, 1 handbag, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3432656291_a6c7981f6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2432061076_0955d52854.jpg: 416x640 1 dog, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2432061076_0955d52854.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272967004_1531726d71.jpg: 416x640 1 person, 6.3ms\n",
      "Speed: 1.4ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2272967004_1531726d71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1107246521_d16a476380.jpg: 448x640 1 bird, 2 dogs, 1 cow, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1107246521_d16a476380.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/245895500_a4eb97af02.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 245895500_a4eb97af02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2402462857_7684848704.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2402462857_7684848704.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3659769138_d907fd9647.jpg: 640x640 (no detections), 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3659769138_d907fd9647.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1121416483_c7902d0d49.jpg: 640x480 1 person, 1 bed, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1121416483_c7902d0d49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3003011417_79b49ff384.jpg: 640x448 7 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3003011417_79b49ff384.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/314940358_ec1958dc1d.jpg: 640x448 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 314940358_ec1958dc1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415809168_af9dabdba5.jpg: 448x640 2 dogs, 1 frisbee, 1 sports ball, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415809168_af9dabdba5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/72964268_d532bb8ec7.jpg: 448x640 1 person, 2 surfboards, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 72964268_d532bb8ec7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3529721084_4b405baf54.jpg: 448x640 3 persons, 3 cars, 2 motorcycles, 1 truck, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3529721084_4b405baf54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2234702530_a265a4df22.jpg: 640x576 1 person, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2234702530_a265a4df22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681414069_71ba164f71.jpg: 480x640 1 cat, 1 sheep, 1 cow, 2 bowls, 8.6ms\n",
      "Speed: 2.4ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3681414069_71ba164f71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540757246_5a849fbdcb.jpg: 480x640 5 persons, 1 frisbee, 6.8ms\n",
      "Speed: 1.9ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2540757246_5a849fbdcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/448257345_ce149c2ea6.jpg: 640x480 1 person, 1 dog, 1 fork, 1 dining table, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 448257345_ce149c2ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263231469_e85c74f5fd.jpg: 640x480 2 persons, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 263231469_e85c74f5fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/248646530_03c6284759.jpg: 480x640 2 persons, 3 surfboards, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 248646530_03c6284759.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2742426734_291df6da08.jpg: 480x640 7 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2742426734_291df6da08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2504277798_936a09c74d.jpg: 448x640 1 person, 1 cell phone, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2504277798_936a09c74d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380537190_11d6c0a412.jpg: 448x640 11 persons, 1 truck, 1 handbag, 1 apple, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 380537190_11d6c0a412.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3050114829_18bc5a6d7c.jpg: 224x640 1 person, 8.9ms\n",
      "Speed: 0.9ms preprocess, 8.9ms inference, 1.1ms postprocess per image at shape (1, 3, 224, 640)\n",
      "Cropped images saved for 3050114829_18bc5a6d7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/756521713_5d3da56a54.jpg: 448x640 1 person, 1 dog, 1 frisbee, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 756521713_5d3da56a54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/265223847_636ba039c1.jpg: 640x416 2 traffic lights, 1 kite, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 265223847_636ba039c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2116316160_d5fa7abdc3.jpg: 640x480 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2116316160_d5fa7abdc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019609769_c7809177f6.jpg: 448x640 8 persons, 1 handbag, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3019609769_c7809177f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1481062342_d9e34366c4.jpg: 640x448 1 person, 1 chair, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1481062342_d9e34366c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3178599352_c57fdebcd2.jpg: 640x480 2 persons, 2 couchs, 1 toilet, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3178599352_c57fdebcd2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3342272425_804316cb3d.jpg: 640x448 4 persons, 4 cars, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3342272425_804316cb3d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3027009366_c8362521e8.jpg: 448x640 9 persons, 1 sports ball, 1 tennis racket, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3027009366_c8362521e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3657209354_cde9bbd2c5.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3657209354_cde9bbd2c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154641421_d1b9b8c24c.jpg: 640x448 10 persons, 2 baseball gloves, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3154641421_d1b9b8c24c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2389107995_ec756f3514.jpg: 480x640 (no detections), 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2389107995_ec756f3514.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2233426944_0959835ced.jpg: 640x640 1 dog, 1 sheep, 3 cows, 1 sports ball, 7.2ms\n",
      "Speed: 2.3ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2233426944_0959835ced.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2075321027_c8fcbaf581.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2075321027_c8fcbaf581.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3725202807_12fbfdd207.jpg: 480x640 3 persons, 4 cars, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3725202807_12fbfdd207.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3127614086_9f1d3cf73d.jpg: 640x512 6 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3127614086_9f1d3cf73d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/211981411_e88b8043c2.jpg: 640x480 1 truck, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 211981411_e88b8043c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681324243_b69fa90842.jpg: 480x640 6 persons, 1 baseball glove, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3681324243_b69fa90842.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/661755629_602ea4cf09.jpg: 480x640 1 dog, 1 sports ball, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 661755629_602ea4cf09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3449846784_278bc1ba92.jpg: 640x384 1 dog, 1 sports ball, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3449846784_278bc1ba92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501386648_e11e3f3152.jpg: 480x640 13 persons, 1 handbag, 3 bananas, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3501386648_e11e3f3152.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887750774_920eb171aa.jpg: 480x640 1 dog, 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2887750774_920eb171aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2684322797_85406f571d.jpg: 480x640 1 person, 1 bottle, 1 chair, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2684322797_85406f571d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/172097783_292c5413d8.jpg: 640x608 1 person, 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 172097783_292c5413d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1479028910_3dab3448c8.jpg: 480x640 1 person, 1 skateboard, 1 potted plant, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1479028910_3dab3448c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1994416869_4dd769a806.jpg: 480x640 2 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1994416869_4dd769a806.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3554210976_fbd0ef33a3.jpg: 448x640 1 person, 9.5ms\n",
      "Speed: 1.6ms preprocess, 9.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3554210976_fbd0ef33a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3653837067_94050699ec.jpg: 640x480 1 person, 1 horse, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3653837067_94050699ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211316116_a2462e327d.jpg: 480x640 1 person, 1 surfboard, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3211316116_a2462e327d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/717673249_ac998cfbe6.jpg: 448x640 2 dogs, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 717673249_ac998cfbe6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1550772959_9ca9fa625f.jpg: 480x640 1 person, 1 bench, 1 surfboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1550772959_9ca9fa625f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154528397_89112faf4b.jpg: 640x640 1 person, 1 skis, 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3154528397_89112faf4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2921094201_2ed70a7963.jpg: 480x640 9 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2921094201_2ed70a7963.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/336551615_a01418bc53.jpg: 416x640 4 persons, 2 backpacks, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 336551615_a01418bc53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3738789925_7d17dbdf25.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3738789925_7d17dbdf25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2953015871_cae796b6e7.jpg: 448x640 1 cow, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2953015871_cae796b6e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539761097_5c6c70425b.jpg: 640x448 3 persons, 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 539761097_5c6c70425b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672057606_cb6393dbd9.jpg: 640x448 1 person, 1 train, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3672057606_cb6393dbd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/233361142_d9d5f0cae9.jpg: 640x480 1 dog, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 233361142_d9d5f0cae9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2547785434_f227bd3680.jpg: 512x640 1 dog, 2 sheeps, 1 cow, 8.5ms\n",
      "Speed: 1.7ms preprocess, 8.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2547785434_f227bd3680.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426789838_8771f0ed56.jpg: 448x640 2 persons, 3 chairs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3426789838_8771f0ed56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2757803246_8aa3499d26.jpg: 448x640 8 persons, 2 chairs, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2757803246_8aa3499d26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3686078365_9e655e238f.jpg: 640x384 8 persons, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3686078365_9e655e238f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1514957266_a19827c538.jpg: 544x640 1 dog, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1514957266_a19827c538.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3386060324_b98fdfa449.jpg: 640x448 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3386060324_b98fdfa449.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3684562647_28dc325522.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3684562647_28dc325522.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356494271_6103d0b556.jpg: 448x640 2 persons, 2 dogs, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3356494271_6103d0b556.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3475111806_f0d2927707.jpg: 512x640 17 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3475111806_f0d2927707.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/724702877_f2a938766b.jpg: 640x448 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 724702877_f2a938766b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3193511842_82549c21fb.jpg: 640x448 10 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3193511842_82549c21fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263946591_a1558b77d3.jpg: 640x448 1 person, 1 motorcycle, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3263946591_a1558b77d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2480850054_de3433b54a.jpg: 480x640 1 person, 1 sheep, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2480850054_de3433b54a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2925737498_57585a7ed9.jpg: 640x512 2 persons, 2 cars, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2925737498_57585a7ed9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3292016893_24d14c8b4f.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3292016893_24d14c8b4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3161044966_27bf6f9dec.jpg: 448x640 1 cat, 1 dog, 1 couch, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3161044966_27bf6f9dec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2956562716_5aa3f6ef38.jpg: 512x640 5 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2956562716_5aa3f6ef38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569406219_f37ebf7b92.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3569406219_f37ebf7b92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1355945307_f9e01a9a05.jpg: 448x640 1 person, 2 trucks, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1355945307_f9e01a9a05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534548254_7bee952a0e.jpg: 640x448 3 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3534548254_7bee952a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410562803_56ec09f41c.jpg: 480x640 3 persons, 2 benchs, 1 laptop, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2410562803_56ec09f41c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/745563422_f4fa7d9157.jpg: 448x640 10 persons, 4 bottles, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 745563422_f4fa7d9157.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3232470286_903a61ea16.jpg: 448x640 16 persons, 1 chair, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3232470286_903a61ea16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2975807155_5a8610c297.jpg: 640x448 8 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2975807155_5a8610c297.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576312396_799c873f3e.jpg: 448x640 13 persons, 4 boats, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3576312396_799c873f3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3338474677_7376e426c2.jpg: 512x640 1 person, 1 bicycle, 3 traffic lights, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3338474677_7376e426c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887614578_ed7ba21775.jpg: 352x640 2 dogs, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2887614578_ed7ba21775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/417577408_eb571658c1.jpg: 640x640 1 dog, 7.0ms\n",
      "Speed: 2.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 417577408_eb571658c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091177347_58c85c1c3b.jpg: 480x640 3 persons, 2 umbrellas, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3091177347_58c85c1c3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3413669228_ec64efeb34.jpg: 640x448 1 person, 1 skateboard, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3413669228_ec64efeb34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2433175169_da939372f2.jpg: 640x448 1 person, 1 fire hydrant, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2433175169_da939372f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246041128_bedb09ed74.jpg: 640x384 3 persons, 2 cups, 1 potted plant, 2 dining tables, 7.9ms\n",
      "Speed: 1.4ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 246041128_bedb09ed74.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487419819_e3f89444ce.jpg: 448x640 1 bird, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3487419819_e3f89444ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987195421_e830c59fb6.jpg: 512x640 4 persons, 1 baseball bat, 1 baseball glove, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2987195421_e830c59fb6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/816084977_21c1811c9a.jpg: 448x640 1 person, 1 horse, 1 cow, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 816084977_21c1811c9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2465497494_43d74df57c.jpg: 544x640 1 person, 1 frisbee, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2465497494_43d74df57c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3209620285_edfc479392.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3209620285_edfc479392.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207654194_43d6bebd68.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3207654194_43d6bebd68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019986034_7453777274.jpg: 640x512 3 persons, 1 car, 1 baseball bat, 1 vase, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3019986034_7453777274.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3210705660_2b14b7fb36.jpg: 448x640 3 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3210705660_2b14b7fb36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1149179852_acad4d7300.jpg: 416x640 1 person, 1 horse, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1149179852_acad4d7300.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3153067758_53f003b1df.jpg: 640x416 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3153067758_53f003b1df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/515702827_be3c6ce857.jpg: 448x640 3 persons, 1 surfboard, 12.4ms\n",
      "Speed: 3.3ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 515702827_be3c6ce857.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3050606344_af711c726c.jpg: 640x480 1 person, 10.4ms\n",
      "Speed: 2.7ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3050606344_af711c726c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046949818_245b05f507.jpg: 448x640 11 persons, 1 car, 1 baseball glove, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3046949818_245b05f507.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2985439112_8a3b77d5c9.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2985439112_8a3b77d5c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2314722788_6262c3aa40.jpg: 416x640 2 dogs, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2314722788_6262c3aa40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183060123_ea3af6278b.jpg: 512x640 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3183060123_ea3af6278b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2800758232_d7fa598065.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2800758232_d7fa598065.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1351764581_4d4fb1b40f.jpg: 448x640 4 persons, 1 truck, 9.8ms\n",
      "Speed: 2.0ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1351764581_4d4fb1b40f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3265527323_6431f00692.jpg: 448x640 1 bird, 1 bear, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3265527323_6431f00692.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2307451605_22e9c06530.jpg: 480x640 1 bear, 1 donut, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2307451605_22e9c06530.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3594822096_e1144b85d6.jpg: 448x640 1 person, 1 bicycle, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3594822096_e1144b85d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2677614492_792023b928.jpg: 640x480 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2677614492_792023b928.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1501297480_8db52c15b0.jpg: 640x448 3 persons, 1 potted plant, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1501297480_8db52c15b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1781227288_6811e734be.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1781227288_6811e734be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/350443876_c9769f5734.jpg: 640x480 2 persons, 1 chair, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 350443876_c9769f5734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2788652511_4f10060e07.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2788652511_4f10060e07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2515247156_c1b759fc33.jpg: 640x480 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2515247156_c1b759fc33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122579598_14841c038a.jpg: 480x640 2 persons, 1 dog, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3122579598_14841c038a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628043835_9d9bd595a7.jpg: 448x640 3 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3628043835_9d9bd595a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2171576939_d1e72daab2.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2171576939_d1e72daab2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2898810636_84fb5c0b63.jpg: 640x448 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2898810636_84fb5c0b63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/413231421_43833a11f5.jpg: 640x480 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 413231421_43833a11f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2066048248_f53f5ef5e2.jpg: 480x640 4 persons, 5 horses, 2 cows, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2066048248_f53f5ef5e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2881441125_b580e3dd4b.jpg: 640x480 2 persons, 1 bicycle, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2881441125_b580e3dd4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3101378069_f8caa14c0a.jpg: 352x640 1 person, 2 boats, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3101378069_f8caa14c0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/442594271_2c3dd38483.jpg: 480x640 13 persons, 1 umbrella, 1 kite, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 442594271_2c3dd38483.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1420060020_7a6984e2ea.jpg: 640x352 1 person, 9.1ms\n",
      "Speed: 1.4ms preprocess, 9.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 1420060020_7a6984e2ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3206999917_e682672cbc.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3206999917_e682672cbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1308472581_9961782889.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1308472581_9961782889.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3440160917_4524cfd9f6.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3440160917_4524cfd9f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/310260324_7f941814bc.jpg: 480x640 6 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 310260324_7f941814bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/415793623_6c1225ae27.jpg: 640x480 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 415793623_6c1225ae27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2729147877_c3ec3445bf.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2729147877_c3ec3445bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1271960365_e54033f883.jpg: 640x640 2 dogs, 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1271960365_e54033f883.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2760715910_87c7bdeb87.jpg: 640x448 5 persons, 1 backpack, 1 umbrella, 1 handbag, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2760715910_87c7bdeb87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3699763582_f28c5130dd.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3699763582_f28c5130dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/481827288_a688be7913.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 481827288_a688be7913.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/514073775_56796be990.jpg: 480x640 7 persons, 1 truck, 1 fire hydrant, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 514073775_56796be990.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/426191845_1e979e9345.jpg: 480x640 1 dog, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 426191845_1e979e9345.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3332467180_d72f9b067d.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3332467180_d72f9b067d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263216826_acf868049c.jpg: 480x640 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 263216826_acf868049c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3687995245_624b54090d.jpg: 640x448 3 persons, 1 chair, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3687995245_624b54090d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3644941648_012ec87848.jpg: 640x448 1 bird, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3644941648_012ec87848.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2444741900_5cb3ef3e1d.jpg: 416x640 16 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2444741900_5cb3ef3e1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3655964639_21e76383d0.jpg: 640x384 3 persons, 1 bicycle, 1 traffic light, 1 backpack, 1 handbag, 1 suitcase, 8.0ms\n",
      "Speed: 1.3ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3655964639_21e76383d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295024992_887a95c700.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3295024992_887a95c700.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025549604_38b86198f5.jpg: 512x640 6 persons, 1 chair, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3025549604_38b86198f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486135177_772628d034.jpg: 640x544 1 person, 2 snowboards, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3486135177_772628d034.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099264059_21653e2536.jpg: 448x640 1 person, 2 skateboards, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3099264059_21653e2536.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2983555530_a89f1f5ed7.jpg: 640x512 3 persons, 2 elephants, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2983555530_a89f1f5ed7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2915538325_59e11276dd.jpg: 544x640 8 persons, 1 bicycle, 1 kite, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2915538325_59e11276dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241374292_11e3198daa.jpg: 480x640 22 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 241374292_11e3198daa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2478929971_9eb6c074b6.jpg: 352x640 6 persons, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2478929971_9eb6c074b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3306464579_1b16a0caf2.jpg: 608x640 2 bowls, 7.8ms\n",
      "Speed: 2.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3306464579_1b16a0caf2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1075716537_62105738b4.jpg: 640x448 1 person, 1 bicycle, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1075716537_62105738b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589156060_3ed8d6bbc3.jpg: 448x640 1 person, 1 bear, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3589156060_3ed8d6bbc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2363100645_c3423a0433.jpg: 480x640 1 person, 1 bed, 1 vase, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2363100645_c3423a0433.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1248734482_3038218f3b.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1248734482_3038218f3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/297285273_688e44c014.jpg: 448x640 3 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 297285273_688e44c014.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3572267708_9d8a81d4a4.jpg: 512x640 3 persons, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3572267708_9d8a81d4a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/436009777_440c7679a1.jpg: 640x608 3 persons, 8 cars, 1 truck, 1 stop sign, 1 backpack, 1 handbag, 1 suitcase, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 436009777_440c7679a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/824782868_a8f532f3a6.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 824782868_a8f532f3a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3523874798_9ba2fa46e3.jpg: 640x448 1 person, 1 skateboard, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3523874798_9ba2fa46e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/520491467_54cbc0a866.jpg: 640x640 2 persons, 7.5ms\n",
      "Speed: 2.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 520491467_54cbc0a866.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293018193_e4e0c8db7c.jpg: 640x448 2 persons, 1 handbag, 1 suitcase, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3293018193_e4e0c8db7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2886533440_dfa832f2fa.jpg: 640x448 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2886533440_dfa832f2fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3663951804_868982f3f9.jpg: 448x640 4 persons, 1 skateboard, 12.8ms\n",
      "Speed: 3.1ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3663951804_868982f3f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/308307853_5a51fbdecc.jpg: 448x640 1 dog, 10.4ms\n",
      "Speed: 2.6ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 308307853_5a51fbdecc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2059616165_b7c99c1009.jpg: 640x512 1 dog, 10.1ms\n",
      "Speed: 2.5ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2059616165_b7c99c1009.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3668900592_a84b0c07db.jpg: 448x640 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3668900592_a84b0c07db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3550255426_4ab03c0d6e.jpg: 640x448 11 persons, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3550255426_4ab03c0d6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2097403787_77a154f5b9.jpg: 640x480 1 person, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2097403787_77a154f5b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2195887578_3ba2f29b48.jpg: 448x640 3 persons, 1 bus, 1 train, 1 tie, 1 cup, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2195887578_3ba2f29b48.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393926562_66cc01b001.jpg: 640x480 1 person, 1 car, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3393926562_66cc01b001.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330102093_1d6e35e78d.jpg: 640x448 2 persons, 1 bicycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3330102093_1d6e35e78d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2061354254_faa5bd294b.jpg: 448x640 4 persons, 1 frisbee, 1 baseball glove, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2061354254_faa5bd294b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3687996279_05b5a2a706.jpg: 448x640 7 persons, 4 umbrellas, 1 tie, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3687996279_05b5a2a706.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2788628994_61123c03d2.jpg: 448x640 1 bird, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2788628994_61123c03d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/707972553_36816e53a2.jpg: 448x640 1 person, 1 frisbee, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 707972553_36816e53a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/757046028_ff5999f91b.jpg: 640x448 10 persons, 1 surfboard, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 757046028_ff5999f91b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2939007933_8a6ef2d073.jpg: 416x640 1 person, 1 motorcycle, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2939007933_8a6ef2d073.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3280644151_3d89cb1e0e.jpg: 448x640 1 person, 12.7ms\n",
      "Speed: 3.0ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3280644151_3d89cb1e0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2974587819_742fb7c338.jpg: 448x640 8 persons, 10.6ms\n",
      "Speed: 2.6ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2974587819_742fb7c338.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346402952_e47d0065b6.jpg: 416x640 4 persons, 1 car, 1 truck, 13.4ms\n",
      "Speed: 3.1ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2346402952_e47d0065b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/757332692_6866ae545c.jpg: 448x640 1 person, 1 dog, 11.8ms\n",
      "Speed: 2.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 757332692_6866ae545c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541141771_67d305c873.jpg: 640x448 2 persons, 1 suitcase, 10.2ms\n",
      "Speed: 2.2ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3541141771_67d305c873.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2490687446_9d46fdf5a9.jpg: 512x640 1 bear, 9.7ms\n",
      "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2490687446_9d46fdf5a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1425013325_bff69bc9da.jpg: 480x640 2 dogs, 1 cow, 9.7ms\n",
      "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1425013325_bff69bc9da.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887103049_a867e74358.jpg: 640x416 3 persons, 1 skateboard, 2 bowls, 9.4ms\n",
      "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2887103049_a867e74358.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/495054019_3dee8a02f5.jpg: 512x640 4 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 495054019_3dee8a02f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2866820467_ae699235a7.jpg: 640x512 2 persons, 1 umbrella, 9.5ms\n",
      "Speed: 2.5ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2866820467_ae699235a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3043904009_3b5e0412cd.jpg: 480x640 2 persons, 1 cup, 1 tv, 1 laptop, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3043904009_3b5e0412cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/997722733_0cb5439472.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 997722733_0cb5439472.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2255266906_8222af18b9.jpg: 480x640 1 bird, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2255266906_8222af18b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2931254547_e97c6d0d63.jpg: 288x640 17 persons, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2931254547_e97c6d0d63.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1414820925_3504c394e1.jpg: 640x352 1 person, 1 handbag, 10.2ms\n",
      "Speed: 1.8ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 1414820925_3504c394e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2539933563_17ff0758c7.jpg: 480x640 15 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2539933563_17ff0758c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2884301336_dc8e974431.jpg: 640x640 1 dog, 9.5ms\n",
      "Speed: 3.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2884301336_dc8e974431.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/942399470_6132d3e5d2.jpg: 640x448 3 persons, 1 horse, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 942399470_6132d3e5d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3435035138_af32890a4c.jpg: 448x640 1 bird, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3435035138_af32890a4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924259848_effb4dcb82.jpg: 512x640 2 persons, 1 motorcycle, 9.4ms\n",
      "Speed: 2.3ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2924259848_effb4dcb82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278388986_78ba84eb8f.jpg: 480x640 2 persons, 2 backpacks, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278388986_78ba84eb8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3154813159_58a195236d.jpg: 640x480 1 person, 1 dog, 1 dining table, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3154813159_58a195236d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2404692474_37da774368.jpg: 448x640 1 dog, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2404692474_37da774368.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2687672606_275169c35d.jpg: 448x640 1 person, 1 book, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2687672606_275169c35d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1517807181_ca6588f2a0.jpg: 640x480 2 persons, 1 kite, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1517807181_ca6588f2a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518126579_e70e0cbb2b.jpg: 640x448 2 persons, 2 skateboards, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3518126579_e70e0cbb2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2688102742_885e578a3f.jpg: 640x448 1 person, 2 handbags, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2688102742_885e578a3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2768972186_92787cd523.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2768972186_92787cd523.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2837908308_8bc25c6b02.jpg: 544x640 1 person, 1 dog, 1 cow, 1 umbrella, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2837908308_8bc25c6b02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3239480519_22540b5016.jpg: 640x448 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3239480519_22540b5016.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3090600019_8808fe7a9d.jpg: 640x576 2 persons, 3 cars, 2 handbags, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3090600019_8808fe7a9d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1057251835_6ded4ada9c.jpg: 640x640 1 dog, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1057251835_6ded4ada9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3156406419_38fbd52007.jpg: 448x640 1 person, 1 skis, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3156406419_38fbd52007.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697378565_7060d9281a.jpg: 480x640 20 persons, 1 frisbee, 1 tv, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3697378565_7060d9281a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275065565_9e2a640fbc.jpg: 480x640 1 cat, 1 dog, 1 potted plant, 1 bed, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3275065565_9e2a640fbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2554570943_122da6438f.jpg: 480x640 1 person, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2554570943_122da6438f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176277818_235486a3cd.jpg: 384x640 11 persons, 1 bottle, 1 teddy bear, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3176277818_235486a3cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2194286203_5dc620006a.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2194286203_5dc620006a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/127450902_533ceeddfc.jpg: 480x640 6 persons, 1 frisbee, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 127450902_533ceeddfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3205839744_24504ba179.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3205839744_24504ba179.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345770_9f8aa6723c.jpg: 448x640 7 persons, 1 baseball glove, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345770_9f8aa6723c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3741827382_71e93298d0.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3741827382_71e93298d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1439282131_3814d6ae04.jpg: 480x640 1 dog, 3 bears, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1439282131_3814d6ae04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/218342358_1755a9cce1.jpg: 480x640 1 person, 1 tennis racket, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 218342358_1755a9cce1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2735290454_1bd8bc5eac.jpg: 640x352 1 dog, 1 frisbee, 8.0ms\n",
      "Speed: 1.2ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2735290454_1bd8bc5eac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1286408831_05282582ed.jpg: 480x640 1 person, 1 bench, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1286408831_05282582ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091594712_2166604334.jpg: 640x448 4 persons, 2 chairs, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3091594712_2166604334.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1500853305_0150615ce9.jpg: 640x448 2 persons, 1 bench, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1500853305_0150615ce9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016651969_746bd36e68.jpg: 640x448 6 persons, 1 bicycle, 1 traffic light, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3016651969_746bd36e68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3220126881_b0a4f7cccb.jpg: 448x640 4 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3220126881_b0a4f7cccb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025334206_76888792e5.jpg: 640x480 2 dogs, 1 cow, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3025334206_76888792e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/127488876_f2d2a89588.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 127488876_f2d2a89588.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3090957866_f1b2b7f214.jpg: 640x416 2 persons, 3 cars, 1 handbag, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3090957866_f1b2b7f214.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3218889785_86cb64014f.jpg: 448x640 3 persons, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3218889785_86cb64014f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1616016569_673de1d678.jpg: 480x640 3 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1616016569_673de1d678.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3286045254_696c6b15bd.jpg: 640x512 4 persons, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3286045254_696c6b15bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/446286714_dcec7f339e.jpg: 416x640 3 persons, 6 cars, 1 bench, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 446286714_dcec7f339e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3146355833_1b7fc80210.jpg: 640x480 6 persons, 2 backpacks, 1 handbag, 1 banana, 7.9ms\n",
      "Speed: 1.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3146355833_1b7fc80210.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934801096_230ae78d7e.jpg: 480x640 2 persons, 2 traffic lights, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2934801096_230ae78d7e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2275029674_6d4891c20e.jpg: 640x640 2 dogs, 7.5ms\n",
      "Speed: 2.5ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2275029674_6d4891c20e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543007912_23fc735b99.jpg: 640x448 2 persons, 12.3ms\n",
      "Speed: 3.1ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 543007912_23fc735b99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1003163366_44323f5815.jpg: 544x640 1 person, 2 benchs, 1 dog, 10.5ms\n",
      "Speed: 3.0ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1003163366_44323f5815.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3281580623_8c3ba0fdb2.jpg: 448x640 2 dogs, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3281580623_8c3ba0fdb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3424424006_98f9d1921c.jpg: 640x448 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3424424006_98f9d1921c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2540884723_03d60ef548.jpg: 480x640 3 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2540884723_03d60ef548.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2030781555_b7ff7be28f.jpg: 576x640 8 persons, 1 bicycle, 10.4ms\n",
      "Speed: 3.6ms preprocess, 10.4ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2030781555_b7ff7be28f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2283966256_70317e1759.jpg: 480x640 1 dog, 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2283966256_70317e1759.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091962081_194f2f3bd4.jpg: 640x480 6 persons, 10.0ms\n",
      "Speed: 2.2ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3091962081_194f2f3bd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2584412512_6767593f24.jpg: 480x640 2 dogs, 1 sheep, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2584412512_6767593f24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569667295_6e51db08ef.jpg: 512x640 1 dog, 9.9ms\n",
      "Speed: 2.3ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3569667295_6e51db08ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3066491113_86569e15be.jpg: 448x640 1 dog, 1 frisbee, 9.8ms\n",
      "Speed: 2.0ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3066491113_86569e15be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/945509052_740bb19bc3.jpg: 640x640 1 person, 1 umbrella, 1 kite, 9.8ms\n",
      "Speed: 2.8ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 945509052_740bb19bc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359551687_68f2f0212a.jpg: 544x640 15 persons, 1 skateboard, 9.7ms\n",
      "Speed: 2.5ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3359551687_68f2f0212a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3565749152_7924d15b04.jpg: 512x640 2 cars, 1 truck, 1 dog, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3565749152_7924d15b04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2869253972_aa72df6bf3.jpg: 512x640 2 persons, 8.2ms\n",
      "Speed: 2.3ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2869253972_aa72df6bf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/42637987_866635edf6.jpg: 480x640 10 persons, 2 benchs, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 42637987_866635edf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2676649969_482caed129.jpg: 480x640 1 person, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2676649969_482caed129.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247052319_da8aba1983.jpg: 480x640 1 person, 1 motorcycle, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3247052319_da8aba1983.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2938072630_d641b63e4d.jpg: 448x640 3 persons, 1 suitcase, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2938072630_d641b63e4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192026581_b782d1355a.jpg: 448x640 5 persons, 1 bird, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2192026581_b782d1355a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3490186050_4cb4193d4d.jpg: 448x640 1 person, 3 cars, 1 sports ball, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3490186050_4cb4193d4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1262583859_653f1469a9.jpg: 640x448 1 person, 1 bicycle, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1262583859_653f1469a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3183875944_b2be694e06.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3183875944_b2be694e06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139895886_5a6d495b13.jpg: 448x640 1 cat, 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3139895886_5a6d495b13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/522700240_d9af45e60d.jpg: 448x640 1 dog, 1 bed, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 522700240_d9af45e60d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532412342_e0a004b404.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532412342_e0a004b404.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2439154641_bbf985aa57.jpg: 448x640 1 person, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2439154641_bbf985aa57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/186890605_ddff5b694e.jpg: 480x640 1 person, 1 bird, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 186890605_ddff5b694e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3506607642_40037b3fbf.jpg: 416x640 4 persons, 1 sports ball, 1 baseball glove, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3506607642_40037b3fbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2187503678_fd743e0a00.jpg: 384x640 1 bird, 1 sheep, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2187503678_fd743e0a00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1801188148_a176954965.jpg: 448x640 2 dogs, 1 sheep, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1801188148_a176954965.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2916586390_664f0139ea.jpg: 640x416 7 persons, 7.5ms\n",
      "Speed: 1.4ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2916586390_664f0139ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638577494_fe55f7b4cb.jpg: 448x640 1 bird, 3 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3638577494_fe55f7b4cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899622876_b673b04967.jpg: 448x640 4 persons, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2899622876_b673b04967.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3027850131_a7772e0ba0.jpg: 448x640 2 persons, 1 surfboard, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3027850131_a7772e0ba0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163198309_bbfe504f0a.jpg: 448x640 8 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3163198309_bbfe504f0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1193116658_c0161c35b5.jpg: 640x640 12 persons, 2 bicycles, 1 skateboard, 7.7ms\n",
      "Speed: 2.5ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1193116658_c0161c35b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2514612680_b0d2d77099.jpg: 448x640 3 persons, 1 handbag, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2514612680_b0d2d77099.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2606433181_f8f9d38579.jpg: 384x640 2 dogs, 8.3ms\n",
      "Speed: 1.4ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2606433181_f8f9d38579.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3093971101_543237971d.jpg: 416x640 1 person, 1 surfboard, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3093971101_543237971d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2495394666_2ef6c37519.jpg: 416x640 3 elephants, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2495394666_2ef6c37519.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3111897772_5211a37a02.jpg: 640x448 7 persons, 1 handbag, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3111897772_5211a37a02.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3554634863_5f6f616639.jpg: 448x640 16 persons, 2 chairs, 1 laptop, 1 book, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3554634863_5f6f616639.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2052702658_da1204f6d1.jpg: 448x640 1 person, 1 car, 1 dog, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2052702658_da1204f6d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/311619377_2ba3b36675.jpg: 640x608 3 persons, 1 stop sign, 7.8ms\n",
      "Speed: 2.2ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 311619377_2ba3b36675.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084034954_fe5737197d.jpg: 640x448 3 persons, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3084034954_fe5737197d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244774022_a12c07afdb.jpg: 384x640 1 person, 1 surfboard, 8.6ms\n",
      "Speed: 1.5ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 244774022_a12c07afdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3424927725_c4d1fcfac3.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3424927725_c4d1fcfac3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2589156742_c46bc82137.jpg: 640x448 1 person, 1 cup, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2589156742_c46bc82137.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260162669_c79a900afb.jpg: 640x608 1 dog, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 260162669_c79a900afb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3630991662_990f397f7c.jpg: 448x640 1 person, 1 sports ball, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3630991662_990f397f7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3066338314_2c3fb731d1.jpg: 640x480 2 persons, 1 bowl, 1 dining table, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3066338314_2c3fb731d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584561689_b6eb24dd70.jpg: 640x448 4 persons, 1 frisbee, 1 sports ball, 1 kite, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3584561689_b6eb24dd70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345533_99c731403a.jpg: 640x448 3 persons, 1 baseball glove, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241345533_99c731403a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/414568315_5adcfc23c0.jpg: 640x448 1 person, 1 bowl, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 414568315_5adcfc23c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564738125_10400f69c0.jpg: 640x512 4 persons, 2 backpacks, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3564738125_10400f69c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1082252566_8c79beef93.jpg: 448x640 1 bicycle, 2 dogs, 2 horses, 1 chair, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1082252566_8c79beef93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862481071_86c65d46fa.jpg: 448x640 1 bus, 3 trucks, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2862481071_86c65d46fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3446762868_06e9d9d899.jpg: 480x640 1 person, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3446762868_06e9d9d899.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/108898978_7713be88fc.jpg: 480x640 3 persons, 1 backpack, 1 skis, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 108898978_7713be88fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3099504809_565e17e49d.jpg: 448x640 1 person, 1 cell phone, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3099504809_565e17e49d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356748019_2251399314.jpg: 448x640 13 persons, 1 skateboard, 6.2ms\n",
      "Speed: 1.5ms preprocess, 6.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3356748019_2251399314.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/649596742_5ba84ce946.jpg: 448x640 2 dogs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 649596742_5ba84ce946.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667908724_65c7d112f2.jpg: 640x352 1 person, 9.0ms\n",
      "Speed: 1.4ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3667908724_65c7d112f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1643915227_9f48068772.jpg: 480x640 10 persons, 3 cars, 1 bus, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1643915227_9f48068772.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3255737244_1f8948fc07.jpg: 640x416 7 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3255737244_1f8948fc07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1472053993_bed67a3ba7.jpg: 480x640 1 person, 1 skis, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1472053993_bed67a3ba7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3086810882_94036f4475.jpg: 640x640 14 birds, 1 dog, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3086810882_94036f4475.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503623999_bbd5dcfb18.jpg: 608x640 3 persons, 8.1ms\n",
      "Speed: 2.3ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3503623999_bbd5dcfb18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2844018783_524b08e5aa.jpg: 480x640 3 persons, 1 chair, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2844018783_524b08e5aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2682382530_f9f8fd1e89.jpg: 512x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2682382530_f9f8fd1e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1304961697_76b86b0c18.jpg: 640x512 2 persons, 2 surfboards, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1304961697_76b86b0c18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2251447809_2de73afcdf.jpg: 448x640 1 person, 1 car, 1 bus, 1 truck, 3 traffic lights, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2251447809_2de73afcdf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3132619510_7dfc947d25.jpg: 480x640 1 dog, 1 sheep, 1 cow, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3132619510_7dfc947d25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3423249426_02bedf9260.jpg: 448x640 4 persons, 1 backpack, 1 handbag, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3423249426_02bedf9260.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/255266148_7ba7df1a88.jpg: 576x640 1 dog, 1 frisbee, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 255266148_7ba7df1a88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296150666_aae2f64348.jpg: 448x640 1 boat, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3296150666_aae2f64348.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3079341641_f65f6b0f8b.jpg: 640x448 4 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3079341641_f65f6b0f8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2191329761_3effd856c5.jpg: 448x640 1 dog, 2 horses, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2191329761_3effd856c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1795151944_d69b82f942.jpg: 448x640 1 person, 5 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1795151944_d69b82f942.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2868324804_5cc8030484.jpg: 480x640 3 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2868324804_5cc8030484.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2458269558_277012780d.jpg: 448x640 1 person, 1 handbag, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2458269558_277012780d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482974845_db4f16befa.jpg: 448x640 2 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3482974845_db4f16befa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501781809_88429e3b83.jpg: 448x640 10 persons, 1 motorcycle, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3501781809_88429e3b83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1053804096_ad278b25f1.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1053804096_ad278b25f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/522652105_a89f1cf260.jpg: 640x480 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 522652105_a89f1cf260.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347441_d3dd9b129f.jpg: 448x640 2 persons, 1 baseball glove, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347441_d3dd9b129f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259225196_750c4ce0f9.jpg: 448x640 3 persons, 1 car, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259225196_750c4ce0f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3050976633_9c25cf6fa0.jpg: 480x640 5 persons, 2 sports balls, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3050976633_9c25cf6fa0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1398606571_f543f7698a.jpg: 640x640 1 person, 7.1ms\n",
      "Speed: 2.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1398606571_f543f7698a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3442138291_3e75f4bdb8.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3442138291_3e75f4bdb8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3186073578_6e115f45f5.jpg: 448x640 6 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3186073578_6e115f45f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3466891862_9afde75568.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3466891862_9afde75568.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710353645_8fbfaa4175.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3710353645_8fbfaa4175.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/437404867_209625774d.jpg: 640x576 6 persons, 1 skateboard, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 437404867_209625774d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3178005751_fca19815ac.jpg: 640x480 2 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3178005751_fca19815ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3354063643_1d8814eb13.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3354063643_1d8814eb13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2877637572_641cd29901.jpg: 480x640 1 dog, 1 frisbee, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2877637572_641cd29901.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1096395242_fc69f0ae5a.jpg: 480x640 1 person, 1 backpack, 1 wine glass, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1096395242_fc69f0ae5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530504007_3272c57e21.jpg: 448x640 6 persons, 1 tie, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3530504007_3272c57e21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3149804151_1cc8d10783.jpg: 448x640 2 persons, 2 motorcycles, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3149804151_1cc8d10783.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2417341107_97dbab9c5e.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2417341107_97dbab9c5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2232518012_8cb0bbc43b.jpg: 448x640 1 person, 1 dog, 1 horse, 3 cows, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2232518012_8cb0bbc43b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/470887781_faae5dae83.jpg: 512x640 9 persons, 1 traffic light, 1 umbrella, 1 clock, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 470887781_faae5dae83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2797511323_bf20acab45.jpg: 448x640 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2797511323_bf20acab45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430100177_5864bf1e73.jpg: 640x640 14 persons, 7.0ms\n",
      "Speed: 2.3ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3430100177_5864bf1e73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3619630328_2d0865b6f4.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3619630328_2d0865b6f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766325714_189bbff388.jpg: 448x640 7 persons, 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2766325714_189bbff388.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3698944019_825ef54f2f.jpg: 640x416 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3698944019_825ef54f2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2075493556_b763648389.jpg: 448x640 3 persons, 1 cell phone, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2075493556_b763648389.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2250555512_71670078f5.jpg: 384x640 2 persons, 1 cell phone, 8.1ms\n",
      "Speed: 1.4ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2250555512_71670078f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562302012_0cbcd01ff9.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3562302012_0cbcd01ff9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345639_1556a883b1.jpg: 448x640 8 persons, 1 baseball glove, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345639_1556a883b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3159092624_66af4e207e.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3159092624_66af4e207e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/327997381_55f90dc834.jpg: 640x640 2 persons, 1 bottle, 2 chairs, 1 tv, 2 laptops, 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 327997381_55f90dc834.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414384480_096867d695.jpg: 448x640 9 persons, 1 backpack, 2 handbags, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2414384480_096867d695.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2613920405_e91e6ebd7a.jpg: 512x640 1 person, 1 cow, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2613920405_e91e6ebd7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2396669903_5217a83641.jpg: 448x640 1 person, 1 parking meter, 2 handbags, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2396669903_5217a83641.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258397351_1a70f1993d.jpg: 448x640 18 persons, 4 chairs, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3258397351_1a70f1993d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752809449_632cd991b3.jpg: 640x448 3 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2752809449_632cd991b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1368383637_614646cc4a.jpg: 640x448 3 persons, 3 bicycles, 1 backpack, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1368383637_614646cc4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3043908909_bb54d2c08e.jpg: 576x640 9 persons, 1 backpack, 1 chair, 1 tv, 4 laptops, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3043908909_bb54d2c08e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2451988767_244bff98d1.jpg: 640x576 2 persons, 7.0ms\n",
      "Speed: 2.0ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2451988767_244bff98d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3400135828_0ac128b6eb.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3400135828_0ac128b6eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236476706_175081ce18.jpg: 640x640 6 persons, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 236476706_175081ce18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2533642917_a5eace85e6.jpg: 640x352 1 person, 8.8ms\n",
      "Speed: 1.3ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2533642917_a5eace85e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3404012438_9baf8dcbaf.jpg: 448x640 1 bird, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3404012438_9baf8dcbaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1865794069_6e3a1e57bb.jpg: 512x640 5 persons, 1 sports ball, 1 chair, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1865794069_6e3a1e57bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3139160252_75109e9e05.jpg: 448x640 3 persons, 1 couch, 1 potted plant, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3139160252_75109e9e05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3660361818_e05367693f.jpg: 384x640 2 persons, 2 traffic lights, 7.8ms\n",
      "Speed: 1.3ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3660361818_e05367693f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2883950737_3b67d24af4.jpg: 640x512 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2883950737_3b67d24af4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2826647354_650ff5eb03.jpg: 448x640 2 persons, 2 motorcycles, 1 bottle, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2826647354_650ff5eb03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2939197393_93dc64c4bb.jpg: 416x640 1 person, 1 motorcycle, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2939197393_93dc64c4bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2431832075_00aa1a4457.jpg: 640x512 1 person, 1 bed, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2431832075_00aa1a4457.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3489774350_a94e6c7bfc.jpg: 640x480 1 person, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3489774350_a94e6c7bfc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3676460610_8c52e8a355.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3676460610_8c52e8a355.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1923476156_e20976b32d.jpg: 448x640 1 person, 1 bicycle, 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1923476156_e20976b32d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1436760519_8d6101a0ed.jpg: 640x544 1 person, 1 dog, 1 frisbee, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1436760519_8d6101a0ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694219419_a7a83d4886.jpg: 480x640 11 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3694219419_a7a83d4886.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/442918418_0f29c97fa9.jpg: 640x480 1 dog, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 442918418_0f29c97fa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/905355838_3a43fdfd4e.jpg: 640x416 1 person, 1 baseball glove, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 905355838_3a43fdfd4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/542269487_5d77b363eb.jpg: 416x640 3 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 542269487_5d77b363eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3717845800_ab45e255b8.jpg: 448x640 6 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3717845800_ab45e255b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/180094434_b0f244832d.jpg: 480x640 8 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 180094434_b0f244832d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2708582445_5e5999b956.jpg: 480x640 1 person, 1 boat, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2708582445_5e5999b956.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3542418447_7c337360d6.jpg: 640x512 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3542418447_7c337360d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3050650135_23f9d9d2f8.jpg: 512x640 2 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3050650135_23f9d9d2f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3384742888_85230c34d5.jpg: 640x640 1 dog, 7.4ms\n",
      "Speed: 2.4ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3384742888_85230c34d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3047144646_2252ff8e04.jpg: 448x640 5 persons, 1 car, 2 dogs, 1 cow, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3047144646_2252ff8e04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1220401002_3f44b1f3f7.jpg: 448x640 2 persons, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1220401002_3f44b1f3f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424379231_23f1ade134.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 424379231_23f1ade134.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3172283002_3c0fc624de.jpg: 512x640 6 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3172283002_3c0fc624de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3444982197_0ff15cc50b.jpg: 448x640 3 persons, 1 bottle, 1 cup, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3444982197_0ff15cc50b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2072574835_febf0c5fb9.jpg: 480x640 1 cow, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2072574835_febf0c5fb9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2871962580_b85ce502ba.jpg: 640x448 6 persons, 1 baseball glove, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2871962580_b85ce502ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2411824767_4eb1fae823.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2411824767_4eb1fae823.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2471447879_6554cefb16.jpg: 640x384 2 persons, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2471447879_6554cefb16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256958382_b9006bfc5b.jpg: 608x640 1 person, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 256958382_b9006bfc5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/334737767_7f344eee16.jpg: 448x640 1 dog, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 334737767_7f344eee16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1368338041_6b4077ca98.jpg: 480x640 3 dogs, 11.1ms\n",
      "Speed: 2.8ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1368338041_6b4077ca98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2587696611_db0378710f.jpg: 448x640 1 person, 10.1ms\n",
      "Speed: 2.4ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2587696611_db0378710f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/952171414_2db16f846f.jpg: 448x640 4 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 952171414_2db16f846f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/493109089_468e105233.jpg: 448x640 1 person, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 493109089_468e105233.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2665904080_8a3b9639d5.jpg: 640x416 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2665904080_8a3b9639d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95734036_bef6d1a871.jpg: 448x640 1 person, 1 bicycle, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 95734036_bef6d1a871.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3005123298_d3017d5f5d.jpg: 640x512 1 person, 1 car, 1 bottle, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3005123298_d3017d5f5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2456030728_d3d147e774.jpg: 448x640 1 person, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2456030728_d3d147e774.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393394134_0caac47e1c.jpg: 448x640 1 dog, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3393394134_0caac47e1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/288025239_5e59ba9c3b.jpg: 448x640 1 person, 1 kite, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 288025239_5e59ba9c3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2710416789_555180e399.jpg: 480x640 2 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2710416789_555180e399.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126724531_f483e1b92a.jpg: 640x640 1 person, 8.9ms\n",
      "Speed: 3.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3126724531_f483e1b92a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271495320_bca47795fb.jpg: 640x544 6 persons, 1 sports ball, 8.9ms\n",
      "Speed: 2.4ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3271495320_bca47795fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2827964381_408a310809.jpg: 480x640 1 bird, 1 cat, 2 dogs, 12.4ms\n",
      "Speed: 3.4ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2827964381_408a310809.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207343907_995f7ac1d2.jpg: 448x640 17 persons, 10.2ms\n",
      "Speed: 2.5ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3207343907_995f7ac1d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2810412010_f8b3bc1207.jpg: 448x640 (no detections), 8.2ms\n",
      "Speed: 2.1ms preprocess, 8.2ms inference, 0.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2810412010_f8b3bc1207.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247423890_163f00a2cb.jpg: 608x640 1 person, 5 sports balls, 8.8ms\n",
      "Speed: 2.8ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3247423890_163f00a2cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3684518763_f3490b647a.jpg: 640x448 8 persons, 1 umbrella, 12.8ms\n",
      "Speed: 3.2ms preprocess, 12.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3684518763_f3490b647a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2102030040_2e8f4738f7.jpg: 448x640 1 person, 12.9ms\n",
      "Speed: 3.3ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2102030040_2e8f4738f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2528521798_fb689eba8d.jpg: 448x640 1 airplane, 10.3ms\n",
      "Speed: 2.5ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2528521798_fb689eba8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/802594049_289e3c8420.jpg: 416x640 1 person, 10.0ms\n",
      "Speed: 2.1ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 802594049_289e3c8420.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3586239953_da4fb3f775.jpg: 480x640 2 persons, 1 frisbee, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3586239953_da4fb3f775.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/775664534_3f6de7d413.jpg: 448x640 3 persons, 1 frisbee, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 775664534_3f6de7d413.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3667492609_97f88b373f.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3667492609_97f88b373f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3744832122_2f4febdff6.jpg: 640x448 1 person, 1 baseball glove, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3744832122_2f4febdff6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2464259416_238ef13a2e.jpg: 480x640 2 dogs, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2464259416_238ef13a2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3545586120_283d728a97.jpg: 448x640 4 persons, 2 sports balls, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3545586120_283d728a97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2130986011_47cb05c8c9.jpg: 448x640 1 person, 1 snowboard, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2130986011_47cb05c8c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2984704498_29b53df5df.jpg: 640x640 1 person, 9.2ms\n",
      "Speed: 2.9ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2984704498_29b53df5df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3611603026_9112b0c53f.jpg: 640x448 1 person, 4 handbags, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3611603026_9112b0c53f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612485611_12dd7742f7.jpg: 448x640 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3612485611_12dd7742f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2158267555_db1d94e468.jpg: 384x640 9 persons, 1 horse, 2 umbrellas, 4 chairs, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2158267555_db1d94e468.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2427490900_5b7a8874b9.jpg: 640x512 8 persons, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2427490900_5b7a8874b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2269961438_cae7a9c725.jpg: 384x640 1 person, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2269961438_cae7a9c725.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/757133580_ba974ef649.jpg: 640x448 1 person, 2 baseball bats, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 757133580_ba974ef649.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3374223949_90776ba934.jpg: 640x480 2 persons, 2 beds, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3374223949_90776ba934.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2854291706_d4c31dbf56.jpg: 416x640 4 persons, 2 cars, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2854291706_d4c31dbf56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/306318683_5f1f875191.jpg: 480x640 1 person, 1 kite, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 306318683_5f1f875191.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3035785330_2fd5e32bb1.jpg: 480x640 1 person, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3035785330_2fd5e32bb1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887798667_ce761d45e8.jpg: 448x640 1 bench, 1 book, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2887798667_ce761d45e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3592992234_6d3fe58a70.jpg: 480x640 1 person, 1 baseball bat, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3592992234_6d3fe58a70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2900048238_74bd69d87d.jpg: 480x640 13 persons, 4 umbrellas, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2900048238_74bd69d87d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429581486_4556471d1a.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3429581486_4556471d1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427614912_b147d083b2.jpg: 640x512 2 persons, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3427614912_b147d083b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2245348304_08bc5642f1.jpg: 640x448 2 persons, 1 potted plant, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2245348304_08bc5642f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1117972841_2b9261f95f.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1117972841_2b9261f95f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223709894_97824ba76f.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3223709894_97824ba76f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2381102729_12fc4d4c76.jpg: 640x480 2 dogs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2381102729_12fc4d4c76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/333031366_a0828c540d.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 333031366_a0828c540d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/481054596_cad8c02103.jpg: 544x640 9 persons, 1 car, 1 truck, 1 traffic light, 1 handbag, 1 potted plant, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 481054596_cad8c02103.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3038941104_17ee91fc03.jpg: 608x640 1 person, 1 skateboard, 7.9ms\n",
      "Speed: 2.6ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3038941104_17ee91fc03.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3559993787_c49644dcc5.jpg: 640x448 1 person, 1 bicycle, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3559993787_c49644dcc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3576060775_d9121519cc.jpg: 480x640 1 person, 1 dog, 1 cow, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3576060775_d9121519cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/436393371_822ee70952.jpg: 640x608 1 horse, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 436393371_822ee70952.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3042488474_0d2ec81eb8.jpg: 640x512 10 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3042488474_0d2ec81eb8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/426805536_d1d5e68c17.jpg: 448x640 1 person, 1 backpack, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 426805536_d1d5e68c17.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3111402233_6285bcba7a.jpg: 448x640 1 horse, 1 sports ball, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3111402233_6285bcba7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3370085095_6abbb67c1d.jpg: 480x640 3 persons, 1 teddy bear, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3370085095_6abbb67c1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2878190821_6e4e03dc5f.jpg: 640x448 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2878190821_6e4e03dc5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/341430859_4519802e8f.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 341430859_4519802e8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3582465732_78f77f34ae.jpg: 448x640 1 person, 1 tennis racket, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3582465732_78f77f34ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3289433994_4c67aab384.jpg: 640x448 2 dogs, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3289433994_4c67aab384.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2726262796_03bd63a155.jpg: 512x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2726262796_03bd63a155.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2135360514_7dcb9ed796.jpg: 480x640 1 person, 1 couch, 1 dining table, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2135360514_7dcb9ed796.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477315700_52a4d740a5.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3477315700_52a4d740a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3621717946_d96f8a6012.jpg: 480x640 2 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3621717946_d96f8a6012.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537400880_8f410d747d.jpg: 640x480 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3537400880_8f410d747d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394003437_184a838aa9.jpg: 640x480 13 persons, 2 cars, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2394003437_184a838aa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2671602981_4edde92658.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2671602981_4edde92658.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410153942_ba4a136358.jpg: 640x576 4 persons, 1 boat, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2410153942_ba4a136358.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3235542079_2fcf4951a1.jpg: 640x448 1 person, 1 snowboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3235542079_2fcf4951a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1675679141_36c9bc2969.jpg: 384x640 1 person, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1675679141_36c9bc2969.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2981372647_2061278c60.jpg: 448x640 1 bird, 2 dogs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2981372647_2061278c60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3407317539_68765a3375.jpg: 640x480 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3407317539_68765a3375.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3423509305_e399d005db.jpg: 640x608 1 horse, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3423509305_e399d005db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1460352062_d64fb633e0.jpg: 640x608 1 person, 6.6ms\n",
      "Speed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 1460352062_d64fb633e0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/561179890_af8e31cb2e.jpg: 640x480 1 person, 2 backpacks, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 561179890_af8e31cb2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2864340145_d28b842faf.jpg: 448x640 1 bear, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2864340145_d28b842faf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3728015645_b43a60258b.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3728015645_b43a60258b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2842032768_9d9ce04385.jpg: 480x640 2 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2842032768_9d9ce04385.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259222690_69737f2a6e.jpg: 480x640 2 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3259222690_69737f2a6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3673035152_da7ed916d9.jpg: 448x640 1 person, 1 bed, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3673035152_da7ed916d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3604496023_c1f546423e.jpg: 416x640 5 persons, 1 handbag, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3604496023_c1f546423e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2663248626_f000f2661d.jpg: 480x640 2 dogs, 1 horse, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2663248626_f000f2661d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/497791037_93499238d8.jpg: 576x640 1 person, 8.0ms\n",
      "Speed: 2.3ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 497791037_93499238d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/368393384_86defdcde8.jpg: 448x640 2 persons, 1 surfboard, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 368393384_86defdcde8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259228898_cefd04580b.jpg: 448x640 18 persons, 3 cars, 2 chairs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259228898_cefd04580b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2533424347_cf2f84872b.jpg: 448x640 16 persons, 1 car, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2533424347_cf2f84872b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2473738924_eca928d12f.jpg: 640x480 3 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2473738924_eca928d12f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/270816949_ffad112278.jpg: 384x640 1 person, 1 horse, 8.8ms\n",
      "Speed: 1.6ms preprocess, 8.8ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 270816949_ffad112278.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/378170167_9b5119d918.jpg: 480x640 1 dog, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 378170167_9b5119d918.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467282545_273a97b628.jpg: 448x640 3 persons, 1 book, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3467282545_273a97b628.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/516394876_8b9b8021bc.jpg: 640x480 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 516394876_8b9b8021bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/782401952_5bc5d3413a.jpg: 512x640 3 persons, 2 bottles, 1 remote, 1 teddy bear, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 782401952_5bc5d3413a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512163695_51a108761d.jpg: 448x640 1 person, 1 bench, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512163695_51a108761d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3412036192_d8cd12ed3f.jpg: 480x640 1 person, 1 bench, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3412036192_d8cd12ed3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3043266735_904dda6ded.jpg: 480x640 2 persons, 3 chairs, 2 potted plants, 1 vase, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3043266735_904dda6ded.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2466171100_5e60cfcc11.jpg: 608x640 2 dogs, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2466171100_5e60cfcc11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3089742441_d42531c14f.jpg: 640x480 5 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3089742441_d42531c14f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1917203130_fcaff8b10e.jpg: 640x512 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1917203130_fcaff8b10e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3593538248_dffa1a5ed4.jpg: 448x640 8 persons, 1 baseball bat, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3593538248_dffa1a5ed4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2552438538_285a05b86c.jpg: 512x640 3 persons, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2552438538_285a05b86c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3130093622_362f32f2bb.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3130093622_362f32f2bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2347921097_f2e35753c0.jpg: 416x640 2 dogs, 1 bed, 1 teddy bear, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2347921097_f2e35753c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/472661386_723aae880b.jpg: 480x640 21 persons, 1 laptop, 2 books, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 472661386_723aae880b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2698487246_e827404cac.jpg: 448x640 10 persons, 1 tie, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2698487246_e827404cac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3249125548_700d874380.jpg: 448x640 4 persons, 2 bicycles, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3249125548_700d874380.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3005872315_e6771763bd.jpg: 448x640 1 person, 1 remote, 1 cell phone, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3005872315_e6771763bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/181777261_84c48b31cb.jpg: 448x640 10 persons, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 181777261_84c48b31cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3203453897_6317aac6ff.jpg: 640x448 2 persons, 3 cars, 1 fire hydrant, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3203453897_6317aac6ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/269898095_d00ac7d7a4.jpg: 448x640 2 persons, 1 car, 1 potted plant, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 269898095_d00ac7d7a4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/755326139_ee344ece7b.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 755326139_ee344ece7b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1799271536_6e69c8f1dc.jpg: 640x640 1 dog, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1799271536_6e69c8f1dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3360876049_9047edeab9.jpg: 640x640 2 persons, 2 motorcycles, 6.4ms\n",
      "Speed: 2.1ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3360876049_9047edeab9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/561417861_8e25d0c0e8.jpg: 640x480 5 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 561417861_8e25d0c0e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/238512430_30dc12b683.jpg: 448x640 1 person, 1 boat, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 238512430_30dc12b683.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3210419174_d083a16f77.jpg: 480x640 12 persons, 1 cell phone, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3210419174_d083a16f77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1413956047_c826f90c8b.jpg: 480x640 3 persons, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1413956047_c826f90c8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430607596_7e4f74e3ff.jpg: 448x640 2 persons, 1 surfboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3430607596_7e4f74e3ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2064790732_219e52e19c.jpg: 640x448 6 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2064790732_219e52e19c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2256133102_e2c8314ecb.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2256133102_e2c8314ecb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2753506871_dc38e7d153.jpg: 448x640 1 cow, 1 elephant, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2753506871_dc38e7d153.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2876494009_9f96d7eaf2.jpg: 640x448 3 persons, 1 skateboard, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2876494009_9f96d7eaf2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3695517194_2a6b604cb2.jpg: 448x640 2 persons, 1 sports ball, 12.7ms\n",
      "Speed: 3.4ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3695517194_2a6b604cb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/466176275_f40d37851e.jpg: 640x640 2 dogs, 10.7ms\n",
      "Speed: 3.6ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 466176275_f40d37851e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301754574_465af5bf6d.jpg: 448x640 1 bird, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3301754574_465af5bf6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3394070357_cb2a3243fc.jpg: 512x640 1 dog, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3394070357_cb2a3243fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/97577988_65e2eae14a.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 97577988_65e2eae14a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/206087108_d4557d38ee.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 206087108_d4557d38ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613030730_0b28b079ba.jpg: 480x640 1 person, 1 banana, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3613030730_0b28b079ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3375991133_87d7c40925.jpg: 448x640 5 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3375991133_87d7c40925.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2412390588_a89cab30f4.jpg: 416x640 1 person, 1 potted plant, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2412390588_a89cab30f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/554526471_a31f8b74ef.jpg: 480x640 16 persons, 1 suitcase, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 554526471_a31f8b74ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3253060519_55d98c208f.jpg: 640x448 8 persons, 1 car, 1 traffic light, 1 umbrella, 1 clock, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3253060519_55d98c208f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2857473929_4f52662c30.jpg: 512x640 2 persons, 3 bicycles, 1 tie, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2857473929_4f52662c30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3512127856_18a4c7aace.jpg: 384x640 1 person, 10.0ms\n",
      "Speed: 1.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3512127856_18a4c7aace.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163563871_cef3cf33ea.jpg: 640x512 1 person, 1 sports ball, 1 tennis racket, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3163563871_cef3cf33ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3436074878_21515a6706.jpg: 448x640 1 person, 1 dog, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3436074878_21515a6706.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2923891109_ea0cc932ed.jpg: 640x448 1 dog, 1 frisbee, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2923891109_ea0cc932ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3685328542_ab999b83bb.jpg: 640x512 9 persons, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3685328542_ab999b83bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345656_861aacefde.jpg: 640x448 5 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241345656_861aacefde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457315666_b943111dec.jpg: 544x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3457315666_b943111dec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3181409177_edb09c2718.jpg: 640x448 3 kites, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3181409177_edb09c2718.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2333288869_8c01e4c859.jpg: 448x640 2 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2333288869_8c01e4c859.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/435827376_4384c3005a.jpg: 640x448 2 persons, 1 car, 2 handbags, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 435827376_4384c3005a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/134894450_dadea45d65.jpg: 640x448 1 person, 1 bench, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 134894450_dadea45d65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/538825260_a4a8784b75.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 538825260_a4a8784b75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520617304_e53d37f0af.jpg: 448x640 3 persons, 1 car, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3520617304_e53d37f0af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1989145280_3b54452188.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1989145280_3b54452188.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2831215155_07ba8f1805.jpg: 640x448 1 person, 1 truck, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2831215155_07ba8f1805.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2928835996_88b9f9503d.jpg: 320x640 1 person, 1 umbrella, 9.2ms\n",
      "Speed: 1.4ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2928835996_88b9f9503d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293945284_6a04e477a9.jpg: 448x640 13 persons, 14 kites, 1 chair, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3293945284_6a04e477a9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3157220149_cc3c8cc84d.jpg: 640x544 2 persons, 1 handbag, 1 cell phone, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3157220149_cc3c8cc84d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335997221_254366c400.jpg: 480x640 5 persons, 1 couch, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3335997221_254366c400.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1522787272_5a31497ef2.jpg: 480x640 1 dog, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1522787272_5a31497ef2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2281054343_95d6d3b882.jpg: 416x640 11 persons, 1 car, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2281054343_95d6d3b882.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3212085754_35fdc9ccaa.jpg: 640x544 2 persons, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3212085754_35fdc9ccaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2171154778_8189169336.jpg: 480x640 3 persons, 1 baseball bat, 1 chair, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2171154778_8189169336.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2959737718_31203fddb5.jpg: 640x416 17 persons, 1 sports ball, 7.1ms\n",
      "Speed: 1.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2959737718_31203fddb5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2192411521_9c7e488c5e.jpg: 640x512 1 person, 1 bicycle, 1 bird, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2192411521_9c7e488c5e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/501650847_b0beba926c.jpg: 480x640 2 persons, 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 501650847_b0beba926c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3609952704_3719ab0524.jpg: 384x640 3 persons, 1 sports ball, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3609952704_3719ab0524.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016741474_72b4355198.jpg: 448x640 5 persons, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3016741474_72b4355198.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3585123310_9a8e94bd2b.jpg: 544x640 1 dog, 1 bottle, 1 couch, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3585123310_9a8e94bd2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2597308074_acacc12e1b.jpg: 480x640 2 persons, 1 frisbee, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2597308074_acacc12e1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425685827_03683e8e5a.jpg: 448x640 9 persons, 1 backpack, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3425685827_03683e8e5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3287904625_f68ca5cea7.jpg: 480x640 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3287904625_f68ca5cea7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/694496803_f2a05869cf.jpg: 640x480 11 persons, 2 bicycles, 1 umbrella, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 694496803_f2a05869cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/350176185_b8c5591e36.jpg: 640x480 1 person, 1 dog, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 350176185_b8c5591e36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2144049642_070cf541b4.jpg: 640x512 1 person, 1 cup, 1 book, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2144049642_070cf541b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/160585932_fa6339f248.jpg: 640x448 1 car, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 160585932_fa6339f248.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3519815055_304dc8e8d6.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3519815055_304dc8e8d6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2707835735_6537b27e8f.jpg: 640x480 5 persons, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2707835735_6537b27e8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1881494074_1bebd93089.jpg: 640x480 3 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1881494074_1bebd93089.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3659090958_a56913ca68.jpg: 448x640 4 persons, 1 dining table, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3659090958_a56913ca68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/140526327_3cb984de09.jpg: 384x640 1 person, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 140526327_3cb984de09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320154278_c67e01b8d1.jpg: 448x640 2 persons, 2 bicycles, 1 motorcycle, 1 bench, 1 backpack, 1 handbag, 1 chair, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3320154278_c67e01b8d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/452416075_60b2bb5832.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 452416075_60b2bb5832.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244443352_d7636e1253.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 244443352_d7636e1253.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3229730008_63f8ca2de2.jpg: 640x448 5 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3229730008_63f8ca2de2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2466171114_3fa51415a7.jpg: 416x640 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2466171114_3fa51415a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3668984985_b60ceb2ae9.jpg: 640x448 5 persons, 1 bench, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3668984985_b60ceb2ae9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339558806_b4afdc8394.jpg: 640x480 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3339558806_b4afdc8394.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2458006588_754c4aa09c.jpg: 640x384 1 dog, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 2458006588_754c4aa09c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384401298_e389c01abc.jpg: 480x640 1 bird, 1 horse, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2384401298_e389c01abc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2473791980_805c819bd4.jpg: 448x640 2 persons, 1 boat, 1 surfboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2473791980_805c819bd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3409506817_775e38d219.jpg: 448x640 23 persons, 2 cars, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3409506817_775e38d219.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2467850190_07a74d89b7.jpg: 512x640 1 dog, 1 chair, 1 tv, 1 book, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2467850190_07a74d89b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2137789511_69a6c6afa8.jpg: 448x640 1 person, 1 bicycle, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2137789511_69a6c6afa8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3504940491_94c43792ed.jpg: 640x640 2 persons, 7.1ms\n",
      "Speed: 2.4ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3504940491_94c43792ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485738889_c2a00876a6.jpg: 640x608 2 persons, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 485738889_c2a00876a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3347500603_13670ee6bf.jpg: 448x640 3 persons, 1 suitcase, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3347500603_13670ee6bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2394267183_735d2dc868.jpg: 480x640 1 person, 5 cars, 1 fire hydrant, 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2394267183_735d2dc868.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3365602213_dd3287a633.jpg: 640x448 2 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3365602213_dd3287a633.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3196995975_3e38eabf01.jpg: 448x640 1 dog, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3196995975_3e38eabf01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119913014_688d4997d7.jpg: 640x480 3 persons, 1 sports ball, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3119913014_688d4997d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3502993968_4ee36afb0e.jpg: 448x640 1 person, 2 bicycles, 1 backpack, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3502993968_4ee36afb0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1009434119_febe49276a.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1009434119_febe49276a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/162743064_bb242faa31.jpg: 480x640 1 car, 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 162743064_bb242faa31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561319255_ce5ede291e.jpg: 512x640 1 person, 1 baseball bat, 1 potted plant, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2561319255_ce5ede291e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/246901891_4c4ea49c3a.jpg: 640x480 2 persons, 7 cars, 1 bench, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 246901891_4c4ea49c3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2942311160_a154104c62.jpg: 480x640 1 skateboard, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2942311160_a154104c62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2381583688_a6dd0a7279.jpg: 480x640 1 person, 1 chair, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2381583688_a6dd0a7279.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2643309379_2cde08516c.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2643309379_2cde08516c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3168354472_866fe70d36.jpg: 480x640 3 persons, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3168354472_866fe70d36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/110595925_f3395c8bd6.jpg: 480x640 1 person, 2 bicycles, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 110595925_f3395c8bd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537579448_a7f92cef58.jpg: 512x640 1 person, 1 frisbee, 1 sports ball, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 537579448_a7f92cef58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/475980315_b8ecd50094.jpg: 480x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 475980315_b8ecd50094.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1358892595_7a37c45788.jpg: 512x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1358892595_7a37c45788.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1142283988_6b227c5231.jpg: 448x640 10 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1142283988_6b227c5231.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3136404885_f4d8f1d15a.jpg: 640x640 3 persons, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3136404885_f4d8f1d15a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766926202_4201bf2bf9.jpg: 448x640 3 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2766926202_4201bf2bf9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030079705_0dee8a3e89.jpg: 480x640 1 person, 6.8ms\n",
      "Speed: 1.4ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3030079705_0dee8a3e89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3468023754_8a07d4c56e.jpg: 640x608 5 persons, 1 cup, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3468023754_8a07d4c56e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2183227136_8bb657846b.jpg: 480x640 2 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2183227136_8bb657846b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/190638179_be9da86589.jpg: 480x640 1 person, 1 truck, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 190638179_be9da86589.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/90011335_cfdf9674c2.jpg: 448x640 1 person, 1 boat, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 90011335_cfdf9674c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2855727603_e917ded363.jpg: 448x640 1 dog, 1 horse, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2855727603_e917ded363.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2282600972_c22d1e03c7.jpg: 448x640 4 persons, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2282600972_c22d1e03c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425851292_de92a072ee.jpg: 512x640 2 persons, 1 sports ball, 1 tennis racket, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3425851292_de92a072ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2318502106_33f2e4b4fc.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2318502106_33f2e4b4fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2029280005_a19609c81a.jpg: 480x640 1 person, 1 bench, 1 skateboard, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2029280005_a19609c81a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3545793128_af3af544dc.jpg: 640x608 13 persons, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3545793128_af3af544dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347460_81d5d62bf6.jpg: 640x448 12 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347460_81d5d62bf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1056359656_662cee0814.jpg: 640x480 2 persons, 1 couch, 1 laptop, 1 book, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1056359656_662cee0814.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/255091927_2eb643beb2.jpg: 480x640 6 persons, 1 bottle, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 255091927_2eb643beb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3578372039_57473f473c.jpg: 512x640 1 person, 1 skateboard, 1 potted plant, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3578372039_57473f473c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3057618932_3b80ae85de.jpg: 448x640 1 person, 1 bicycle, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3057618932_3b80ae85de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/367925122_335ed279a8.jpg: 608x640 1 person, 1 chair, 7.2ms\n",
      "Speed: 2.1ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 367925122_335ed279a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3658016590_f761e72dc3.jpg: 640x448 3 horses, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3658016590_f761e72dc3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/890734502_a5ae67beac.jpg: 480x640 3 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 890734502_a5ae67beac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3703107969_175da4b276.jpg: 640x448 3 persons, 1 bicycle, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3703107969_175da4b276.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2894229082_ddc395f138.jpg: 448x640 1 person, 7.7ms\n",
      "Speed: 1.6ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2894229082_ddc395f138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3493255026_5fdaa52cbe.jpg: 640x448 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3493255026_5fdaa52cbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2842865689_e37256d9ce.jpg: 448x640 3 persons, 1 bench, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2842865689_e37256d9ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420284416_a90af1fb7a.jpg: 512x640 12 persons, 1 sports ball, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3420284416_a90af1fb7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2206600240_f65df56a09.jpg: 384x640 7 persons, 1 surfboard, 2 cups, 2 chairs, 1 potted plant, 1 dining table, 7.9ms\n",
      "Speed: 1.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2206600240_f65df56a09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2822891602_ff61df2ece.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2822891602_ff61df2ece.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2337757064_08c4033824.jpg: 448x640 1 person, 1 sports ball, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2337757064_08c4033824.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/756909515_a416161656.jpg: 448x640 3 dogs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 756909515_a416161656.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3284460070_6805990149.jpg: 448x640 1 person, 1 bicycle, 1 motorcycle, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3284460070_6805990149.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584829998_25e59fdef3.jpg: 448x640 3 persons, 4 dogs, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584829998_25e59fdef3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/345785626_9fa59f38ce.jpg: 640x480 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 345785626_9fa59f38ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/923550133_ac9d7a2932.jpg: 640x640 2 persons, 1 bird, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 923550133_ac9d7a2932.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1936215201_d03a75cbba.jpg: 640x480 1 person, 2 bicycles, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1936215201_d03a75cbba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/582899605_d96f9201f1.jpg: 640x480 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 582899605_d96f9201f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/557601144_50b8c40393.jpg: 640x480 1 bench, 1 dog, 2 chairs, 5 potted plants, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 557601144_50b8c40393.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3092370204_029b6bc10a.jpg: 640x448 10 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3092370204_029b6bc10a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2748435417_ea7bbcc17c.jpg: 640x640 5 persons, 1 skateboard, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2748435417_ea7bbcc17c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2505360288_c972bd29c4.jpg: 512x640 1 fire hydrant, 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2505360288_c972bd29c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300148649_72f7f0399c.jpg: 640x448 2 persons, 1 tennis racket, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 300148649_72f7f0399c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1499554025_a8ffe0e479.jpg: 448x640 14 persons, 1 dog, 2 sheeps, 1 cow, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1499554025_a8ffe0e479.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2824004868_1fc0a81173.jpg: 480x640 1 person, 1 potted plant, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2824004868_1fc0a81173.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3135504530_0f4130d8f8.jpg: 448x640 3 persons, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3135504530_0f4130d8f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2861100960_457ceda7fa.jpg: 448x640 7 persons, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2861100960_457ceda7fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3700554247_9824ae6f3a.jpg: 416x640 1 bird, 8.0ms\n",
      "Speed: 1.7ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3700554247_9824ae6f3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2981702521_2459f2c1c4.jpg: 640x448 6 persons, 1 train, 1 handbag, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2981702521_2459f2c1c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2110898123_07729c1461.jpg: 480x640 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2110898123_07729c1461.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173461705_b5cdeef1eb.jpg: 448x640 2 dogs, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3173461705_b5cdeef1eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/426920445_d07d1fd0f7.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 426920445_d07d1fd0f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3580082200_ea10bf2f68.jpg: 640x448 1 person, 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3580082200_ea10bf2f68.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/58363928_6f7074608c.jpg: 480x640 1 person, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 58363928_6f7074608c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/541063517_35044c554a.jpg: 480x640 1 person, 2 backpacks, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 541063517_35044c554a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627216820_4952bacbcb.jpg: 448x640 1 bench, 2 dogs, 1 frisbee, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3627216820_4952bacbcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351667632_00f586a30c.jpg: 448x640 13 persons, 1 tie, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3351667632_00f586a30c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3421480658_b3518b6819.jpg: 448x640 2 persons, 1 cat, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3421480658_b3518b6819.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3209523192_05a4cef844.jpg: 480x640 1 person, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3209523192_05a4cef844.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/455611732_d65bf3e976.jpg: 448x640 1 person, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 455611732_d65bf3e976.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211289105_e0360a9c7f.jpg: 544x640 7 persons, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3211289105_e0360a9c7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3406409018_03de95181e.jpg: 480x640 8 persons, 1 backpack, 1 skateboard, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3406409018_03de95181e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2455528149_6c3477fd33.jpg: 480x640 3 persons, 1 frisbee, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2455528149_6c3477fd33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2533010184_ef2fd71297.jpg: 416x640 1 person, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2533010184_ef2fd71297.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3231880001_193a579b97.jpg: 480x640 2 persons, 1 umbrella, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3231880001_193a579b97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2521938802_853224f378.jpg: 448x640 1 person, 1 tennis racket, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2521938802_853224f378.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2772532341_c4597a94ed.jpg: 640x640 10 persons, 1 sports ball, 1 cup, 2 chairs, 8.1ms\n",
      "Speed: 2.5ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2772532341_c4597a94ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454754632_977c1523be.jpg: 640x512 1 person, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3454754632_977c1523be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380041023_0dfd712ef1.jpg: 448x640 1 dog, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 380041023_0dfd712ef1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2076906555_c20dc082db.jpg: 640x448 3 persons, 12.6ms\n",
      "Speed: 3.3ms preprocess, 12.6ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2076906555_c20dc082db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628103548_2708abcda2.jpg: 448x640 4 persons, 1 tie, 10.5ms\n",
      "Speed: 2.5ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3628103548_2708abcda2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2176980976_7054c99621.jpg: 480x640 8 persons, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2176980976_7054c99621.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3565654691_22b97d3994.jpg: 640x480 1 person, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3565654691_22b97d3994.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3051125715_db76cebd1e.jpg: 448x640 (no detections), 11.8ms\n",
      "Speed: 2.9ms preprocess, 11.8ms inference, 0.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3051125715_db76cebd1e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347067_e58d05dbdc.jpg: 448x640 5 persons, 9.3ms\n",
      "Speed: 2.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241347067_e58d05dbdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1042020065_fb3d3ba5ba.jpg: 448x640 6 persons, 10 boats, 8.2ms\n",
      "Speed: 2.2ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1042020065_fb3d3ba5ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1295698260_e10c53c137.jpg: 640x352 10 persons, 1 handbag, 9.7ms\n",
      "Speed: 1.7ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 1295698260_e10c53c137.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/997338199_7343367d7f.jpg: 512x640 1 person, 12.8ms\n",
      "Speed: 3.9ms preprocess, 12.8ms inference, 1.9ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 997338199_7343367d7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2339516180_12493e8ecf.jpg: 640x480 1 person, 6 cars, 2 dogs, 10.6ms\n",
      "Speed: 2.8ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2339516180_12493e8ecf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1680126311_b92a2e8e72.jpg: 448x640 3 persons, 1 sports ball, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1680126311_b92a2e8e72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/44129946_9eeb385d77.jpg: 480x640 2 persons, 2 boats, 1 bench, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 44129946_9eeb385d77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3216901052_269ace7b3c.jpg: 544x640 1 person, 1 dog, 1 bed, 9.1ms\n",
      "Speed: 2.4ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3216901052_269ace7b3c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474406285_01f3d24b71.jpg: 448x640 3 persons, 3 dogs, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3474406285_01f3d24b71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2854959952_3991a385ab.jpg: 640x480 2 dogs, 1 sports ball, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2854959952_3991a385ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3495490064_8db40a83af.jpg: 608x640 2 persons, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3495490064_8db40a83af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3234375022_1464ea7f8a.jpg: 640x448 1 snowboard, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3234375022_1464ea7f8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339140382_2e49bc324a.jpg: 448x640 1 bird, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3339140382_2e49bc324a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3477712686_8428614c75.jpg: 448x640 12 persons, 1 baseball glove, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3477712686_8428614c75.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2354456107_bf5c766a05.jpg: 448x640 10 persons, 1 backpack, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2354456107_bf5c766a05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3478591390_b526580644.jpg: 448x640 2 persons, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3478591390_b526580644.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3579686259_b1fe6aefc9.jpg: 480x640 1 dog, 1 bear, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3579686259_b1fe6aefc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3591170729_406fdb74e5.jpg: 448x640 5 persons, 2 skateboards, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3591170729_406fdb74e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2189995738_352607a63b.jpg: 448x640 2 persons, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2189995738_352607a63b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3296715418_29542dcdc2.jpg: 512x640 1 dog, 8.7ms\n",
      "Speed: 2.3ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3296715418_29542dcdc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109780402_dbae082dc5.jpg: 480x640 2 persons, 8.7ms\n",
      "Speed: 2.2ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3109780402_dbae082dc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2577972703_a22c5f2a87.jpg: 480x640 1 person, 8.0ms\n",
      "Speed: 2.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2577972703_a22c5f2a87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3553476195_fb3747d7c1.jpg: 320x640 2 dogs, 1 carrot, 9.6ms\n",
      "Speed: 1.5ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3553476195_fb3747d7c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697379772_40d831392b.jpg: 640x480 2 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3697379772_40d831392b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2800004913_c8394ba332.jpg: 640x448 3 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2800004913_c8394ba332.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1392272228_cf104086e6.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1392272228_cf104086e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3258395783_2de3a4ba27.jpg: 448x640 4 persons, 1 bicycle, 2 chairs, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3258395783_2de3a4ba27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/640203018_c0e7175831.jpg: 448x640 15 persons, 1 motorcycle, 1 handbag, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 640203018_c0e7175831.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2057160636_6e9cf3b5f0.jpg: 320x640 3 boats, 8.2ms\n",
      "Speed: 1.2ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 2057160636_6e9cf3b5f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1312954382_cf6d70d63a.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1312954382_cf6d70d63a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3687062281_e62f70baf3.jpg: 480x640 3 persons, 1 handbag, 3 cups, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3687062281_e62f70baf3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3025513877_1a6160070d.jpg: 448x640 1 person, 8.0ms\n",
      "Speed: 1.6ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3025513877_1a6160070d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2998861375_02817e0147.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2998861375_02817e0147.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2419797375_553f867472.jpg: 448x640 1 person, 1 chair, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2419797375_553f867472.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/146098876_0d99d7fb98.jpg: 480x640 4 persons, 1 handbag, 1 frisbee, 1 sports ball, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 146098876_0d99d7fb98.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335097235_538f4777c3.jpg: 448x640 1 dog, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3335097235_538f4777c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/195962284_e57178054a.jpg: 448x640 6 persons, 1 frisbee, 8.3ms\n",
      "Speed: 1.6ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 195962284_e57178054a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/522486784_978021d537.jpg: 480x640 2 dogs, 7.3ms\n",
      "Speed: 2.0ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 522486784_978021d537.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1718184338_5968d88edb.jpg: 448x640 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1718184338_5968d88edb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2845691057_d4ab89d889.jpg: 448x640 4 persons, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2845691057_d4ab89d889.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1418019748_51c7d59c11.jpg: 640x448 3 persons, 1 bottle, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1418019748_51c7d59c11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486340101_ff01d8f3f9.jpg: 448x640 7 persons, 1 car, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3486340101_ff01d8f3f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2991994607_06f24ec7a6.jpg: 480x640 4 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2991994607_06f24ec7a6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/47870024_73a4481f7d.jpg: 480x640 1 person, 1 skateboard, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 47870024_73a4481f7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3044745642_1d0784ff29.jpg: 480x640 3 persons, 2 chairs, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3044745642_1d0784ff29.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/468918320_9c275b877f.jpg: 576x640 2 persons, 1 bench, 7.6ms\n",
      "Speed: 2.4ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 468918320_9c275b877f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1797507760_384744fb34.jpg: 640x448 3 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1797507760_384744fb34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/529198549_5cd9fedf3f.jpg: 480x640 3 persons, 1 boat, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 529198549_5cd9fedf3f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/356143774_ef3e93eede.jpg: 416x640 2 birds, 7.7ms\n",
      "Speed: 1.5ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 356143774_ef3e93eede.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1539166395_0cdc0accee.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1539166395_0cdc0accee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/461505235_590102a5bf.jpg: 640x480 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 461505235_590102a5bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294179574_4f67e67d6f.jpg: 448x640 9 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3294179574_4f67e67d6f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1075867198_27ca2e7efe.jpg: 448x640 1 person, 2 dogs, 1 kite, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1075867198_27ca2e7efe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/778885185_3f6905370b.jpg: 448x640 2 dogs, 1 sports ball, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 778885185_3f6905370b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1884065356_c6c34b4568.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1884065356_c6c34b4568.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2885111681_dc328ecfff.jpg: 640x448 1 person, 2 handbags, 4 potted plants, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2885111681_dc328ecfff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3072172967_630e9c69d0.jpg: 480x640 5 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3072172967_630e9c69d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2463067409_78188c584c.jpg: 544x640 1 dog, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2463067409_78188c584c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3402638444_dab914a3de.jpg: 512x640 9 persons, 1 train, 1 chair, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3402638444_dab914a3de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2522540026_6ee8ab4c6a.jpg: 448x640 2 persons, 1 car, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2522540026_6ee8ab4c6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2517284816_9b8fd3c6b6.jpg: 448x640 7 persons, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2517284816_9b8fd3c6b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2441313372_6a1d59582b.jpg: 640x480 2 persons, 1 couch, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2441313372_6a1d59582b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3121521593_18f0ec14f7.jpg: 512x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3121521593_18f0ec14f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2588456052_8842b47005.jpg: 640x480 2 persons, 1 tennis racket, 13.4ms\n",
      "Speed: 3.6ms preprocess, 13.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2588456052_8842b47005.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2246717855_c0c08fe5d2.jpg: 640x480 6 persons, 1 handbag, 1 clock, 10.1ms\n",
      "Speed: 2.8ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2246717855_c0c08fe5d2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2422482455_b98d9c2120.jpg: 448x640 3 persons, 1 dog, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2422482455_b98d9c2120.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613424631_3ae537624f.jpg: 448x640 1 person, 1 bicycle, 8.6ms\n",
      "Speed: 2.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3613424631_3ae537624f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3318564834_4ccea90497.jpg: 448x640 2 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3318564834_4ccea90497.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496606439_9333831e73.jpg: 480x640 2 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 496606439_9333831e73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1417295167_5299df6db8.jpg: 640x448 2 persons, 1 cup, 1 cell phone, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1417295167_5299df6db8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595080592_5fd55570e5.jpg: 576x640 9 persons, 1 bicycle, 9.3ms\n",
      "Speed: 2.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3595080592_5fd55570e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2533414541_362bf043bb.jpg: 448x640 16 persons, 1 car, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2533414541_362bf043bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119887967_271a097464.jpg: 448x640 2 persons, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3119887967_271a097464.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496110746_a93ca191ae.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 496110746_a93ca191ae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2281075738_230892b241.jpg: 288x640 3 dogs, 10.5ms\n",
      "Speed: 1.4ms preprocess, 10.5ms inference, 1.4ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 2281075738_230892b241.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613005134_bb7f304da1.jpg: 640x544 5 persons, 9.9ms\n",
      "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3613005134_bb7f304da1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1302657647_46b36c0d66.jpg: 640x480 1 person, 1 sports ball, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1302657647_46b36c0d66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532192208_64b069d05d.jpg: 448x640 4 persons, 1 laptop, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532192208_64b069d05d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3085667865_fa001816be.jpg: 448x640 2 persons, 3 surfboards, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3085667865_fa001816be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3125041578_c1f2d73b6d.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3125041578_c1f2d73b6d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484820303_7be0e914b4.jpg: 576x640 2 persons, 1 bicycle, 1 umbrella, 2 handbags, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3484820303_7be0e914b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/237277765_9e6fa5b99a.jpg: 448x640 1 person, 10.3ms\n",
      "Speed: 2.2ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 237277765_9e6fa5b99a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1306145560_1e37081b91.jpg: 640x448 1 dog, 1 bear, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1306145560_1e37081b91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2659606300_bea3feaf8b.jpg: 640x448 1 person, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2659606300_bea3feaf8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3758175529_81941e7cc9.jpg: 640x640 1 person, 1 surfboard, 1 vase, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3758175529_81941e7cc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320356356_1497e53f80.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3320356356_1497e53f80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547368652_0d85c665d3.jpg: 480x640 1 person, 1 bench, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3547368652_0d85c665d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3102363657_dc95fe6850.jpg: 640x448 3 persons, 1 teddy bear, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3102363657_dc95fe6850.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3622929632_26fb800000.jpg: 512x640 1 dog, 1 chair, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3622929632_26fb800000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3250593457_9049a73b61.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3250593457_9049a73b61.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486831913_2b9390ebbc.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3486831913_2b9390ebbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/370442541_60d93ecd13.jpg: 448x640 2 persons, 13 birds, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 370442541_60d93ecd13.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543264612_c53cc163b4.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 543264612_c53cc163b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2773400732_5b65a25857.jpg: 480x640 1 person, 1 baseball glove, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2773400732_5b65a25857.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247168324_c45eaf734d.jpg: 480x640 1 person, 2 cars, 1 bus, 2 trucks, 1 traffic light, 6.3ms\n",
      "Speed: 1.7ms preprocess, 6.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3247168324_c45eaf734d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3420278866_1d17c12713.jpg: 640x512 1 person, 1 tennis racket, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3420278866_1d17c12713.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089426086_7acc98a3a8.jpg: 416x640 2 persons, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2089426086_7acc98a3a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2862676319_a9dab1309f.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2862676319_a9dab1309f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/386160015_d4b31df68e.jpg: 448x640 2 dogs, 1 horse, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 386160015_d4b31df68e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/264859622_f3a00ab409.jpg: 640x480 1 person, 1 sports ball, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 264859622_f3a00ab409.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/847782643_57248bbdab.jpg: 640x608 2 persons, 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 847782643_57248bbdab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/496555371_3e1ee0d97d.jpg: 480x640 3 dogs, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 496555371_3e1ee0d97d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2622517932_57c52c376f.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2622517932_57c52c376f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2608289957_044849f73e.jpg: 640x448 1 person, 1 suitcase, 2 potted plants, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2608289957_044849f73e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3679407035_708774de34.jpg: 640x448 10 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3679407035_708774de34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3470303255_fbb41b8dd0.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3470303255_fbb41b8dd0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223606402_bb2aa6db95.jpg: 448x640 4 persons, 2 skateboards, 12.1ms\n",
      "Speed: 3.6ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3223606402_bb2aa6db95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/160566014_59528ff897.jpg: 480x640 2 persons, 6 cars, 10.4ms\n",
      "Speed: 2.7ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 160566014_59528ff897.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315250232_83e24a2d51.jpg: 448x640 1 person, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3315250232_83e24a2d51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2752331711_cb18abba5a.jpg: 448x640 1 person, 1 chair, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2752331711_cb18abba5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524360969_472a7152f0.jpg: 480x640 2 persons, 1 stop sign, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 524360969_472a7152f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3465473743_7da0c5d973.jpg: 640x448 2 persons, 2 cars, 2 trains, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3465473743_7da0c5d973.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351111378_b5d80783a1.jpg: 448x640 1 person, 2 surfboards, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3351111378_b5d80783a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2704934519_457dc38986.jpg: 640x448 1 person, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2704934519_457dc38986.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2935649082_1ca60180c6.jpg: 448x640 2 persons, 1 surfboard, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2935649082_1ca60180c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2247889670_413db8094b.jpg: 480x640 1 person, 1 dog, 1 handbag, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2247889670_413db8094b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/749840385_e004bf3b7c.jpg: 448x640 1 person, 1 surfboard, 13.2ms\n",
      "Speed: 3.5ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 749840385_e004bf3b7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3322200641_c2e51ff37b.jpg: 448x640 1 cat, 1 dog, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3322200641_c2e51ff37b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/562588230_edb2c071c8.jpg: 480x640 1 person, 1 motorcycle, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 562588230_edb2c071c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2256138896_3e24b0b28d.jpg: 640x448 1 person, 2 skateboards, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2256138896_3e24b0b28d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3315726723_64c9b0a945.jpg: 512x640 2 persons, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3315726723_64c9b0a945.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3425573919_409d9e15b2.jpg: 448x640 20 persons, 2 benchs, 1 handbag, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3425573919_409d9e15b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2701487024_e866eb4550.jpg: 448x640 1 person, 1 surfboard, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2701487024_e866eb4550.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2579460386_94c489028d.jpg: 448x640 1 bird, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2579460386_94c489028d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/396360611_941e5849a3.jpg: 448x640 1 person, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 396360611_941e5849a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3339105374_cc41e0b7d7.jpg: 384x640 3 persons, 1 dog, 1 backpack, 10.6ms\n",
      "Speed: 1.8ms preprocess, 10.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3339105374_cc41e0b7d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2968135512_51fbb56e3e.jpg: 640x640 1 person, 1 car, 1 traffic light, 1 skateboard, 9.2ms\n",
      "Speed: 2.8ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2968135512_51fbb56e3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2675190069_d5c3b2c876.jpg: 480x640 2 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2675190069_d5c3b2c876.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2708176152_1634cb754d.jpg: 640x448 9 persons, 2 bicycles, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2708176152_1634cb754d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3619381206_5bc8b406f9.jpg: 640x448 1 person, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3619381206_5bc8b406f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2535746605_8124bf4e4f.jpg: 416x640 2 persons, 9.2ms\n",
      "Speed: 1.9ms preprocess, 9.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2535746605_8124bf4e4f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/950273886_88c324e663.jpg: 448x640 2 persons, 1 cell phone, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 950273886_88c324e663.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2173677067_9d0732bcc2.jpg: 640x480 2 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2173677067_9d0732bcc2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364715316_df8c20bac1.jpg: 640x640 2 persons, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3364715316_df8c20bac1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2713897716_c8cd610360.jpg: 512x640 1 bird, 1 dog, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2713897716_c8cd610360.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3713324467_104d72f7db.jpg: 544x640 2 persons, 1 umbrella, 1 kite, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3713324467_104d72f7db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2640000969_b5404a5143.jpg: 448x640 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2640000969_b5404a5143.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/452345346_afe1248586.jpg: 608x640 2 horses, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 452345346_afe1248586.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3349258288_5300c40430.jpg: 448x640 1 person, 1 bench, 1 skateboard, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3349258288_5300c40430.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2999735171_87ca43c225.jpg: 448x640 4 persons, 1 car, 1 backpack, 1 skateboard, 1 book, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2999735171_87ca43c225.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1691573772_1adef8e40e.jpg: 480x640 1 person, 1 bicycle, 2 cars, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1691573772_1adef8e40e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2319087586_919472310f.jpg: 448x640 3 persons, 1 backpack, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2319087586_919472310f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/377872672_d499aae449.jpg: 576x640 1 person, 2 dogs, 1 horse, 7.8ms\n",
      "Speed: 2.1ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 377872672_d499aae449.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543603259_ef26d9c72d.jpg: 576x640 3 persons, 6.3ms\n",
      "Speed: 1.8ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 543603259_ef26d9c72d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259992722_4c5e895734.jpg: 448x640 16 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259992722_4c5e895734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2141065212_463a6997e1.jpg: 544x640 1 person, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2141065212_463a6997e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/368212336_bc19b0bb72.jpg: 448x640 2 persons, 1 car, 2 traffic lights, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 368212336_bc19b0bb72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2865376471_43c5e6b941.jpg: 448x640 4 persons, 2 dogs, 1 bottle, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2865376471_43c5e6b941.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3014546644_d53db746ec.jpg: 448x640 1 bird, 1 dog, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3014546644_d53db746ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/515335111_c4afd5b903.jpg: 480x640 1 person, 1 boat, 8.7ms\n",
      "Speed: 1.8ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 515335111_c4afd5b903.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3324056835_84904fe2f8.jpg: 448x640 2 persons, 4 birds, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3324056835_84904fe2f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3259229498_2b5708c0c6.jpg: 448x640 18 persons, 1 car, 6 chairs, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3259229498_2b5708c0c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503011427_a4ee547c77.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3503011427_a4ee547c77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3638178504_be1ff246bd.jpg: 544x640 1 person, 2 horses, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3638178504_be1ff246bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3427402225_234d712eeb.jpg: 448x640 2 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3427402225_234d712eeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/123997871_6a9ca987b1.jpg: 416x640 10 persons, 1 sports ball, 1 baseball bat, 1 tennis racket, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 123997871_6a9ca987b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090386465_b6ebb7df2c.jpg: 640x640 3 persons, 7.8ms\n",
      "Speed: 2.6ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2090386465_b6ebb7df2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3131990048_369b081021.jpg: 640x448 2 birds, 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3131990048_369b081021.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/613030608_4355e007c7.jpg: 640x384 1 person, 1 skateboard, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 613030608_4355e007c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2409312675_7755a7b816.jpg: 640x608 2 persons, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2409312675_7755a7b816.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856456013_335297f587.jpg: 608x640 5 persons, 2 cars, 1 sports ball, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2856456013_335297f587.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421706022_1ddb6a7a78.jpg: 480x640 2 persons, 1 teddy bear, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 421706022_1ddb6a7a78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/530454257_66d58b49ee.jpg: 480x640 2 persons, 1 orange, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 530454257_66d58b49ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595398879_13e33b8916.jpg: 640x448 2 persons, 1 handbag, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3595398879_13e33b8916.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3057497487_57ecc60ff1.jpg: 640x480 2 persons, 1 airplane, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3057497487_57ecc60ff1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3457784061_8f77f43a9c.jpg: 448x640 2 persons, 1 laptop, 8.1ms\n",
      "Speed: 1.6ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3457784061_8f77f43a9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3651107058_d84d4c3c25.jpg: 512x640 15 persons, 1 backpack, 1 suitcase, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3651107058_d84d4c3c25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3109136206_f7d201b368.jpg: 640x640 1 dog, 8.2ms\n",
      "Speed: 2.8ms preprocess, 8.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3109136206_f7d201b368.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3623302162_099f983d58.jpg: 544x640 1 bench, 1 dog, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3623302162_099f983d58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701249979_8bc757e171.jpg: 448x640 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3701249979_8bc757e171.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3338217927_3c5cf3f7c6.jpg: 448x640 1 person, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3338217927_3c5cf3f7c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/305749904_54a612fd1a.jpg: 640x448 1 person, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 305749904_54a612fd1a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/285586547_c81f8905a1.jpg: 544x640 1 person, 1 motorcycle, 1 dog, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 285586547_c81f8905a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/543102698_38e7e38bbc.jpg: 640x480 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 543102698_38e7e38bbc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2695085632_10c4e6ea78.jpg: 416x640 5 persons, 1 motorcycle, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2695085632_10c4e6ea78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3538527033_df13112d51.jpg: 640x608 13 persons, 2 traffic lights, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3538527033_df13112d51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1574401950_6bedc0d29b.jpg: 512x640 1 person, 1 truck, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1574401950_6bedc0d29b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1131155939_b4b457b05e.jpg: 448x640 3 persons, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1131155939_b4b457b05e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3662871327_b128d25f04.jpg: 480x640 3 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3662871327_b128d25f04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/354999632_915ea81e53.jpg: 448x640 (no detections), 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 0.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 354999632_915ea81e53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/86542183_5e312ae4d4.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 86542183_5e312ae4d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2908512223_7e27631ed4.jpg: 448x640 4 persons, 2 cars, 1 handbag, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2908512223_7e27631ed4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2787868417_810985234d.jpg: 640x448 1 person, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2787868417_810985234d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/664470170_6a1ad20c45.jpg: 480x640 2 persons, 1 cake, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 664470170_6a1ad20c45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/317109978_cb557802e1.jpg: 640x480 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 317109978_cb557802e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2272426567_9e9fb79db0.jpg: 640x480 2 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2272426567_9e9fb79db0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3062273350_fd66106f21.jpg: 416x640 9 persons, 1 tie, 7.4ms\n",
      "Speed: 1.4ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3062273350_fd66106f21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2279980395_989d48ae72.jpg: 480x640 2 persons, 1 sports ball, 3 apples, 1 orange, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2279980395_989d48ae72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3571039224_b34fa2f94c.jpg: 448x640 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3571039224_b34fa2f94c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2727051596_be65bfb3d3.jpg: 448x640 3 persons, 1 tennis racket, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2727051596_be65bfb3d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399284917_721aefe2a7.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3399284917_721aefe2a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561481438_447b852e4d.jpg: 480x640 1 bear, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2561481438_447b852e4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677613006_4689cb8e4e.jpg: 448x640 4 persons, 2 bottles, 1 tv, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677613006_4689cb8e4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2815745115_c8479d560c.jpg: 480x640 1 person, 1 carrot, 9 teddy bears, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2815745115_c8479d560c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2977379863_2e8d7a104e.jpg: 576x640 8 persons, 1 chair, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2977379863_2e8d7a104e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2987576188_f82304f394.jpg: 480x640 7 persons, 2 handbags, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2987576188_f82304f394.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/303607405_f36edf16c6.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 303607405_f36edf16c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3662963630_8f097e38d4.jpg: 448x640 8 persons, 1 bicycle, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3662963630_8f097e38d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2879241506_b421536330.jpg: 448x640 2 persons, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2879241506_b421536330.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2818735880_68b3dfe1f5.jpg: 640x480 1 person, 2 horses, 2 sheeps, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2818735880_68b3dfe1f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636491114_ab34dac833.jpg: 640x480 1 person, 2 chairs, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3636491114_ab34dac833.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497069793_2d4baf5b4b.jpg: 640x448 3 persons, 1 couch, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3497069793_2d4baf5b4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346772831_b2748ba1f0.jpg: 640x448 8 persons, 1 backpack, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2346772831_b2748ba1f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2679851489_a58780291e.jpg: 640x480 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2679851489_a58780291e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3355494822_61353a224d.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3355494822_61353a224d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3411022255_210eefc375.jpg: 640x480 6 persons, 1 traffic light, 3 handbags, 1 suitcase, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3411022255_210eefc375.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3295418287_5d590dac43.jpg: 448x640 8 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3295418287_5d590dac43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3539840291_1c3eed701d.jpg: 512x640 7 persons, 1 sports ball, 1 baseball glove, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3539840291_1c3eed701d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1352410176_af6b139734.jpg: 640x480 1 person, 2 suitcases, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1352410176_af6b139734.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2556392380_ee57514233.jpg: 640x480 1 person, 2 chairs, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2556392380_ee57514233.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/319851847_7212423309.jpg: 640x448 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 319851847_7212423309.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/131632409_4de0d4e710.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 131632409_4de0d4e710.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2832978253_8fcc72da3b.jpg: 480x640 1 person, 1 frisbee, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2832978253_8fcc72da3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/774009278_8e75b7d498.jpg: 480x640 1 dog, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 774009278_8e75b7d498.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1957371533_62bc720bac.jpg: 448x640 1 bear, 8.3ms\n",
      "Speed: 1.8ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1957371533_62bc720bac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/370614351_98b8a166b9.jpg: 640x480 3 persons, 2 tvs, 1 book, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 370614351_98b8a166b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3589052481_059e5e2c37.jpg: 448x640 2 persons, 8.3ms\n",
      "Speed: 1.7ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3589052481_059e5e2c37.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3147758035_e8a70818cb.jpg: 480x640 2 persons, 1 bicycle, 2 cars, 1 fire hydrant, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3147758035_e8a70818cb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3337461409_e4e317853d.jpg: 416x640 3 sheeps, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3337461409_e4e317853d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1821238649_2fda79d6d7.jpg: 640x480 1 person, 2 handbags, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1821238649_2fda79d6d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3719461451_07de35af3a.jpg: 448x640 2 birds, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3719461451_07de35af3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629334536_11f2d49e05.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2629334536_11f2d49e05.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/502783522_3656f27014.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 502783522_3656f27014.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2127207912_9298824e66.jpg: 640x480 3 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2127207912_9298824e66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2188688248_f57a28a5a7.jpg: 448x640 13 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2188688248_f57a28a5a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030294889_78b2ccbe51.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3030294889_78b2ccbe51.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/366548880_3d3e914746.jpg: 480x640 2 dogs, 6.4ms\n",
      "Speed: 1.7ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 366548880_3d3e914746.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429391520_930b153f94.jpg: 640x448 2 persons, 1 bicycle, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3429391520_930b153f94.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3480052428_c034b98a08.jpg: 480x640 11 persons, 1 umbrella, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3480052428_c034b98a08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3507076266_8b17993fbb.jpg: 640x448 2 persons, 1 horse, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3507076266_8b17993fbb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2797188545_aeb26c54c0.jpg: 480x640 3 persons, 1 boat, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2797188545_aeb26c54c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3173157541_70b16b4318.jpg: 448x640 5 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3173157541_70b16b4318.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3542425197_2ef81c6051.jpg: 640x448 2 persons, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3542425197_2ef81c6051.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3157622277_9f59b4f62f.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3157622277_9f59b4f62f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3205336477_037d4b6bd9.jpg: 448x640 10 persons, 1 bottle, 1 chair, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3205336477_037d4b6bd9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3257207516_9d2bc0ea04.jpg: 480x640 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3257207516_9d2bc0ea04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2430860418_fd0726f414.jpg: 448x640 2 persons, 2 dogs, 1 potted plant, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2430860418_fd0726f414.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3040033126_9f4b88261b.jpg: 480x640 1 dog, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3040033126_9f4b88261b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2513260012_03d33305cf.jpg: 512x640 2 dogs, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2513260012_03d33305cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3308488725_f91d9aba27.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3308488725_f91d9aba27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3332136681_9aecf101fd.jpg: 480x640 5 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3332136681_9aecf101fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/236518934_c62a133077.jpg: 640x480 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 236518934_c62a133077.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2482629385_f370b290d1.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2482629385_f370b290d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/823697339_aadbeef495.jpg: 448x640 7 persons, 1 handbag, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 823697339_aadbeef495.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2073964624_52da3a0fc4.jpg: 480x640 1 person, 1 backpack, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2073964624_52da3a0fc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3030823649_3b7b6c728d.jpg: 480x640 1 person, 1 dog, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3030823649_3b7b6c728d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/431018958_84b2beebff.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 431018958_84b2beebff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/801607443_f15956d1ce.jpg: 640x448 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 801607443_f15956d1ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2708634088_a4686be24c.jpg: 256x640 1 dog, 1 horse, 1 frisbee, 8.7ms\n",
      "Speed: 1.0ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 256, 640)\n",
      "Cropped images saved for 2708634088_a4686be24c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3355683198_715fb1a2ac.jpg: 448x640 1 person, 2 skateboards, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3355683198_715fb1a2ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3181328245_7c04ce1691.jpg: 480x640 4 persons, 1 chair, 1 bed, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3181328245_7c04ce1691.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2558312618_13d362df66.jpg: 448x640 1 dog, 1 horse, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2558312618_13d362df66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3650188378_cc8aea89f0.jpg: 640x352 4 persons, 3 cars, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 3650188378_cc8aea89f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2937497894_e3664a9513.jpg: 640x640 4 persons, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2937497894_e3664a9513.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3327487011_1372c425fb.jpg: 448x640 17 persons, 1 cup, 8.0ms\n",
      "Speed: 1.8ms preprocess, 8.0ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3327487011_1372c425fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3006926228_cf3c067b3e.jpg: 640x576 4 persons, 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3006926228_cf3c067b3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497502407_ec566442c9.jpg: 640x480 1 person, 5 cars, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3497502407_ec566442c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/445148321_9f2f3ac711.jpg: 480x640 1 person, 1 bird, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 445148321_9f2f3ac711.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3720210639_18bb34e475.jpg: 640x448 3 persons, 1 bicycle, 1 truck, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3720210639_18bb34e475.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393343330_b13df4d8ec.jpg: 480x640 5 persons, 2 backpacks, 2 books, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3393343330_b13df4d8ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016606751_0e8be20abd.jpg: 512x640 1 person, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3016606751_0e8be20abd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/267836606_bbea2267c8.jpg: 480x640 1 dog, 1 cow, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 267836606_bbea2267c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2597958208_e03aa149c9.jpg: 448x640 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2597958208_e03aa149c9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143680966_0010ff8c60.jpg: 640x384 (no detections), 8.5ms\n",
      "Speed: 1.5ms preprocess, 8.5ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 143680966_0010ff8c60.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3479245321_6a5bc470f8.jpg: 480x640 11 persons, 5 umbrellas, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3479245321_6a5bc470f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710468717_c051d96a5f.jpg: 448x640 2 persons, 1 bench, 1 dog, 1 cow, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3710468717_c051d96a5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/150387174_24825cf871.jpg: 448x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 150387174_24825cf871.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1296770308_3db2022f5a.jpg: 480x640 4 persons, 1 chair, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1296770308_3db2022f5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3603064161_a8f3b6455d.jpg: 640x512 2 persons, 1 chair, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3603064161_a8f3b6455d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3672106148_56cfb5fc8d.jpg: 448x640 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3672106148_56cfb5fc8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3441511444_b031585b45.jpg: 448x640 5 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3441511444_b031585b45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/84713990_d3f3cef78b.jpg: 480x640 3 persons, 1 surfboard, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 84713990_d3f3cef78b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3739833689_a0038545bd.jpg: 640x448 1 person, 1 baseball bat, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3739833689_a0038545bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2782850287_1408f7ec43.jpg: 448x640 2 persons, 1 cell phone, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2782850287_1408f7ec43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174726084_c108de0a64.jpg: 448x640 2 persons, 1 umbrella, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3174726084_c108de0a64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2714674623_8cd05ac114.jpg: 480x640 3 persons, 2 chairs, 8.6ms\n",
      "Speed: 1.7ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2714674623_8cd05ac114.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3191027142_33e86636ca.jpg: 544x640 14 persons, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3191027142_33e86636ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3351493005_6e5030f596.jpg: 608x640 12 persons, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3351493005_6e5030f596.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3449170348_34dac4a380.jpg: 640x448 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3449170348_34dac4a380.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2896180326_88785fe078.jpg: 448x640 7 persons, 6 motorcycles, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2896180326_88785fe078.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2905975229_7c37156dbe.jpg: 384x640 4 persons, 1 airplane, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2905975229_7c37156dbe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497237366_366997495d.jpg: 640x448 6 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3497237366_366997495d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3284955091_59317073f0.jpg: 448x640 1 person, 1 snowboard, 1 skateboard, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3284955091_59317073f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3668259129_e073af1533.jpg: 640x448 1 person, 1 skateboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3668259129_e073af1533.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3364797223_1f0b2f98ed.jpg: 544x640 7 persons, 1 bicycle, 1 handbag, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3364797223_1f0b2f98ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484906808_ee20714408.jpg: 544x640 3 persons, 6.6ms\n",
      "Speed: 2.1ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3484906808_ee20714408.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/961611340_251081fcb8.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 961611340_251081fcb8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/226531363_33ac01d931.jpg: 640x640 1 dog, 6.9ms\n",
      "Speed: 2.2ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 226531363_33ac01d931.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2674784195_704f6b79d0.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2674784195_704f6b79d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3482787182_a5c6d4b386.jpg: 448x640 19 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3482787182_a5c6d4b386.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1277185009_06478dd457.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1277185009_06478dd457.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3198231851_6b1727482b.jpg: 544x640 2 dogs, 1 sports ball, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3198231851_6b1727482b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2223382277_9efa58ec45.jpg: 640x512 2 persons, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2223382277_9efa58ec45.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091580843_178042c50b.jpg: 480x640 2 persons, 12.9ms\n",
      "Speed: 3.7ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3091580843_178042c50b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3166578139_33500f7e8a.jpg: 448x640 1 person, 2 backpacks, 10.5ms\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3166578139_33500f7e8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2760716468_b541e9fd0f.jpg: 640x640 1 person, 2 backpacks, 1 handbag, 9.2ms\n",
      "Speed: 3.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2760716468_b541e9fd0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3134387321_3a253224c1.jpg: 576x640 3 persons, 1 tie, 1 wine glass, 1 cup, 9.0ms\n",
      "Speed: 2.6ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3134387321_3a253224c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3525403875_7f95e0cbfe.jpg: 480x640 2 dogs, 1 sheep, 1 sports ball, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3525403875_7f95e0cbfe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489865145_65ea6d1c14.jpg: 608x640 1 person, 9.2ms\n",
      "Speed: 2.8ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 489865145_65ea6d1c14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3262793378_773b21ec19.jpg: 640x640 4 persons, 1 bicycle, 1 umbrella, 8.9ms\n",
      "Speed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3262793378_773b21ec19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294830188_e46bd9b93c.jpg: 416x640 3 birds, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3294830188_e46bd9b93c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2926233397_71e617f3a3.jpg: 448x640 1 dog, 1 elephant, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2926233397_71e617f3a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1523800748_a59e980eee.jpg: 640x480 3 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1523800748_a59e980eee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2823200990_7b02b7cc36.jpg: 640x448 2 persons, 2 chairs, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2823200990_7b02b7cc36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2834103050_512e5b330a.jpg: 480x640 7 persons, 2 bicycles, 2 backpacks, 1 umbrella, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2834103050_512e5b330a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2861932486_52befd8592.jpg: 640x480 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2861932486_52befd8592.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2928152792_b16c73434a.jpg: 640x480 1 person, 1 motorcycle, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2928152792_b16c73434a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3559374748_f18c7caa55.jpg: 416x640 1 truck, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3559374748_f18c7caa55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3019923691_3b3c5a4766.jpg: 448x640 1 dog, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3019923691_3b3c5a4766.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/482882719_165722082d.jpg: 448x640 7 persons, 3 boats, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 482882719_165722082d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949982320_c704b31626.jpg: 640x448 2 persons, 1 skateboard, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2949982320_c704b31626.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2229177914_3308fe7d20.jpg: 448x640 12 persons, 2 dogs, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2229177914_3308fe7d20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2619454551_c4bb726a85.jpg: 608x640 1 bird, 1 dog, 1 sports ball, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2619454551_c4bb726a85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090387793_274ab4cf7d.jpg: 640x512 5 persons, 1 baseball glove, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2090387793_274ab4cf7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3393152604_27bd1037f2.jpg: 448x640 1 bear, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3393152604_27bd1037f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2449518585_113dc4a8e5.jpg: 640x448 2 persons, 1 surfboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2449518585_113dc4a8e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2039457436_fc30f5e1ce.jpg: 640x448 3 persons, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2039457436_fc30f5e1ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560977956_e08d2cd531.jpg: 480x640 2 persons, 1 boat, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3560977956_e08d2cd531.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3642220260_3aa8a52670.jpg: 640x448 1 dog, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3642220260_3aa8a52670.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/72218201_e0e9c7d65b.jpg: 480x640 9 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 72218201_e0e9c7d65b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/407008823_bdd7fc6ed5.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 407008823_bdd7fc6ed5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2549968784_39bfbe44f9.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2549968784_39bfbe44f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3254662117_b2e7dede6e.jpg: 640x448 2 persons, 1 remote, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3254662117_b2e7dede6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/415657941_454d370721.jpg: 640x448 2 persons, 1 tennis racket, 6.4ms\n",
      "Speed: 1.5ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 415657941_454d370721.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2955673642_4279b32097.jpg: 480x640 2 cars, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2955673642_4279b32097.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2934000107_d2ff15c814.jpg: 448x640 1 dog, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2934000107_d2ff15c814.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3374759363_d6f7a0df41.jpg: 640x480 3 persons, 1 bench, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3374759363_d6f7a0df41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2198511848_311d8a8c2f.jpg: 448x640 1 person, 1 snowboard, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2198511848_311d8a8c2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/240696675_7d05193aa0.jpg: 480x640 1 dog, 13.5ms\n",
      "Speed: 3.6ms preprocess, 13.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 240696675_7d05193aa0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2119660490_ce0d4d1f73.jpg: 448x640 1 dog, 1 sports ball, 1 bed, 10.6ms\n",
      "Speed: 2.4ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2119660490_ce0d4d1f73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/640506101_ae1145b6d1.jpg: 640x480 1 person, 1 dog, 1 cow, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 640506101_ae1145b6d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214100656_80cda1b86b.jpg: 448x640 8 persons, 12.9ms\n",
      "Speed: 3.2ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3214100656_80cda1b86b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2120571547_05cd56de85.jpg: 640x480 3 persons, 1 train, 11.2ms\n",
      "Speed: 2.8ms preprocess, 11.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2120571547_05cd56de85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3283897411_af9d0b497d.jpg: 640x448 1 person, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3283897411_af9d0b497d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3508882611_3947c0dbf5.jpg: 448x640 1 person, 1 dog, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3508882611_3947c0dbf5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3552206648_123bf4ac82.jpg: 448x640 2 birds, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3552206648_123bf4ac82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/166507476_9be5b9852a.jpg: 480x640 1 bird, 1 elephant, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 166507476_9be5b9852a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3105315670_5f86f73753.jpg: 448x640 4 persons, 7 cars, 1 traffic light, 3 benchs, 1 handbag, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3105315670_5f86f73753.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1491192153_7c395991e5.jpg: 640x448 1 person, 1 cup, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1491192153_7c395991e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2967630001_cdc5560c0b.jpg: 480x640 1 dog, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2967630001_cdc5560c0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3520199925_ca18d0f41e.jpg: 448x640 4 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3520199925_ca18d0f41e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3419634480_c390f62a6e.jpg: 640x544 1 dog, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3419634480_c390f62a6e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/152029243_b3582c36fa.jpg: 448x640 1 person, 1 motorcycle, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 152029243_b3582c36fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3352871762_c9e88592d3.jpg: 448x640 3 persons, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3352871762_c9e88592d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3064097919_e536ab9693.jpg: 448x640 2 persons, 2 skateboards, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3064097919_e536ab9693.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/300577374_bfccb0430b.jpg: 448x640 17 persons, 1 handbag, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 300577374_bfccb0430b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215847501_c723905ba4.jpg: 480x640 8 persons, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3215847501_c723905ba4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2512876666_9da03f9589.jpg: 640x480 1 person, 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2512876666_9da03f9589.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1118557877_736f339752.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1118557877_736f339752.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/434433505_966e50e17d.jpg: 512x640 2 persons, 1 skateboard, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 434433505_966e50e17d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211453055_05cbfe37cd.jpg: 640x448 3 persons, 2 skiss, 1 snowboard, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3211453055_05cbfe37cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3385593926_d3e9c21170.jpg: 448x640 1 dog, 2 horses, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3385593926_d3e9c21170.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3446347599_0ecc49a9d5.jpg: 384x640 2 persons, 8.6ms\n",
      "Speed: 1.5ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3446347599_0ecc49a9d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486538055_bcf4d3dfda.jpg: 576x640 2 dogs, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3486538055_bcf4d3dfda.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3235746553_a40416c00e.jpg: 544x640 1 dog, 1 horse, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3235746553_a40416c00e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2897832422_0cbdb1421e.jpg: 416x640 1 person, 1 chair, 7.5ms\n",
      "Speed: 1.4ms preprocess, 7.5ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2897832422_0cbdb1421e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/446291803_2fd4641b99.jpg: 448x640 4 persons, 1 backpack, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 446291803_2fd4641b99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2466420387_86fe77c966.jpg: 448x640 14 persons, 1 umbrella, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2466420387_86fe77c966.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1321949151_77b77b4617.jpg: 640x640 6 persons, 1 handbag, 7.3ms\n",
      "Speed: 2.3ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 1321949151_77b77b4617.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2449289139_08fc1092c1.jpg: 480x640 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2449289139_08fc1092c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2832076014_ff08c92037.jpg: 384x640 1 boat, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2832076014_ff08c92037.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3701509233_a2275a4e57.jpg: 384x640 (no detections), 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3701509233_a2275a4e57.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2453318633_550228acd4.jpg: 640x512 1 dog, 1 kite, 7.0ms\n",
      "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2453318633_550228acd4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/267015208_d80b3eb94d.jpg: 352x640 13 persons, 5 cars, 1 umbrella, 8.7ms\n",
      "Speed: 1.4ms preprocess, 8.7ms inference, 1.3ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 267015208_d80b3eb94d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640443200_b8066f37f6.jpg: 448x640 9 persons, 1 couch, 2 potted plants, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3640443200_b8066f37f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2672354635_3a03f76486.jpg: 640x448 3 dogs, 1 sports ball, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2672354635_3a03f76486.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1468389504_c724bdcad0.jpg: 480x640 3 persons, 1 baseball bat, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1468389504_c724bdcad0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/533979933_a95b03323b.jpg: 640x480 1 person, 1 surfboard, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 533979933_a95b03323b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2755053974_5cc157512e.jpg: 480x640 1 person, 1 car, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2755053974_5cc157512e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3163273640_8d3ef22eaf.jpg: 640x544 6 persons, 4 elephants, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3163273640_8d3ef22eaf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3040051410_6205682ba3.jpg: 608x640 7 persons, 6.9ms\n",
      "Speed: 2.0ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3040051410_6205682ba3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565657591_6c1cdfc092.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2565657591_6c1cdfc092.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661138991_d55aa0e5dc.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2661138991_d55aa0e5dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2426781076_e3f4d2685c.jpg: 480x640 1 person, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2426781076_e3f4d2685c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/465859490_b077219424.jpg: 416x640 7 persons, 1 bottle, 2 cups, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 465859490_b077219424.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2312747482_20a81b2230.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2312747482_20a81b2230.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2875583266_4da13ae12d.jpg: 640x448 1 person, 1 motorcycle, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2875583266_4da13ae12d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/221973402_ecb1cd51f1.jpg: 640x544 1 person, 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 2.1ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 221973402_ecb1cd51f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2977246776_b14be8290d.jpg: 544x640 3 persons, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2977246776_b14be8290d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/897621891_efb1e00d1d.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 897621891_efb1e00d1d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2745663684_650f84e1e6.jpg: 640x448 1 person, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2745663684_650f84e1e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/138705546_be7a6845dd.jpg: 480x640 4 persons, 1 boat, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 138705546_be7a6845dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2214132302_80064fd79d.jpg: 448x640 5 persons, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2214132302_80064fd79d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2462153092_e3f4d8f6a2.jpg: 480x640 1 person, 2 dogs, 1 sheep, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2462153092_e3f4d8f6a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3516312179_f520469038.jpg: 448x640 4 persons, 1 sports ball, 1 baseball bat, 1 baseball glove, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3516312179_f520469038.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3540416139_c884f38351.jpg: 480x640 1 person, 2 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3540416139_c884f38351.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2038662925_f4fa8c2534.jpg: 640x512 2 persons, 1 bench, 7.2ms\n",
      "Speed: 2.0ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2038662925_f4fa8c2534.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560891822_7d4c1e3580.jpg: 640x448 2 persons, 1 skateboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3560891822_7d4c1e3580.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472703856_568d9778b5.jpg: 480x640 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3472703856_568d9778b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3456362961_d8f7e347a8.jpg: 640x448 2 persons, 2 benchs, 1 sports ball, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3456362961_d8f7e347a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3171347658_f0d5469c56.jpg: 448x640 1 motorcycle, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3171347658_f0d5469c56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2709683703_5385ea9ef4.jpg: 640x608 1 person, 1 dog, 2 frisbees, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 2709683703_5385ea9ef4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2993388841_6746140656.jpg: 640x448 10 persons, 1 frisbee, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2993388841_6746140656.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2328106090_b7c2725501.jpg: 512x640 1 person, 1 bicycle, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2328106090_b7c2725501.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2782480767_064c95eff2.jpg: 640x512 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2782480767_064c95eff2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/252846811_7b250935a7.jpg: 480x640 1 dog, 2 cows, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 252846811_7b250935a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/189721896_1ffe76d89e.jpg: 448x640 1 person, 8.9ms\n",
      "Speed: 2.5ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 189721896_1ffe76d89e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2198964806_c57b0534d3.jpg: 640x480 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2198964806_c57b0534d3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1625306051_7099519baa.jpg: 640x480 2 persons, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1625306051_7099519baa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/58357057_dea882479e.jpg: 480x640 1 person, 1 kite, 8.6ms\n",
      "Speed: 1.8ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 58357057_dea882479e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873445888_8764699246.jpg: 640x448 1 person, 3 frisbees, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2873445888_8764699246.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2892992529_f3335d0a71.jpg: 640x448 6 persons, 1 dog, 1 frisbee, 1 chair, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2892992529_f3335d0a71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/182493240_40410254b0.jpg: 480x640 3 persons, 1 snowboard, 8.3ms\n",
      "Speed: 1.9ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 182493240_40410254b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2644326817_8f45080b87.jpg: 448x640 1 person, 1 dog, 1 frisbee, 8.6ms\n",
      "Speed: 1.9ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2644326817_8f45080b87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2265096094_8cc34d669c.jpg: 512x640 1 person, 1 horse, 7.7ms\n",
      "Speed: 2.0ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2265096094_8cc34d669c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2891961886_b7a2f0b0fd.jpg: 448x640 1 person, 4 cars, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2891961886_b7a2f0b0fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2568656919_6e49d2a82b.jpg: 480x640 1 person, 1 dog, 9.8ms\n",
      "Speed: 2.5ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2568656919_6e49d2a82b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155361712_2cbf59c78e.jpg: 640x640 1 person, 1 snowboard, 9.1ms\n",
      "Speed: 3.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3155361712_2cbf59c78e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2242178517_2325b85e5f.jpg: 640x480 1 person, 1 skis, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2242178517_2325b85e5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3649916507_b88a3d2082.jpg: 640x448 1 person, 1 bicycle, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3649916507_b88a3d2082.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3485486737_953f9d3be2.jpg: 480x640 2 persons, 1 bus, 1 truck, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3485486737_953f9d3be2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2421446839_fe7d46c177.jpg: 480x640 1 car, 2 dogs, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2421446839_fe7d46c177.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3037558954_20115165e3.jpg: 480x640 6 persons, 1 boat, 12.5ms\n",
      "Speed: 3.6ms preprocess, 12.5ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3037558954_20115165e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3736786640_70df13be2c.jpg: 480x640 3 persons, 10.8ms\n",
      "Speed: 3.0ms preprocess, 10.8ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3736786640_70df13be2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3165750962_e2e3843679.jpg: 608x640 1 dog, 9.9ms\n",
      "Speed: 3.1ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3165750962_e2e3843679.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2956413620_d59de03a06.jpg: 480x640 1 truck, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2956413620_d59de03a06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/535309053_ec737abde8.jpg: 448x640 4 persons, 1 dog, 5 umbrellas, 1 dining table, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 535309053_ec737abde8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/374103966_2987706be1.jpg: 480x640 7 persons, 1 wine glass, 2 cups, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 374103966_2987706be1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/399246804_b4b5dc70e1.jpg: 640x448 1 person, 2 dogs, 1 chair, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 399246804_b4b5dc70e1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485741580_ab523fa657.jpg: 640x480 1 dog, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 485741580_ab523fa657.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2633722629_5eeb649c09.jpg: 448x640 2 persons, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2633722629_5eeb649c09.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1818403842_553a2a392c.jpg: 480x640 1 dog, 12.3ms\n",
      "Speed: 3.4ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1818403842_553a2a392c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/458004873_f084c47a88.jpg: 448x640 2 persons, 10.3ms\n",
      "Speed: 2.4ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 458004873_f084c47a88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2901074943_041aba4607.jpg: 640x448 2 persons, 1 bottle, 1 book, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2901074943_041aba4607.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514188115_f51932ae5d.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3514188115_f51932ae5d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2369840118_a1c4240ab7.jpg: 448x640 5 persons, 1 dog, 1 sheep, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2369840118_a1c4240ab7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461106572_920c8c0112.jpg: 480x640 7 persons, 1 handbag, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3461106572_920c8c0112.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1808007704_ee8a93abb4.jpg: 640x448 3 persons, 1 elephant, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1808007704_ee8a93abb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2839038702_e168128665.jpg: 640x480 1 dog, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2839038702_e168128665.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2215165918_2bf5b659dd.jpg: 448x640 1 dog, 11.0ms\n",
      "Speed: 2.5ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2215165918_2bf5b659dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3223973114_6c15538ce9.jpg: 640x640 (no detections), 9.9ms\n",
      "Speed: 3.2ms preprocess, 9.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3223973114_6c15538ce9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3153828367_5fc2c37c07.jpg: 544x640 1 person, 1 teddy bear, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3153828367_5fc2c37c07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/447111935_5af98563e3.jpg: 576x640 2 dogs, 9.2ms\n",
      "Speed: 2.7ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 447111935_5af98563e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3487261028_30791528ec.jpg: 480x640 1 person, 1 dog, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3487261028_30791528ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2728813605_cfc943e1ab.jpg: 640x544 1 person, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2728813605_cfc943e1ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1248953128_24c9f8d924.jpg: 640x544 1 person, 1 sports ball, 8.4ms\n",
      "Speed: 2.4ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 1248953128_24c9f8d924.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/397286183_745abbf40d.jpg: 480x640 3 persons, 1 bottle, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 397286183_745abbf40d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3126981064_1e803c3d7f.jpg: 480x640 1 person, 1 dog, 1 surfboard, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3126981064_1e803c3d7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3697456750_460aea7252.jpg: 448x640 13 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3697456750_460aea7252.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2275372714_017c269742.jpg: 512x640 3 persons, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2275372714_017c269742.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2073174497_18b779999c.jpg: 480x640 1 person, 1 bicycle, 3 cars, 1 backpack, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2073174497_18b779999c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514184232_b336414040.jpg: 448x640 1 person, 2 baseball gloves, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514184232_b336414040.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1547883892_e29b3db42e.jpg: 480x640 1 person, 1 car, 2 benchs, 1 cup, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1547883892_e29b3db42e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/391579205_c8373b5411.jpg: 640x544 1 person, 1 dog, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 391579205_c8373b5411.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3722572342_6904d11d52.jpg: 384x640 1 airplane, 1 boat, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3722572342_6904d11d52.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2910758605_73a3f5a5c2.jpg: 448x640 7 persons, 1 bus, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2910758605_73a3f5a5c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3214237686_6566b8b52f.jpg: 448x640 1 person, 2 snowboards, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3214237686_6566b8b52f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417143124_6feb8290cc.jpg: 640x512 2 persons, 10 cars, 1 dog, 8.2ms\n",
      "Speed: 1.8ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3417143124_6feb8290cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/400851260_5911898657.jpg: 480x640 1 person, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 400851260_5911898657.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2510560080_1439fe32f2.jpg: 448x640 2 persons, 1 horse, 8.1ms\n",
      "Speed: 1.9ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2510560080_1439fe32f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3400082864_9c737c1450.jpg: 512x640 9 persons, 1 tennis racket, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3400082864_9c737c1450.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3158327361_6f1a518228.jpg: 480x640 4 persons, 3 birds, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3158327361_6f1a518228.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/252504549_135b0db5a3.jpg: 512x640 1 dog, 1 frisbee, 1 sports ball, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 252504549_135b0db5a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/519061891_320061864e.jpg: 640x640 1 person, 2 frisbees, 1 carrot, 8.5ms\n",
      "Speed: 3.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 519061891_320061864e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2699426519_228719b1db.jpg: 480x640 1 dog, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2699426519_228719b1db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3688858505_e8afd1475d.jpg: 640x480 3 persons, 1 bottle, 1 cup, 1 chair, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3688858505_e8afd1475d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/544257613_d9a1fea3f7.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 544257613_d9a1fea3f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3086507638_d8a2cd0ac3.jpg: 480x640 3 dogs, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3086507638_d8a2cd0ac3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2021602343_03023e1fd1.jpg: 448x640 2 persons, 1 sports ball, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2021602343_03023e1fd1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1295671216_cde1b9c9d1.jpg: 640x256 6 persons, 9.2ms\n",
      "Speed: 1.1ms preprocess, 9.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 256)\n",
      "Cropped images saved for 1295671216_cde1b9c9d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/871290646_307cddd4e7.jpg: 448x640 4 persons, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 871290646_307cddd4e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3193335577_9bdbaf9f70.jpg: 640x480 1 person, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3193335577_9bdbaf9f70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2544246151_727427ee07.jpg: 544x640 2 persons, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2544246151_727427ee07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3242718240_3358f2d6e6.jpg: 480x640 7 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3242718240_3358f2d6e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3468346269_9d162aacfe.jpg: 448x640 2 persons, 1 car, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3468346269_9d162aacfe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3350785999_462f333c44.jpg: 448x640 16 persons, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3350785999_462f333c44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3510218982_318f738b76.jpg: 448x640 3 persons, 6.2ms\n",
      "Speed: 1.7ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3510218982_318f738b76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2092419948_eea8001d0f.jpg: 640x480 1 person, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2092419948_eea8001d0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3663307538_468739e4c3.jpg: 480x640 11 persons, 1 suitcase, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3663307538_468739e4c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3551170666_01df31412d.jpg: 480x640 1 dog, 1 couch, 1 teddy bear, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3551170666_01df31412d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564157681_03a13b7112.jpg: 480x640 4 persons, 1 dog, 1 frisbee, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3564157681_03a13b7112.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3279524184_d5e2ffbaed.jpg: 384x640 2 dogs, 1 frisbee, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3279524184_d5e2ffbaed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2776029171_5abdd5a22f.jpg: 448x640 2 persons, 8.1ms\n",
      "Speed: 1.7ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2776029171_5abdd5a22f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143680442_2f03f76944.jpg: 640x384 1 person, 9.0ms\n",
      "Speed: 1.5ms preprocess, 9.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 143680442_2f03f76944.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/838074897_9d6270b3cd.jpg: 640x512 1 person, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 838074897_9d6270b3cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3324746155_71e14f60ce.jpg: 640x448 2 persons, 1 sports ball, 8.2ms\n",
      "Speed: 1.7ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3324746155_71e14f60ce.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/210625425_fb1ef5d23b.jpg: 480x640 1 truck, 7.8ms\n",
      "Speed: 1.9ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 210625425_fb1ef5d23b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566446626_9793890f95.jpg: 640x480 2 persons, 1 bench, 8.6ms\n",
      "Speed: 2.0ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 566446626_9793890f95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3216762979_813c45a8ec.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3216762979_813c45a8ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569755200_cef7ee2233.jpg: 640x448 1 person, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3569755200_cef7ee2233.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347760_d44c8d3a01.jpg: 640x448 3 persons, 7.2ms\n",
      "Speed: 3.1ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347760_d44c8d3a01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/248858242_1c33c54ada.jpg: 480x640 2 dogs, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 248858242_1c33c54ada.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109738916_236dc456ac.jpg: 480x640 3 persons, 2 motorcycles, 7.2ms\n",
      "Speed: 1.9ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 109738916_236dc456ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2514581496_8f4102377e.jpg: 640x512 1 cat, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2514581496_8f4102377e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677954655_df4c0845aa.jpg: 640x448 1 person, 1 car, 1 traffic light, 1 fire hydrant, 1 skateboard, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3677954655_df4c0845aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2689001252_e0016c89f0.jpg: 480x640 3 persons, 1 frisbee, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2689001252_e0016c89f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2943334864_6bab479a3e.jpg: 448x640 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2943334864_6bab479a3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3049770416_0fb1954315.jpg: 448x640 1 person, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3049770416_0fb1954315.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2435685480_a79d42e564.jpg: 640x448 1 person, 1 skateboard, 13.3ms\n",
      "Speed: 3.5ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2435685480_a79d42e564.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3001612175_53567ffb58.jpg: 416x640 2 persons, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3001612175_53567ffb58.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1428681303_04213524e3.jpg: 512x640 17 persons, 1 bicycle, 1 skateboard, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1428681303_04213524e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2838619742_eed855d8a8.jpg: 448x640 1 person, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2838619742_eed855d8a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/280706862_14c30d734a.jpg: 480x640 1 dog, 1 frisbee, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 280706862_14c30d734a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3357416302_fcfcdd7b86.jpg: 448x640 1 person, 1 snowboard, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3357416302_fcfcdd7b86.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426144752_28d63615ca.jpg: 640x448 2 persons, 1 tie, 9.0ms\n",
      "Speed: 2.0ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3426144752_28d63615ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2410618963_fb78307d18.jpg: 640x448 1 person, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2410618963_fb78307d18.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3635911776_dbc2763f2c.jpg: 480x640 3 persons, 2 bowls, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3635911776_dbc2763f2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/390987167_2d5905b459.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 390987167_2d5905b459.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/396763804_3b7f1e12a8.jpg: 480x640 5 persons, 3 dogs, 1 frisbee, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 396763804_3b7f1e12a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3150440350_b0f2a9e774.jpg: 608x640 4 persons, 9.3ms\n",
      "Speed: 2.4ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3150440350_b0f2a9e774.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836703077_fa9c736203.jpg: 512x640 4 persons, 1 car, 1 truck, 1 chair, 9.4ms\n",
      "Speed: 2.4ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2836703077_fa9c736203.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549464203_8ab9c6160b.jpg: 544x640 2 persons, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3549464203_8ab9c6160b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3474330484_a01d8af624.jpg: 480x640 1 horse, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3474330484_a01d8af624.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1650280501_29810b46e5.jpg: 640x480 9 persons, 2 traffic lights, 1 chair, 1 laptop, 1 book, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1650280501_29810b46e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3608849440_e7d2bed29f.jpg: 480x640 3 persons, 2 cars, 1 sports ball, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3608849440_e7d2bed29f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3211577298_14296db6fd.jpg: 448x640 1 person, 1 surfboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3211577298_14296db6fd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2447972568_1e9b287691.jpg: 640x480 3 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2447972568_1e9b287691.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3530342993_a4a1f0e516.jpg: 448x640 13 persons, 3 bicycles, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3530342993_a4a1f0e516.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033741581_136889ac73.jpg: 640x448 1 person, 2 umbrellas, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3033741581_136889ac73.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636632926_09f39f2629.jpg: 448x640 3 persons, 1 skateboard, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3636632926_09f39f2629.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2218609886_892dcd6915.jpg: 640x448 1 person, 10.3ms\n",
      "Speed: 2.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2218609886_892dcd6915.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694064560_467683205b.jpg: 544x640 2 persons, 9.1ms\n",
      "Speed: 2.6ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3694064560_467683205b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/880220939_0ef1c37f1f.jpg: 480x640 12 persons, 1 handbag, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 880220939_0ef1c37f1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2701042060_92508ea8fa.jpg: 448x640 2 persons, 2 dogs, 1 horse, 1 sports ball, 13.0ms\n",
      "Speed: 3.3ms preprocess, 13.0ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2701042060_92508ea8fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2268115375_69884e958d.jpg: 640x480 1 person, 10.5ms\n",
      "Speed: 2.8ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2268115375_69884e958d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3435648640_b2f68efb78.jpg: 448x640 2 dogs, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3435648640_b2f68efb78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2058124718_89822bc96e.jpg: 448x640 2 dogs, 8.2ms\n",
      "Speed: 2.0ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2058124718_89822bc96e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2299859649_07ca44a222.jpg: 480x640 5 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2299859649_07ca44a222.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122606953_a979dd3d33.jpg: 480x640 2 dogs, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3122606953_a979dd3d33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3348785391_c243faf6bb.jpg: 512x640 8 persons, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3348785391_c243faf6bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3397310901_cbef5c06ef.jpg: 448x640 1 person, 1 skateboard, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3397310901_cbef5c06ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2172493537_128bc8b187.jpg: 448x640 3 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2172493537_128bc8b187.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3016200560_5bf8a70797.jpg: 640x640 1 dog, 1 sheep, 9.0ms\n",
      "Speed: 2.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3016200560_5bf8a70797.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/461019788_bc0993dabd.jpg: 640x544 2 persons, 8.8ms\n",
      "Speed: 2.5ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 461019788_bc0993dabd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437693401_202afef348.jpg: 448x640 2 dogs, 1 horse, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3437693401_202afef348.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3670075789_92ea9a183a.jpg: 480x640 3 persons, 1 boat, 1 bench, 1 surfboard, 1 potted plant, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3670075789_92ea9a183a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3747543364_bf5b548527.jpg: 448x640 4 persons, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3747543364_bf5b548527.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2888702775_0939a6680e.jpg: 448x640 6 persons, 1 skateboard, 1 potted plant, 8.0ms\n",
      "Speed: 2.0ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2888702775_0939a6680e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3607969989_68cc411493.jpg: 480x640 1 person, 1 skateboard, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3607969989_68cc411493.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2612608861_92beaa3d0b.jpg: 480x640 9 persons, 8.3ms\n",
      "Speed: 2.2ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2612608861_92beaa3d0b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3040575300_0e4328d205.jpg: 448x640 4 persons, 1 surfboard, 12.1ms\n",
      "Speed: 3.0ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3040575300_0e4328d205.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1771490732_0ab5f029ac.jpg: 480x640 3 persons, 1 sports ball, 1 baseball glove, 10.0ms\n",
      "Speed: 2.5ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1771490732_0ab5f029ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556598205_86c180769d.jpg: 448x640 1 person, 5 cars, 1 motorcycle, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3556598205_86c180769d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2415803492_56a673dc25.jpg: 640x480 9 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2415803492_56a673dc25.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1138784872_69ade3f2ab.jpg: 480x640 3 persons, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1138784872_69ade3f2ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3274691778_94bb57bba3.jpg: 480x640 3 persons, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3274691778_94bb57bba3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2108799322_e25aa6e185.jpg: 480x640 1 dog, 1 frisbee, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2108799322_e25aa6e185.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2949497756_be8e58e6bd.jpg: 448x640 2 dogs, 1 horse, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2949497756_be8e58e6bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2933912528_52b05f84a1.jpg: 480x640 2 dogs, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2933912528_52b05f84a1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2208055895_37cd8e1edf.jpg: 448x640 1 person, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2208055895_37cd8e1edf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091921457_83eee69591.jpg: 416x640 5 persons, 8.9ms\n",
      "Speed: 1.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3091921457_83eee69591.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2567035103_3511020c8f.jpg: 480x640 2 dogs, 8.8ms\n",
      "Speed: 2.2ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2567035103_3511020c8f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/95734035_84732a92c1.jpg: 480x640 2 persons, 1 motorcycle, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 95734035_84732a92c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694093650_547259731e.jpg: 480x640 7 persons, 8.1ms\n",
      "Speed: 2.1ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3694093650_547259731e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1404832008_68e432665b.jpg: 640x448 5 persons, 8.9ms\n",
      "Speed: 2.1ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1404832008_68e432665b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/111497985_38e9f88856.jpg: 640x480 1 person, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 111497985_38e9f88856.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3653462288_bfe2360a64.jpg: 640x448 5 persons, 1 bicycle, 1 handbag, 2 skateboards, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3653462288_bfe2360a64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3374722123_6fe6fef449.jpg: 640x480 2 persons, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3374722123_6fe6fef449.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2939475047_84585ea45c.jpg: 448x640 4 persons, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2939475047_84585ea45c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3399944164_ec24123945.jpg: 448x640 4 persons, 1 couch, 1 bed, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3399944164_ec24123945.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429194423_98e911a101.jpg: 544x640 2 persons, 1 skis, 7.7ms\n",
      "Speed: 2.3ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3429194423_98e911a101.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3438858409_136345fa07.jpg: 608x640 4 persons, 1 car, 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3438858409_136345fa07.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602415701_7674eb19e4.jpg: 640x448 6 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2602415701_7674eb19e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3000428313_8a1e65e20e.jpg: 448x640 2 persons, 1 surfboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3000428313_8a1e65e20e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2244024374_54d7e88c2b.jpg: 448x640 2 sheeps, 2 cows, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2244024374_54d7e88c2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3549006919_3604bc813e.jpg: 640x480 2 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3549006919_3604bc813e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/256444892_efcb3bd824.jpg: 480x640 17 persons, 3 cars, 3 handbags, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 256444892_efcb3bd824.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2507182524_7e83c6de82.jpg: 448x640 1 dog, 1 sports ball, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2507182524_7e83c6de82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3048461682_e89f81b1c7.jpg: 640x512 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3048461682_e89f81b1c7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3238654429_d899e34287.jpg: 448x640 1 person, 1 snowboard, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3238654429_d899e34287.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2620113705_a8fa89b8f6.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2620113705_a8fa89b8f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3149038044_c7c94688c6.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3149038044_c7c94688c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372340429_91c4f4af30.jpg: 448x640 3 dogs, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3372340429_91c4f4af30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2860202109_97b2b22652.jpg: 448x640 3 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2860202109_97b2b22652.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/315436114_6d386b8c36.jpg: 480x640 2 dogs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 315436114_6d386b8c36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2085078076_b9db242d21.jpg: 480x640 1 person, 1 bowl, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2085078076_b9db242d21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2772084628_c0ae29d87a.jpg: 640x576 4 persons, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2772084628_c0ae29d87a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1566117559_f5d98fbeb0.jpg: 448x640 1 dog, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1566117559_f5d98fbeb0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3069216757_c419b3898e.jpg: 544x640 1 person, 1 potted plant, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3069216757_c419b3898e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/307301755_48919ef1b2.jpg: 448x640 1 person, 2 chairs, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 307301755_48919ef1b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2559114800_17310f3015.jpg: 512x640 3 dogs, 7.9ms\n",
      "Speed: 2.0ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2559114800_17310f3015.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3563924606_5914392cd8.jpg: 448x640 2 dogs, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3563924606_5914392cd8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2490365757_b869282cb3.jpg: 512x640 2 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2490365757_b869282cb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2551344688_17a12a6948.jpg: 448x640 2 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2551344688_17a12a6948.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2885387575_9127ea10f1.jpg: 576x640 2 dogs, 1 sports ball, 7.1ms\n",
      "Speed: 2.1ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2885387575_9127ea10f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1763020597_d4cc8f0f8a.jpg: 640x448 5 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1763020597_d4cc8f0f8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1211015912_9f3ee3a995.jpg: 448x640 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1211015912_9f3ee3a995.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2092177624_13ab757e8b.jpg: 640x448 1 person, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2092177624_13ab757e8b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1468962616_5803b4397f.jpg: 640x512 3 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1468962616_5803b4397f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2136514643_93d8f75a77.jpg: 640x640 2 persons, 2 handbags, 7.6ms\n",
      "Speed: 2.4ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2136514643_93d8f75a77.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3119875880_22f9129a1c.jpg: 640x448 2 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3119875880_22f9129a1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2201978994_c444e64810.jpg: 480x640 2 persons, 1 chair, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2201978994_c444e64810.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/531197115_2be4d5034b.jpg: 640x640 2 persons, 1 car, 1 potted plant, 1 dining table, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 531197115_2be4d5034b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661294481_b86058b504.jpg: 480x640 14 persons, 3 traffic lights, 2 umbrellas, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2661294481_b86058b504.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3456251289_c4ae31d817.jpg: 512x640 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3456251289_c4ae31d817.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3285180819_a9712fd2bc.jpg: 640x512 9 persons, 1 tie, 1 frisbee, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3285180819_a9712fd2bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3188036349_8e4e2d6ca8.jpg: 448x640 2 persons, 1 airplane, 1 bench, 7.8ms\n",
      "Speed: 1.5ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3188036349_8e4e2d6ca8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/207930963_af3a2f1784.jpg: 480x640 3 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 207930963_af3a2f1784.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1732217138_aa0199ef87.jpg: 640x512 3 persons, 1 bowl, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1732217138_aa0199ef87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/888517718_3d5b4b7b43.jpg: 640x448 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 888517718_3d5b4b7b43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/172092464_d9eb4f4f2f.jpg: 640x480 1 person, 1 boat, 1 surfboard, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 172092464_d9eb4f4f2f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2059842472_f4fb61ea08.jpg: 640x480 1 person, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2059842472_f4fb61ea08.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1784309115_0ad6791146.jpg: 448x640 7 persons, 1 backpack, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1784309115_0ad6791146.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174156702_95a1cda2d9.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3174156702_95a1cda2d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2520909293_9bb7f7364e.jpg: 480x640 1 person, 2 bicycles, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2520909293_9bb7f7364e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3091912922_0d6ebc8f6a.jpg: 448x640 1 bench, 1 dog, 1 sports ball, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3091912922_0d6ebc8f6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2287023569_fd7a9c60b8.jpg: 640x480 1 person, 1 dog, 8.4ms\n",
      "Speed: 1.7ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2287023569_fd7a9c60b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3085973779_29f44fbdaa.jpg: 640x384 1 person, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Cropped images saved for 3085973779_29f44fbdaa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2112661738_de71b60b88.jpg: 480x640 1 sheep, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2112661738_de71b60b88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/616177206_0e16c33f6b.jpg: 640x416 9 persons, 7.9ms\n",
      "Speed: 1.5ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 616177206_0e16c33f6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3017521547_f5ef8848e3.jpg: 512x640 3 persons, 1 sports ball, 1 baseball bat, 1 tennis racket, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3017521547_f5ef8848e3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2245914678_1f82fc3d80.jpg: 640x480 1 person, 2 cell phones, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2245914678_1f82fc3d80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535084928_858544f49a.jpg: 640x448 1 dog, 1 sports ball, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3535084928_858544f49a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2956895529_ec6275060e.jpg: 448x640 1 person, 1 bicycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2956895529_ec6275060e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2747640247_b54bfa6886.jpg: 448x640 1 person, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2747640247_b54bfa6886.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3123770450_cedc16d162.jpg: 448x640 5 birds, 1 dog, 1 sheep, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3123770450_cedc16d162.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3551447084_becc6a4666.jpg: 640x448 6 persons, 1 motorcycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3551447084_becc6a4666.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3453284877_8866189055.jpg: 448x640 1 person, 1 snowboard, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3453284877_8866189055.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1773928579_5664a810dc.jpg: 448x640 18 persons, 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1773928579_5664a810dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2474047296_fd9179d438.jpg: 480x640 1 person, 8.0ms\n",
      "Speed: 1.9ms preprocess, 8.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2474047296_fd9179d438.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3728164558_52729baefa.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3728164558_52729baefa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3537920947_6c5a956f47.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3537920947_6c5a956f47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247693965_845b3b4349.jpg: 512x640 2 dogs, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3247693965_845b3b4349.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3372167201_f7f909d480.jpg: 480x640 9 persons, 4 benchs, 1 bottle, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3372167201_f7f909d480.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2559503010_84f20b3bc9.jpg: 480x640 1 person, 1 car, 2 potted plants, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2559503010_84f20b3bc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1977827746_4e13d7e19f.jpg: 640x576 2 persons, 1 carrot, 8.0ms\n",
      "Speed: 2.3ms preprocess, 8.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 1977827746_4e13d7e19f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2848266893_9693c66275.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2848266893_9693c66275.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3157039116_d82da4e66b.jpg: 640x448 1 person, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3157039116_d82da4e66b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3256603992_67312b5a36.jpg: 480x640 1 person, 8.2ms\n",
      "Speed: 1.9ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3256603992_67312b5a36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3561130207_d1ed166daa.jpg: 384x640 6 persons, 1 handbag, 8.6ms\n",
      "Speed: 1.4ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3561130207_d1ed166daa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3165123595_8db9f918bf.jpg: 512x640 4 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3165123595_8db9f918bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2291485126_b8d41a63f4.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2291485126_b8d41a63f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3622216490_1314a58b66.jpg: 640x640 2 persons, 1 skateboard, 7.6ms\n",
      "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3622216490_1314a58b66.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3288274849_07ff76ee93.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3288274849_07ff76ee93.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3710971182_cb01c97d15.jpg: 480x640 5 persons, 1 car, 2 baseball bats, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3710971182_cb01c97d15.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189307452_aebc12380b.jpg: 512x640 1 cow, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3189307452_aebc12380b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2279496715_8ef3ee6edb.jpg: 544x640 2 persons, 7.8ms\n",
      "Speed: 2.0ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2279496715_8ef3ee6edb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1093737381_b313cd49ff.jpg: 640x480 2 persons, 1 kite, 1 potted plant, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1093737381_b313cd49ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215315009_47577bf8f7.jpg: 640x448 1 person, 1 snowboard, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3215315009_47577bf8f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/432496659_f01464d9fb.jpg: 448x640 1 dog, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 432496659_f01464d9fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/219301555_17883a51bd.jpg: 640x448 2 persons, 13.4ms\n",
      "Speed: 3.4ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 219301555_17883a51bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636543173_15f56515e5.jpg: 448x640 4 persons, 1 tie, 1 cup, 11.8ms\n",
      "Speed: 2.8ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3636543173_15f56515e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2176147758_9a8deba576.jpg: 544x640 1 person, 2 bicycles, 3 cars, 2 trucks, 2 traffic lights, 10.0ms\n",
      "Speed: 2.8ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2176147758_9a8deba576.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3046916429_8e2570b613.jpg: 480x640 2 persons, 1 chair, 9.6ms\n",
      "Speed: 2.2ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3046916429_8e2570b613.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/262570082_6364f58f33.jpg: 480x640 2 dogs, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 262570082_6364f58f33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3437315443_ba2263f92e.jpg: 640x448 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3437315443_ba2263f92e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2932519416_11f23b6297.jpg: 640x448 1 person, 1 bicycle, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2932519416_11f23b6297.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3027365101_3818be6e16.jpg: 448x640 1 person, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3027365101_3818be6e16.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/429283612_37f6e7fb7f.jpg: 448x640 2 persons, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 429283612_37f6e7fb7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/478592803_f57cc9c461.jpg: 448x640 5 persons, 2 bicycles, 4 cars, 1 motorcycle, 1 bus, 1 truck, 8.4ms\n",
      "Speed: 2.1ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 478592803_f57cc9c461.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/487071033_27e460a1b9.jpg: 640x640 14 persons, 1 bicycle, 1 backpack, 1 skateboard, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 487071033_27e460a1b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/225699652_53f6fb33cd.jpg: 512x640 1 person, 2 birds, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 225699652_53f6fb33cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/435739506_2daf7f4887.jpg: 512x640 1 dog, 8.4ms\n",
      "Speed: 2.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 435739506_2daf7f4887.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3426962078_13e87e10de.jpg: 416x640 9 persons, 9.8ms\n",
      "Speed: 1.9ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3426962078_13e87e10de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3343197133_9256848fa9.jpg: 448x640 3 persons, 1 kite, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3343197133_9256848fa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174417550_d2e6100278.jpg: 640x480 1 person, 1 snowboard, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3174417550_d2e6100278.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1253275679_e955fb7304.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1253275679_e955fb7304.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/409327234_7b29eecb4e.jpg: 448x640 1 person, 1 handbag, 3 chairs, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 409327234_7b29eecb4e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/114051287_dd85625a04.jpg: 448x640 1 person, 1 car, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 114051287_dd85625a04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627290893_561e176e80.jpg: 448x640 4 persons, 1 cell phone, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3627290893_561e176e80.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534824784_7133119316.jpg: 448x640 3 persons, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3534824784_7133119316.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3138746531_f6b816c126.jpg: 448x640 5 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3138746531_f6b816c126.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341084434_db5e7d1fdc.jpg: 448x640 1 person, 1 skis, 6.5ms\n",
      "Speed: 1.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3341084434_db5e7d1fdc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3324375078_9441f72898.jpg: 448x640 1 dog, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3324375078_9441f72898.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/134724228_30408cd77f.jpg: 416x640 3 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 134724228_30408cd77f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3727740053_3baa94ffcb.jpg: 448x640 9 persons, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3727740053_3baa94ffcb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3498482871_4e02f31c35.jpg: 640x480 7 persons, 2 cows, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3498482871_4e02f31c35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3196100539_102fe877b3.jpg: 512x640 10 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3196100539_102fe877b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867736861_43c9487a65.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2867736861_43c9487a65.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3417672954_46b75dea8d.jpg: 640x448 1 person, 1 skateboard, 7.8ms\n",
      "Speed: 1.6ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3417672954_46b75dea8d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3386953179_84c2d7922f.jpg: 480x640 2 persons, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3386953179_84c2d7922f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225880532_c8d5d1d798.jpg: 640x448 2 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3225880532_c8d5d1d798.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3330007895_78303e8a40.jpg: 640x416 3 persons, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3330007895_78303e8a40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3627011534_485f667b10.jpg: 640x608 1 person, 8.1ms\n",
      "Speed: 2.5ms preprocess, 8.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3627011534_485f667b10.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1448511770_1a4a9c453b.jpg: 352x640 1 bird, 8.6ms\n",
      "Speed: 1.2ms preprocess, 8.6ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 1448511770_1a4a9c453b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2036407732_d5a0389bba.jpg: 480x640 5 persons, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2036407732_d5a0389bba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3222842866_cb85243ce4.jpg: 448x640 1 bird, 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3222842866_cb85243ce4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636796219_9916c0465a.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3636796219_9916c0465a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1446053356_a924b4893f.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1446053356_a924b4893f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3676432043_0ca418b861.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3676432043_0ca418b861.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2265100168_175f8218af.jpg: 448x640 4 persons, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2265100168_175f8218af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/210839948_bbd5bfa3b6.jpg: 448x640 1 person, 6.7ms\n",
      "Speed: 1.5ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 210839948_bbd5bfa3b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3035949542_cb249790f5.jpg: 448x640 5 persons, 1 frisbee, 1 sports ball, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3035949542_cb249790f5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3264337159_e1680a35ba.jpg: 640x640 1 bear, 7.0ms\n",
      "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3264337159_e1680a35ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547647914_4dd56a8c1b.jpg: 448x640 16 persons, 1 car, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3547647914_4dd56a8c1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/214501174_6db1f4d69c.jpg: 640x448 1 dog, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 214501174_6db1f4d69c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/425088533_a460dc4617.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 425088533_a460dc4617.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3145869775_85dfae43bd.jpg: 640x448 5 persons, 1 car, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3145869775_85dfae43bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/405331006_4e94e07698.jpg: 512x640 1 person, 1 horse, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 405331006_4e94e07698.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/223299137_b0e81ac145.jpg: 448x640 4 persons, 1 surfboard, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 223299137_b0e81ac145.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/422756764_e7eaac76bf.jpg: 448x640 1 dog, 1 sports ball, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 422756764_e7eaac76bf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/856985136_649c0a3881.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 856985136_649c0a3881.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/273248777_eaf0288ab3.jpg: 640x480 4 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 273248777_eaf0288ab3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485921585_1974b1577a.jpg: 640x640 17 persons, 2 bicycles, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 485921585_1974b1577a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/278496691_c1fd93e2d8.jpg: 480x640 15 persons, 1 sports ball, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 278496691_c1fd93e2d8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2873648844_8efc7d78f1.jpg: 480x640 1 dog, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2873648844_8efc7d78f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/143552829_72b6ba49d4.jpg: 640x640 1 person, 1 motorcycle, 7.2ms\n",
      "Speed: 2.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 143552829_72b6ba49d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/197924859_f6e39a7dfa.jpg: 640x480 (no detections), 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 0.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 197924859_f6e39a7dfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2519594430_551225e5bd.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2519594430_551225e5bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2966190737_ceb6eb4b53.jpg: 448x640 3 persons, 2 baseball bats, 1 baseball glove, 7.9ms\n",
      "Speed: 1.6ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2966190737_ceb6eb4b53.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1178705300_c224d9a4f1.jpg: 448x640 1 boat, 1 dog, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1178705300_c224d9a4f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347300_7c84ecf764.jpg: 640x448 8 persons, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347300_7c84ecf764.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1299459562_ed0e064aee.jpg: 480x640 3 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1299459562_ed0e064aee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/929679367_ff8c7df2ee.jpg: 640x480 1 dog, 1 frisbee, 1 sports ball, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 929679367_ff8c7df2ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2444339090_bf7b3211f4.jpg: 448x640 1 dog, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2444339090_bf7b3211f4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535372414_4c51c86fc4.jpg: 608x640 2 persons, 8.1ms\n",
      "Speed: 2.5ms preprocess, 8.1ms inference, 1.2ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3535372414_4c51c86fc4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3094317837_b31cbf969e.jpg: 448x640 1 person, 1 surfboard, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3094317837_b31cbf969e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3070713991_8696796937.jpg: 448x640 4 persons, 7.1ms\n",
      "Speed: 1.9ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3070713991_8696796937.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3081363964_d404eccae8.jpg: 416x640 1 dog, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3081363964_d404eccae8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/418796494_bdb441de42.jpg: 640x448 1 dog, 1 bed, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 418796494_bdb441de42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3347798761_5c5260b000.jpg: 640x448 1 person, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3347798761_5c5260b000.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2453971388_76616b6a82.jpg: 480x640 6 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2453971388_76616b6a82.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856923934_6eb8832c9a.jpg: 352x640 1 person, 1 boat, 1 surfboard, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2856923934_6eb8832c9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2657301826_aab4c36e6c.jpg: 640x448 1 person, 1 couch, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2657301826_aab4c36e6c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3526018344_450c517a72.jpg: 448x640 1 person, 5 dogs, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3526018344_450c517a72.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2869009633_ea3cafd437.jpg: 640x448 4 persons, 2 boats, 1 sports ball, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2869009633_ea3cafd437.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2944193661_7b255af9cc.jpg: 448x640 1 person, 1 bench, 1 chair, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2944193661_7b255af9cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2524084967_a5e011b73d.jpg: 640x448 1 person, 1 bicycle, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2524084967_a5e011b73d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3235076435_1eaa40bd0a.jpg: 480x640 1 person, 1 surfboard, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3235076435_1eaa40bd0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1034276567_49bb87c51c.jpg: 448x640 1 person, 1 pizza, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1034276567_49bb87c51c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3606084228_6286a52875.jpg: 640x480 19 persons, 1 motorcycle, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3606084228_6286a52875.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460797929_66446c13db.jpg: 640x480 9 persons, 1 boat, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2460797929_66446c13db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/416992999_654a99a903.jpg: 480x640 2 persons, 1 car, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 416992999_654a99a903.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2414986483_004936f84b.jpg: 640x480 4 persons, 1 suitcase, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2414986483_004936f84b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2527163162_d0fb802992.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2527163162_d0fb802992.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2629445284_83390e83af.jpg: 480x640 1 person, 1 car, 1 dog, 1 umbrella, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2629445284_83390e83af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3327563443_870a33f748.jpg: 512x640 2 persons, 1 umbrella, 7.6ms\n",
      "Speed: 2.0ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3327563443_870a33f748.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331102049_bc65cf6198.jpg: 480x640 1 bird, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3331102049_bc65cf6198.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/136552115_6dc3e7231c.jpg: 480x640 2 persons, 1 bicycle, 2 motorcycles, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 136552115_6dc3e7231c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2899501488_90d5da5474.jpg: 448x640 8 persons, 4 motorcycles, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2899501488_90d5da5474.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2595713720_30534e8de2.jpg: 480x640 1 person, 1 sheep, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2595713720_30534e8de2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3634281981_d9cf1d1a33.jpg: 448x640 1 person, 1 bicycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3634281981_d9cf1d1a33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3584406900_039f30b34c.jpg: 448x640 3 persons, 1 car, 1 truck, 1 kite, 6.6ms\n",
      "Speed: 1.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3584406900_039f30b34c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2641770481_c98465ff35.jpg: 448x640 1 person, 1 boat, 1 surfboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2641770481_c98465ff35.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2393196444_8f4f540f5f.jpg: 640x480 4 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2393196444_8f4f540f5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3328247381_a9f7fb4898.jpg: 416x640 2 persons, 2 surfboards, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3328247381_a9f7fb4898.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3595643050_d312e4b652.jpg: 448x640 2 dogs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3595643050_d312e4b652.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3398746625_5199beea71.jpg: 448x640 1 person, 1 bird, 1 kite, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3398746625_5199beea71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484576025_a8c50942aa.jpg: 480x640 1 dog, 1 potted plant, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3484576025_a8c50942aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3336759846_5220e27deb.jpg: 640x448 3 persons, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3336759846_5220e27deb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/710878348_323082babd.jpg: 480x640 6 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 710878348_323082babd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2724485630_7d2452df00.jpg: 640x640 6 persons, 7.2ms\n",
      "Speed: 2.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2724485630_7d2452df00.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/107318069_e9f2ef32de.jpg: 480x640 11 persons, 1 umbrella, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 107318069_e9f2ef32de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244910177_7c4ec3f65b.jpg: 640x640 5 persons, 1 handbag, 1 cup, 7.3ms\n",
      "Speed: 2.2ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 244910177_7c4ec3f65b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2696060728_3043cfc38c.jpg: 448x640 1 person, 1 surfboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2696060728_3043cfc38c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/98377566_e4674d1ebd.jpg: 640x608 1 dog, 7.4ms\n",
      "Speed: 2.2ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 98377566_e4674d1ebd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2867968184_908d87cf2c.jpg: 640x448 1 person, 3 skateboards, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2867968184_908d87cf2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460195978_fc522a4979.jpg: 480x640 1 person, 1 dog, 1 kite, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 460195978_fc522a4979.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3084380974_268a0f9236.jpg: 448x640 1 person, 1 surfboard, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3084380974_268a0f9236.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2461990494_c5ece064cc.jpg: 416x640 8 persons, 2 umbrellas, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2461990494_c5ece064cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3174431688_ae84778db0.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3174431688_ae84778db0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3681575323_433d007650.jpg: 640x480 4 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3681575323_433d007650.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3517362674_0f5296de19.jpg: 640x640 1 person, 1 baseball bat, 6.9ms\n",
      "Speed: 2.1ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3517362674_0f5296de19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2362481035_a7600875d0.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2362481035_a7600875d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562001359_65c63aeda3.jpg: 448x640 1 person, 1 boat, 1 backpack, 1 surfboard, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3562001359_65c63aeda3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2869491449_1041485a6b.jpg: 448x640 1 dog, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2869491449_1041485a6b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3603116579_4a28a932e2.jpg: 640x608 1 person, 1 sports ball, 1 baseball glove, 7.9ms\n",
      "Speed: 2.3ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3603116579_4a28a932e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/822836318_21544f0f78.jpg: 480x640 1 person, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 822836318_21544f0f78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2581066814_179d28f306.jpg: 512x640 14 persons, 4 bicycles, 3 buss, 1 traffic light, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2581066814_179d28f306.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3368671163_0171259581.jpg: 448x640 1 bench, 3 dogs, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3368671163_0171259581.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2286239223_d84ffc4e4a.jpg: 384x640 5 persons, 1 car, 1 motorcycle, 1 bench, 8.4ms\n",
      "Speed: 1.4ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2286239223_d84ffc4e4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3451523035_b61d79f6a8.jpg: 448x640 1 horse, 1 cow, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3451523035_b61d79f6a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2257798999_d9d1b9a45a.jpg: 480x640 3 dogs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2257798999_d9d1b9a45a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2416964653_db3c2b6a0e.jpg: 480x640 2 persons, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2416964653_db3c2b6a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3319489465_c65c91e4f2.jpg: 448x640 8 persons, 1 horse, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3319489465_c65c91e4f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2709648336_15455e60b2.jpg: 512x640 2 persons, 1 dog, 1 frisbee, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2709648336_15455e60b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3058627443_1d57ff0a2c.jpg: 448x640 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3058627443_1d57ff0a2c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/662606040_8cc8cd9f1b.jpg: 640x480 1 dog, 1 sports ball, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 662606040_8cc8cd9f1b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2968182121_b3b491df85.jpg: 448x640 1 dog, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2968182121_b3b491df85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/989851184_9ef368e520.jpg: 448x640 2 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 989851184_9ef368e520.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3381392182_db2c42430e.jpg: 640x512 1 person, 1 kite, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3381392182_db2c42430e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/224702242_a62aaa6dff.jpg: 480x640 1 person, 1 bench, 1 dog, 4 potted plants, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 224702242_a62aaa6dff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/763577068_4b96ed768b.jpg: 512x640 1 person, 1 dog, 7.6ms\n",
      "Speed: 1.9ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 763577068_4b96ed768b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/232193739_ed5f348c7a.jpg: 640x448 1 person, 1 car, 2 skateboards, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 232193739_ed5f348c7a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3535879138_9281dc83d5.jpg: 640x480 11 persons, 2 bicycles, 3 backpacks, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3535879138_9281dc83d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/393810324_1c33760a95.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 393810324_1c33760a95.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1056873310_49c665eb22.jpg: 480x640 3 dogs, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1056873310_49c665eb22.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677927146_1696f0b075.jpg: 448x640 5 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677927146_1696f0b075.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/460478198_83039f2593.jpg: 416x640 1 dog, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 460478198_83039f2593.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/559102835_472ff702b5.jpg: 640x480 3 persons, 1 car, 2 sports balls, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 559102835_472ff702b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2101128963_fdf8b2a0d7.jpg: 448x640 4 persons, 1 baseball glove, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2101128963_fdf8b2a0d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533484468_0787830d49.jpg: 640x448 7 persons, 1 traffic light, 1 cell phone, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3533484468_0787830d49.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244870123_dcb6e53643.jpg: 480x640 2 dogs, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 244870123_dcb6e53643.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3541915243_956c1aa8ef.jpg: 640x480 13 persons, 1 cell phone, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3541915243_956c1aa8ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2131762850_5293a288d9.jpg: 448x640 1 person, 1 tie, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2131762850_5293a288d9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3103587323_7f093d5b90.jpg: 480x640 1 person, 1 bear, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3103587323_7f093d5b90.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3515451715_ac5ac04efa.jpg: 640x640 3 persons, 7.7ms\n",
      "Speed: 2.4ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 3515451715_ac5ac04efa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1089755335_0bfbfd30e6.jpg: 448x640 6 persons, 3 handbags, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1089755335_0bfbfd30e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2467856402_0490413d38.jpg: 448x640 2 persons, 1 orange, 1 cell phone, 6.3ms\n",
      "Speed: 1.5ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2467856402_0490413d38.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2469498117_b4543e1460.jpg: 448x640 1 person, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2469498117_b4543e1460.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2749124446_d4432787b5.jpg: 640x448 1 bird, 1 dog, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2749124446_d4432787b5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3384314832_dffc944152.jpg: 512x640 1 bench, 1 dog, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3384314832_dffc944152.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2346189044_546ed84aa9.jpg: 544x640 5 persons, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2346189044_546ed84aa9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1527513023_3d8152b379.jpg: 448x640 4 persons, 1 surfboard, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1527513023_3d8152b379.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3472364264_dbde5a8d0a.jpg: 480x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3472364264_dbde5a8d0a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/975131015_9acd25db9c.jpg: 448x640 2 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 975131015_9acd25db9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2384728877_48c85d58af.jpg: 640x480 4 persons, 3 cars, 1 handbag, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2384728877_48c85d58af.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/968081289_cdba83ce2e.jpg: 640x576 2 persons, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 968081289_cdba83ce2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/263522013_d118d46b2d.jpg: 640x576 1 bear, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 263522013_d118d46b2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3095225232_2e6e6dc92e.jpg: 640x480 4 persons, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3095225232_2e6e6dc92e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2472896179_245e7d142f.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2472896179_245e7d142f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2554531876_5d7f193992.jpg: 480x640 1 motorcycle, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2554531876_5d7f193992.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3569284680_44fef444ef.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3569284680_44fef444ef.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2151300603_248a9fe715.jpg: 448x640 1 person, 1 surfboard, 1 chair, 1 dining table, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2151300603_248a9fe715.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1016887272_03199f49c4.jpg: 640x448 5 persons, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1016887272_03199f49c4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/391723162_3bdeb7ea33.jpg: 480x640 1 dog, 2 beds, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 391723162_3bdeb7ea33.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/186890601_8a6b0f1769.jpg: 480x640 1 person, 1 dog, 1 kite, 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 186890601_8a6b0f1769.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2309327462_82a24538d4.jpg: 448x640 2 dogs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2309327462_82a24538d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2249264723_d08655d9f2.jpg: 640x448 1 person, 1 couch, 1 bed, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2249264723_d08655d9f2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1562478333_43d13e5427.jpg: 480x640 1 person, 1 boat, 1 backpack, 1 kite, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1562478333_43d13e5427.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/987907964_5a06a63609.jpg: 352x640 2 dogs, 8.3ms\n",
      "Speed: 1.2ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 987907964_5a06a63609.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3547600292_6f8aac7f2e.jpg: 512x640 1 bird, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3547600292_6f8aac7f2e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2800990525_a1f8427272.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2800990525_a1f8427272.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2574194729_1f099647ee.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2574194729_1f099647ee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2914206497_5e36ac6324.jpg: 448x640 1 car, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2914206497_5e36ac6324.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1119015538_e8e796281e.jpg: 416x640 1 dog, 1 cow, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1119015538_e8e796281e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2713554148_64cd465e71.jpg: 640x544 4 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2713554148_64cd465e71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2309860995_c2e2a0feeb.jpg: 640x416 2 persons, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2309860995_c2e2a0feeb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3501206996_477be0f318.jpg: 448x640 1 person, 1 sports ball, 6.8ms\n",
      "Speed: 1.5ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3501206996_477be0f318.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2730994020_64ac1d18be.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2730994020_64ac1d18be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2913207978_9e9624e249.jpg: 448x640 2 persons, 1 motorcycle, 1 bus, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2913207978_9e9624e249.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2274602044_b3d55df235.jpg: 416x640 1 person, 2 benchs, 2 dogs, 6.9ms\n",
      "Speed: 1.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2274602044_b3d55df235.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3335501468_628655d608.jpg: 448x640 1 bird, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3335501468_628655d608.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/549887636_0ea5ae4739.jpg: 480x640 3 persons, 4 cars, 1 handbag, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 549887636_0ea5ae4739.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3239021459_a6b71bb400.jpg: 480x640 7 persons, 1 snowboard, 6.7ms\n",
      "Speed: 1.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3239021459_a6b71bb400.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3403797144_53e49412ec.jpg: 480x640 1 person, 1 surfboard, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3403797144_53e49412ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3133044777_8cc930a4ec.jpg: 448x640 1 person, 4 cars, 5 bananas, 3 apples, 1 clock, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3133044777_8cc930a4ec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2170187328_65c2f11891.jpg: 480x640 2 persons, 2 surfboards, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2170187328_65c2f11891.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/377872472_35805fc143.jpg: 544x640 3 dogs, 7.5ms\n",
      "Speed: 2.1ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 377872472_35805fc143.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1019604187_d087bf9a5f.jpg: 448x640 2 cars, 1 dog, 1 sports ball, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1019604187_d087bf9a5f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3085357792_efcf297c71.jpg: 480x640 8 persons, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3085357792_efcf297c71.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3331009729_d3b14738e6.jpg: 576x640 2 persons, 2 backpacks, 1 handbag, 1 vase, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3331009729_d3b14738e6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1289142574_2bd6a082dd.jpg: 480x640 1 person, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1289142574_2bd6a082dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2723477522_d89f5ac62b.jpg: 384x640 2 dogs, 8.2ms\n",
      "Speed: 1.4ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2723477522_d89f5ac62b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3287236038_8998e6b82f.jpg: 448x640 1 person, 1 bench, 1 potted plant, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3287236038_8998e6b82f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/339822505_be3ccbb71f.jpg: 448x640 6 persons, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 339822505_be3ccbb71f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502905671_c6039804ab.jpg: 448x640 1 person, 1 dog, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2502905671_c6039804ab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3247341210_5d1e50df23.jpg: 480x640 17 persons, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3247341210_5d1e50df23.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2300920203_f29260b1db.jpg: 480x640 1 dog, 2 chairs, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2300920203_f29260b1db.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/751109943_2a7f8e117f.jpg: 640x448 1 person, 1 bench, 7.2ms\n",
      "Speed: 1.5ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 751109943_2a7f8e117f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3376942201_2c45d99237.jpg: 448x640 1 person, 1 cat, 1 dog, 1 chair, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3376942201_2c45d99237.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/416788726_5b4eb1466e.jpg: 512x640 11 persons, 1 surfboard, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 416788726_5b4eb1466e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3562169000_6aa7f1043d.jpg: 448x640 1 car, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3562169000_6aa7f1043d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/109671650_f7bbc297fa.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 109671650_f7bbc297fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2846037553_1a1de50709.jpg: 640x352 1 person, 8.7ms\n",
      "Speed: 1.3ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 352)\n",
      "Cropped images saved for 2846037553_1a1de50709.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/493542985_c85ad29ebe.jpg: 640x640 2 persons, 7.2ms\n",
      "Speed: 2.2ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 493542985_c85ad29ebe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/311267421_e204e643cf.jpg: 480x640 2 cows, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 311267421_e204e643cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1144288288_e5c9558b6a.jpg: 640x448 2 persons, 1 car, 2 potted plants, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1144288288_e5c9558b6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3294717824_3bb7b5d1c8.jpg: 640x512 1 person, 1 surfboard, 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3294717824_3bb7b5d1c8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/146577646_9e64b8c2dc.jpg: 416x640 1 person, 2 surfboards, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 146577646_9e64b8c2dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2522230304_1581d52961.jpg: 640x480 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2522230304_1581d52961.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/458183774_afe65abf67.jpg: 448x640 1 person, 1 bird, 1 dog, 6.9ms\n",
      "Speed: 1.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 458183774_afe65abf67.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3326249355_e7a7c71f06.jpg: 448x640 2 persons, 7.0ms\n",
      "Speed: 1.7ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3326249355_e7a7c71f06.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3260191163_6c1551eee8.jpg: 352x640 3 dogs, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3260191163_6c1551eee8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241347611_cb265be138.jpg: 640x448 7 persons, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 241347611_cb265be138.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225296260_2ee72b4917.jpg: 448x640 1 car, 1 truck, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3225296260_2ee72b4917.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3634032601_2236676cdd.jpg: 608x640 15 persons, 7.1ms\n",
      "Speed: 2.2ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3634032601_2236676cdd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568197730_a071d7595b.jpg: 640x512 3 persons, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3568197730_a071d7595b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2969380952_9f1eb7f93b.jpg: 480x640 7 persons, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2969380952_9f1eb7f93b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/530661899_94655d7d0e.jpg: 416x640 2 persons, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 530661899_94655d7d0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2095444126_201ff9f222.jpg: 640x448 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2095444126_201ff9f222.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3121482932_f77ca12c01.jpg: 448x640 1 person, 1 motorcycle, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3121482932_f77ca12c01.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3694555931_7807db2fb4.jpg: 640x576 1 person, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3694555931_7807db2fb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090327868_9f99e2740d.jpg: 480x640 4 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2090327868_9f99e2740d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421932359_edbf181f44.jpg: 640x640 1 dog, 15.1ms\n",
      "Speed: 4.7ms preprocess, 15.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 421932359_edbf181f44.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3369354061_2bab79f91f.jpg: 640x448 1 person, 1 handbag, 18.7ms\n",
      "Speed: 4.1ms preprocess, 18.7ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3369354061_2bab79f91f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3648081498_76ec091495.jpg: 640x448 1 person, 1 skateboard, 12.0ms\n",
      "Speed: 3.2ms preprocess, 12.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3648081498_76ec091495.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2652522323_9218afd8c2.jpg: 448x640 5 persons, 1 sports ball, 10.7ms\n",
      "Speed: 2.5ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2652522323_9218afd8c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3518675890_2f65e23ff9.jpg: 448x640 1 dog, 11.9ms\n",
      "Speed: 3.4ms preprocess, 11.9ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3518675890_2f65e23ff9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1288909046_d2b2b62607.jpg: 384x640 1 dog, 1 bear, 1 frisbee, 4 potted plants, 11.4ms\n",
      "Speed: 2.4ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1288909046_d2b2b62607.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2191453879_11dfe2ba3a.jpg: 640x448 4 persons, 9.7ms\n",
      "Speed: 2.3ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2191453879_11dfe2ba3a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2502835694_4fe121bbea.jpg: 480x640 11 persons, 1 traffic light, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2502835694_4fe121bbea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2327240505_e73cc73246.jpg: 448x640 2 persons, 2 cars, 1 skateboard, 9.9ms\n",
      "Speed: 2.2ms preprocess, 9.9ms inference, 3.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2327240505_e73cc73246.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2308256827_3c0a7d514d.jpg: 480x640 2 persons, 14.5ms\n",
      "Speed: 3.4ms preprocess, 14.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2308256827_3c0a7d514d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2626158969_ac09aeb88d.jpg: 448x640 1 person, 12.1ms\n",
      "Speed: 2.4ms preprocess, 12.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2626158969_ac09aeb88d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1311388430_4ab0cd1a1f.jpg: 448x640 (no detections), 10.6ms\n",
      "Speed: 2.1ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1311388430_4ab0cd1a1f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2224450291_4c133fabe8.jpg: 448x640 1 person, 1 cell phone, 10.3ms\n",
      "Speed: 2.1ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2224450291_4c133fabe8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2351479551_e8820a1ff3.jpg: 416x640 1 person, 11.4ms\n",
      "Speed: 2.0ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2351479551_e8820a1ff3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/419116771_642800891d.jpg: 448x640 1 person, 1 car, 3 dogs, 12.4ms\n",
      "Speed: 3.1ms preprocess, 12.4ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 419116771_642800891d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1430154945_71bbaa094a.jpg: 640x448 5 persons, 2 cups, 1 sandwich, 1 dining table, 14.7ms\n",
      "Speed: 3.2ms preprocess, 14.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1430154945_71bbaa094a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/421730441_6b2267fd31.jpg: 480x640 1 person, 13.1ms\n",
      "Speed: 2.9ms preprocess, 13.1ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 421730441_6b2267fd31.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3726025663_e7d35d23f6.jpg: 480x640 7 persons, 9 bicycles, 10.4ms\n",
      "Speed: 2.4ms preprocess, 10.4ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3726025663_e7d35d23f6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/234241682_51d9fabb27.jpg: 416x640 8 persons, 11.0ms\n",
      "Speed: 1.9ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 234241682_51d9fabb27.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3533145793_5d69f72e41.jpg: 480x640 18 persons, 4 umbrellas, 1 skateboard, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3533145793_5d69f72e41.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2121140070_a09644550b.jpg: 480x640 13 persons, 1 frisbee, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2121140070_a09644550b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2599131872_65789d86d5.jpg: 448x640 2 persons, 11.0ms\n",
      "Speed: 2.0ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2599131872_65789d86d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3692593096_fbaea67476.jpg: 480x640 (no detections), 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 0.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3692593096_fbaea67476.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1417031097_ab656bc4bd.jpg: 640x448 (no detections), 11.2ms\n",
      "Speed: 2.1ms preprocess, 11.2ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1417031097_ab656bc4bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/418616992_22090c6195.jpg: 480x640 1 person, 1 dog, 10.9ms\n",
      "Speed: 2.2ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 418616992_22090c6195.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2125454445_5c5c4bf906.jpg: 480x640 1 bird, 1 dog, 10.1ms\n",
      "Speed: 2.2ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2125454445_5c5c4bf906.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/477254932_56b48d775d.jpg: 448x640 13 persons, 2 benchs, 1 chair, 10.9ms\n",
      "Speed: 2.1ms preprocess, 10.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 477254932_56b48d775d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3116985493_04b1dc3345.jpg: 448x640 3 persons, 10.0ms\n",
      "Speed: 2.1ms preprocess, 10.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3116985493_04b1dc3345.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3461110860_37ef15af8e.jpg: 448x640 15 persons, 10.1ms\n",
      "Speed: 2.0ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3461110860_37ef15af8e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2808870080_4ea4f3327e.jpg: 576x640 3 cows, 11.6ms\n",
      "Speed: 2.7ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2808870080_4ea4f3327e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2252123185_487f21e336.jpg: 448x640 13 persons, 1 bus, 11.5ms\n",
      "Speed: 2.1ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2252123185_487f21e336.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1110208841_5bb6806afe.jpg: 480x640 14 persons, 1 dog, 1 frisbee, 14.0ms\n",
      "Speed: 3.8ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1110208841_5bb6806afe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2053733930_e245615ad4.jpg: 480x640 1 backpack, 1 skis, 11.3ms\n",
      "Speed: 3.1ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2053733930_e245615ad4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3412249548_00820fc4ca.jpg: 640x448 1 person, 11.3ms\n",
      "Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3412249548_00820fc4ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2924884899_f512c84332.jpg: 640x512 6 persons, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2924884899_f512c84332.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/691770760_48ce80a674.jpg: 480x640 2 persons, 1 dog, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 691770760_48ce80a674.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3056530884_27766059bc.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3056530884_27766059bc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3542341321_faa2d2d48a.jpg: 480x640 1 dog, 1 cow, 8.9ms\n",
      "Speed: 2.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3542341321_faa2d2d48a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564742915_5f940b95b4.jpg: 640x480 2 persons, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3564742915_5f940b95b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/964197865_0133acaeb4.jpg: 640x448 1 person, 1 baseball bat, 8.8ms\n",
      "Speed: 2.1ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 964197865_0133acaeb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3636418958_f038130bb2.jpg: 640x608 11 persons, 7 chairs, 8.9ms\n",
      "Speed: 2.9ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 608)\n",
      "Cropped images saved for 3636418958_f038130bb2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3321063116_4e5deeac83.jpg: 320x640 1 dog, 1 bear, 10.3ms\n",
      "Speed: 1.6ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3321063116_4e5deeac83.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3682038869_585075b5ff.jpg: 448x640 (no detections), 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 0.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3682038869_585075b5ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3240090389_97a8c5d386.jpg: 448x640 2 persons, 2 skateboards, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3240090389_97a8c5d386.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/327415627_6313d32a64.jpg: 512x640 2 dogs, 1 cow, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 327415627_6313d32a64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1075881101_d55c46bece.jpg: 640x576 5 persons, 9.2ms\n",
      "Speed: 2.6ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 1075881101_d55c46bece.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3251460982_4578a568bb.jpg: 448x640 1 dog, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3251460982_4578a568bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/769260947_02bc973d76.jpg: 448x640 1 dog, 1 cow, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 769260947_02bc973d76.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3597210806_95b07bb968.jpg: 640x480 1 person, 1 kite, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3597210806_95b07bb968.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2458033289_f0616879df.jpg: 448x640 1 person, 1 bowl, 2 chairs, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2458033289_f0616879df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3433982387_3fa993cf5a.jpg: 480x640 3 persons, 1 sports ball, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3433982387_3fa993cf5a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1169307342_e7a4685a5c.jpg: 480x640 2 dogs, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1169307342_e7a4685a5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/141140165_9002a04f19.jpg: 640x448 2 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 141140165_9002a04f19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662262499_3cdf49cedd.jpg: 640x448 8 persons, 1 car, 1 sports ball, 1 baseball bat, 1 chair, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2662262499_3cdf49cedd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3603301825_5817727be2.jpg: 416x640 5 persons, 1 sports ball, 1 baseball bat, 7.6ms\n",
      "Speed: 1.5ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3603301825_5817727be2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3558370311_5734a15890.jpg: 640x448 5 persons, 1 bicycle, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3558370311_5734a15890.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2868136205_3cf679208d.jpg: 480x640 3 persons, 2 chairs, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2868136205_3cf679208d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2603125422_659391f961.jpg: 448x640 3 persons, 1 car, 1 clock, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2603125422_659391f961.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1332492622_8c66992b62.jpg: 544x640 (no detections), 7.5ms\n",
      "Speed: 2.0ms preprocess, 7.5ms inference, 0.5ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1332492622_8c66992b62.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/96399948_b86c61bfe6.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 96399948_b86c61bfe6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1388346434_524d0b6dfa.jpg: 480x640 1 person, 6 cars, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1388346434_524d0b6dfa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2780031669_a0345cfc26.jpg: 480x640 3 persons, 3 snowboards, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2780031669_a0345cfc26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/544122267_e9e0100bc5.jpg: 640x480 1 person, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 544122267_e9e0100bc5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2552949275_b8cdc450cc.jpg: 448x640 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2552949275_b8cdc450cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1067790824_f3cc97239b.jpg: 480x640 2 dogs, 8.8ms\n",
      "Speed: 1.9ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1067790824_f3cc97239b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2234910971_80e0325918.jpg: 480x640 2 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2234910971_80e0325918.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176968956_d942a93513.jpg: 448x640 1 person, 2 boats, 1 surfboard, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3176968956_d942a93513.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3094064787_aed1666fc9.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3094064787_aed1666fc9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/527272653_8a5bd818e5.jpg: 640x416 2 persons, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 527272653_8a5bd818e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3443460885_46115463b4.jpg: 480x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3443460885_46115463b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3276475986_66cd9cc7e4.jpg: 608x640 5 persons, 2 cars, 1 truck, 3 handbags, 7.4ms\n",
      "Speed: 2.5ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 3276475986_66cd9cc7e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3008370541_ce29ce49f0.jpg: 448x640 1 dog, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3008370541_ce29ce49f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2837127816_24441e5f7c.jpg: 448x640 2 persons, 1 dog, 2 umbrellas, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2837127816_24441e5f7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1424237335_b3be9920ba.jpg: 640x448 2 persons, 1 sports ball, 8.5ms\n",
      "Speed: 1.9ms preprocess, 8.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1424237335_b3be9920ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3655326478_4472c5c630.jpg: 512x640 5 persons, 1 dog, 2 frisbees, 7.8ms\n",
      "Speed: 2.2ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3655326478_4472c5c630.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662816021_ac474e0fde.jpg: 480x640 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2662816021_ac474e0fde.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2647229826_e0e0c65ef1.jpg: 448x640 4 persons, 1 skateboard, 7.1ms\n",
      "Speed: 1.5ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2647229826_e0e0c65ef1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/524036004_6747cf909b.jpg: 640x480 4 persons, 1 boat, 2 sports balls, 1 cup, 7.9ms\n",
      "Speed: 1.8ms preprocess, 7.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 524036004_6747cf909b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/566921157_07c18a41e2.jpg: 480x640 2 dogs, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 566921157_07c18a41e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596876977_b61ee7ee78.jpg: 448x640 1 person, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2596876977_b61ee7ee78.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3381584882_341ee3092f.jpg: 448x640 1 person, 1 snowboard, 11.7ms\n",
      "Speed: 3.1ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3381584882_341ee3092f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1428578577_82864facae.jpg: 480x640 1 person, 11.4ms\n",
      "Speed: 2.8ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1428578577_82864facae.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1562392511_522a26063b.jpg: 512x640 3 persons, 2 dogs, 9.9ms\n",
      "Speed: 2.5ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1562392511_522a26063b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275704430_a75828048f.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3275704430_a75828048f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/41999070_838089137e.jpg: 480x640 7 persons, 7 dogs, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 41999070_838089137e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2911919938_6bb6587a36.jpg: 480x640 1 person, 1 sports ball, 8.5ms\n",
      "Speed: 2.2ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2911919938_6bb6587a36.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/862156271_3eacea90a8.jpg: 448x640 3 persons, 1 wine glass, 1 refrigerator, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 862156271_3eacea90a8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3219122000_bd6b4ae5ff.jpg: 448x640 1 dog, 10.8ms\n",
      "Speed: 3.0ms preprocess, 10.8ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3219122000_bd6b4ae5ff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3122938209_2b2c6c1fab.jpg: 640x448 1 person, 1 bicycle, 10.0ms\n",
      "Speed: 2.4ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3122938209_2b2c6c1fab.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3263215700_e27f81f8b9.jpg: 448x640 1 bear, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3263215700_e27f81f8b9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3598447435_f66cd10bd6.jpg: 448x640 1 person, 2 skiss, 1 surfboard, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3598447435_f66cd10bd6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1357753846_6185e26040.jpg: 480x640 1 dog, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1357753846_6185e26040.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3276448136_0d9f5069c5.jpg: 320x640 12 persons, 1 handbag, 9.9ms\n",
      "Speed: 1.5ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 3276448136_0d9f5069c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/472860064_a96a228796.jpg: 480x640 1 dog, 1 frisbee, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 472860064_a96a228796.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2870194345_0bcbac1aa5.jpg: 640x448 1 person, 1 frisbee, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2870194345_0bcbac1aa5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3708244207_0d3a2b2f92.jpg: 448x640 2 persons, 1 airplane, 2 boats, 1 umbrella, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3708244207_0d3a2b2f92.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344632789_af90d54746.jpg: 640x448 6 persons, 1 skateboard, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3344632789_af90d54746.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3367034082_31658a89bb.jpg: 640x448 11 persons, 1 bicycle, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3367034082_31658a89bb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3484625231_5b1a1a07b8.jpg: 640x448 1 person, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3484625231_5b1a1a07b8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/380527679_574749123d.jpg: 448x640 14 persons, 1 backpack, 1 umbrella, 2 handbags, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 380527679_574749123d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3130970054_04a3865c43.jpg: 480x640 4 persons, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3130970054_04a3865c43.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/533713001_2d36e93509.jpg: 480x640 10 persons, 1 umbrella, 7 chairs, 4 potted plants, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 533713001_2d36e93509.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3176072448_b84c99cf7f.jpg: 448x640 2 persons, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3176072448_b84c99cf7f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/166321294_4a5e68535f.jpg: 544x640 4 persons, 1 motorcycle, 7.6ms\n",
      "Speed: 2.1ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 166321294_4a5e68535f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3544803461_a418ca611e.jpg: 448x640 3 persons, 1 frisbee, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3544803461_a418ca611e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3612485097_b706d950ed.jpg: 448x640 4 persons, 1 sports ball, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3612485097_b706d950ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3170856184_efabfd0297.jpg: 448x640 10 persons, 1 baseball bat, 1 tennis racket, 1 chair, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3170856184_efabfd0297.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3382105769_b1a4e4c60d.jpg: 448x640 1 bear, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3382105769_b1a4e4c60d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3452127051_fa54a902b3.jpg: 512x640 1 dog, 1 frisbee, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3452127051_fa54a902b3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/537225246_dd0e2158a7.jpg: 416x640 12 persons, 1 suitcase, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 537225246_dd0e2158a7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2185793891_5a5e903ca6.jpg: 480x640 3 persons, 1 tie, 7.7ms\n",
      "Speed: 1.9ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2185793891_5a5e903ca6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359636318_39267812a0.jpg: 448x640 6 persons, 1 backpack, 8.5ms\n",
      "Speed: 2.3ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3359636318_39267812a0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3605676864_0fb491267e.jpg: 448x640 2 persons, 1 surfboard, 6.8ms\n",
      "Speed: 1.6ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3605676864_0fb491267e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/427167162_2c99779444.jpg: 448x640 6 persons, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 427167162_2c99779444.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2575647360_f5de38c751.jpg: 480x640 1 person, 4 chairs, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2575647360_f5de38c751.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/529101401_ab1f6b1206.jpg: 448x640 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 529101401_ab1f6b1206.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/909808296_23c427022d.jpg: 480x640 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 909808296_23c427022d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/476759700_8911f087f8.jpg: 640x416 5 persons, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 476759700_8911f087f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3032790880_d216197d55.jpg: 448x640 4 persons, 1 sports ball, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3032790880_d216197d55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/265528702_8653eab9fa.jpg: 480x640 1 person, 1 tie, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 265528702_8653eab9fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/244399048_8332bb3270.jpg: 640x480 1 dog, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 244399048_8332bb3270.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2261962622_e9318a95eb.jpg: 480x640 5 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2261962622_e9318a95eb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3271468462_701eb88d3b.jpg: 512x640 3 horses, 1 cow, 1 giraffe, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3271468462_701eb88d3b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2737759676_4bc9be2daf.jpg: 416x640 5 persons, 7.5ms\n",
      "Speed: 1.5ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2737759676_4bc9be2daf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539493423_9d7d1b77fa.jpg: 480x640 1 person, 1 kite, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 539493423_9d7d1b77fa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3255732353_fbc487aefc.jpg: 448x640 2 persons, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3255732353_fbc487aefc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3503689049_63212220be.jpg: 640x544 1 person, 7.5ms\n",
      "Speed: 2.2ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3503689049_63212220be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856252334_1b1a230e70.jpg: 480x640 1 person, 1 sports ball, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2856252334_1b1a230e70.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2766726291_b83eb5d315.jpg: 448x640 1 person, 1 chair, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2766726291_b83eb5d315.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3400041870_4e7732b40f.jpg: 448x640 8 persons, 1 backpack, 1 tie, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3400041870_4e7732b40f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2078311270_f01c9eaf4c.jpg: 448x640 2 persons, 1 handbag, 6.7ms\n",
      "Speed: 1.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2078311270_f01c9eaf4c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2426303900_0a8d52eb14.jpg: 640x640 1 person, 2 umbrellas, 7.6ms\n",
      "Speed: 2.3ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2426303900_0a8d52eb14.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512045825_1be2083922.jpg: 480x640 1 dog, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 512045825_1be2083922.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565703445_dd6899bc0e.jpg: 544x640 1 person, 7.4ms\n",
      "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2565703445_dd6899bc0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2573625591_70291c894a.jpg: 480x640 3 dogs, 2 cows, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2573625591_70291c894a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2562463210_d0dfd545ca.jpg: 448x640 1 person, 1 baseball bat, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2562463210_d0dfd545ca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2426828433_ce894d1c54.jpg: 480x640 1 dog, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2426828433_ce894d1c54.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2642350864_099c0f2152.jpg: 480x640 2 persons, 1 tennis racket, 1 broccoli, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2642350864_099c0f2152.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2768662025_44001fe5d1.jpg: 640x448 5 persons, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2768662025_44001fe5d1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/489065557_0eb08889cd.jpg: 480x640 1 person, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 489065557_0eb08889cd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2424111022_4e332b8aee.jpg: 480x640 1 person, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2424111022_4e332b8aee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2471297228_b784ff61a2.jpg: 544x640 9 persons, 1 handbag, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 2471297228_b784ff61a2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2355819665_39021ff642.jpg: 448x640 1 person, 1 bicycle, 1 car, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2355819665_39021ff642.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1198194316_543cc7b945.jpg: 640x480 4 persons, 2 handbags, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1198194316_543cc7b945.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2061144717_5b3a1864f0.jpg: 384x640 2 persons, 1 fire hydrant, 14.2ms\n",
      "Speed: 3.3ms preprocess, 14.2ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 2061144717_5b3a1864f0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2707969386_94dde00ce4.jpg: 480x640 1 dog, 12.1ms\n",
      "Speed: 3.3ms preprocess, 12.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2707969386_94dde00ce4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3583516290_1c87a13770.jpg: 448x640 11 persons, 10.9ms\n",
      "Speed: 2.5ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3583516290_1c87a13770.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/616045808_0286d0574b.jpg: 640x480 11 persons, 1 motorcycle, 2 handbags, 9.6ms\n",
      "Speed: 2.3ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 616045808_0286d0574b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/498794783_cc2ac62b47.jpg: 640x480 2 persons, 8.8ms\n",
      "Speed: 2.3ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 498794783_cc2ac62b47.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1443807993_aebfb2784a.jpg: 448x640 2 persons, 1 bed, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1443807993_aebfb2784a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3415311628_c220a65762.jpg: 448x640 2 persons, 1 truck, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3415311628_c220a65762.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/252578659_9e404b6430.jpg: 448x640 4 sheeps, 8.5ms\n",
      "Speed: 2.1ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 252578659_9e404b6430.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3215695965_69fbeba3d5.jpg: 480x640 (no detections), 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3215695965_69fbeba3d5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2173061319_1f267765dc.jpg: 448x640 1 person, 1 surfboard, 9.1ms\n",
      "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2173061319_1f267765dc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2294598473_40637b5c04.jpg: 640x512 1 dog, 1 sheep, 1 cow, 1 frisbee, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2294598473_40637b5c04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3273163189_dece7babf4.jpg: 416x640 1 dog, 9.3ms\n",
      "Speed: 1.9ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3273163189_dece7babf4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1579287915_4257c54451.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1579287915_4257c54451.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3366904106_e996320d20.jpg: 480x640 1 person, 1 skateboard, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3366904106_e996320d20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/491564019_1ca68d16c1.jpg: 448x640 1 person, 3 cars, 1 dog, 1 surfboard, 1 chair, 9.1ms\n",
      "Speed: 2.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 491564019_1ca68d16c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3429956016_3c7e3096c2.jpg: 640x480 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3429956016_3c7e3096c2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3428038648_993a453f9e.jpg: 480x640 1 person, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3428038648_993a453f9e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2662845514_8620aaee96.jpg: 640x448 2 persons, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2662845514_8620aaee96.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2106772874_381824648b.jpg: 640x512 1 dog, 1 couch, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 2106772874_381824648b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2830309113_c79d7be554.jpg: 640x448 1 person, 8.7ms\n",
      "Speed: 2.0ms preprocess, 8.7ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2830309113_c79d7be554.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3225310099_d8e419ba56.jpg: 352x640 4 persons, 2 boats, 8.5ms\n",
      "Speed: 1.3ms preprocess, 8.5ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 3225310099_d8e419ba56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2670560883_7e7b563092.jpg: 640x480 11 persons, 1 bench, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2670560883_7e7b563092.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2882056260_4399dd4d7c.jpg: 448x640 2 persons, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2882056260_4399dd4d7c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2959581023_54402c8d88.jpg: 640x448 1 bird, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2959581023_54402c8d88.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2159447283_fab8c272b0.jpg: 640x416 2 persons, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2159447283_fab8c272b0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2561341745_2d77d3ff7d.jpg: 416x640 1 person, 5 cars, 7.4ms\n",
      "Speed: 1.5ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2561341745_2d77d3ff7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/294709836_87126898fb.jpg: 480x640 1 cat, 1 cow, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 294709836_87126898fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2882743431_c3e6cd1b5c.jpg: 448x640 3 dogs, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2882743431_c3e6cd1b5c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3676561090_9828a9f6d0.jpg: 448x640 1 bird, 7.0ms\n",
      "Speed: 1.8ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3676561090_9828a9f6d0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3170802797_3c851bb475.jpg: 640x448 3 persons, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3170802797_3c851bb475.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3556571710_19cee6f5bd.jpg: 448x640 6 persons, 2 bicycles, 1 tie, 7.6ms\n",
      "Speed: 1.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3556571710_19cee6f5bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3430287726_94a1825bbf.jpg: 448x640 1 person, 1 motorcycle, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3430287726_94a1825bbf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2216695423_1362cb25f3.jpg: 480x640 2 dogs, 1 sheep, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2216695423_1362cb25f3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3619232550_0b1e1fd4e4.jpg: 448x640 3 persons, 7.8ms\n",
      "Speed: 1.7ms preprocess, 7.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3619232550_0b1e1fd4e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/424869823_7aec015d87.jpg: 640x448 1 person, 12.7ms\n",
      "Speed: 3.3ms preprocess, 12.7ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 424869823_7aec015d87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/365128300_6966058139.jpg: 416x640 2 persons, 1 surfboard, 10.6ms\n",
      "Speed: 2.5ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 365128300_6966058139.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2565350330_c7f305e7f7.jpg: 640x480 1 person, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2565350330_c7f305e7f7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2887744223_029f2fd5fe.jpg: 480x640 4 persons, 1 sports ball, 12.2ms\n",
      "Speed: 3.2ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2887744223_029f2fd5fe.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3110649716_c17e14670e.jpg: 480x640 8 persons, 1 truck, 9.4ms\n",
      "Speed: 2.5ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3110649716_c17e14670e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3652859271_908ae0ae89.jpg: 512x640 2 persons, 2 cups, 9.2ms\n",
      "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3652859271_908ae0ae89.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3307563498_e2b4f19272.jpg: 480x640 1 person, 16.5ms\n",
      "Speed: 3.9ms preprocess, 16.5ms inference, 2.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3307563498_e2b4f19272.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3654869593_c8599a8e20.jpg: 448x640 10 persons, 12 umbrellas, 12.6ms\n",
      "Speed: 2.6ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3654869593_c8599a8e20.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2391812384_7429b5e567.jpg: 608x640 3 persons, 11.1ms\n",
      "Speed: 3.1ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2391812384_7429b5e567.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2204550058_2707d92338.jpg: 448x640 6 persons, 10.9ms\n",
      "Speed: 2.1ms preprocess, 10.9ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2204550058_2707d92338.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2807177340_bc85291df5.jpg: 448x640 2 persons, 2 cars, 10.0ms\n",
      "Speed: 2.1ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2807177340_bc85291df5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3136043366_b3f8607a0e.jpg: 480x640 7 persons, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3136043366_b3f8607a0e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2699125097_c6801d80ed.jpg: 640x576 1 person, 11.1ms\n",
      "Speed: 2.6ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2699125097_c6801d80ed.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/317383917_d8bfa350b6.jpg: 320x640 2 dogs, 1 horse, 2 bears, 11.7ms\n",
      "Speed: 1.6ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 320, 640)\n",
      "Cropped images saved for 317383917_d8bfa350b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3148811252_2fa9490a04.jpg: 640x512 9 persons, 1 baseball glove, 11.0ms\n",
      "Speed: 2.3ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3148811252_2fa9490a04.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/136644885_f7d2bbf546.jpg: 480x640 4 persons, 11.1ms\n",
      "Speed: 2.2ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 136644885_f7d2bbf546.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3260975858_75d0612a69.jpg: 384x640 2 persons, 2 cars, 2 dogs, 2 horses, 11.6ms\n",
      "Speed: 1.8ms preprocess, 11.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 3260975858_75d0612a69.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532587748_7e64bb223a.jpg: 640x448 1 person, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3532587748_7e64bb223a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/832128857_1390386ea6.jpg: 448x640 1 person, 2 baseball gloves, 11.1ms\n",
      "Speed: 2.1ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 832128857_1390386ea6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1662261486_db967930de.jpg: 544x640 1 person, 2 dogs, 11.1ms\n",
      "Speed: 2.5ms preprocess, 11.1ms inference, 1.8ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 1662261486_db967930de.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3189964753_a95536ced9.jpg: 480x640 12 persons, 11.0ms\n",
      "Speed: 2.2ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3189964753_a95536ced9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3352791995_8db4979aca.jpg: 640x416 2 persons, 11.2ms\n",
      "Speed: 2.0ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 3352791995_8db4979aca.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1947351225_288d788983.jpg: 480x640 22 persons, 11.3ms\n",
      "Speed: 2.2ms preprocess, 11.3ms inference, 1.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1947351225_288d788983.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2358447641_10f1e9d21f.jpg: 416x640 1 person, 9.8ms\n",
      "Speed: 1.6ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2358447641_10f1e9d21f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3194134352_bc1b2a25d7.jpg: 640x448 1 person, 9.2ms\n",
      "Speed: 1.8ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3194134352_bc1b2a25d7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/471402959_0b187560df.jpg: 480x640 2 persons, 1 chair, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 471402959_0b187560df.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2537197415_af7c30dfc8.jpg: 640x480 5 persons, 1 chair, 9.7ms\n",
      "Speed: 1.9ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2537197415_af7c30dfc8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2238759450_6475641bdb.jpg: 640x480 1 person, 1 baseball bat, 8.4ms\n",
      "Speed: 1.9ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2238759450_6475641bdb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3027397797_4f1d305ced.jpg: 480x640 1 person, 1 skateboard, 9.7ms\n",
      "Speed: 1.8ms preprocess, 9.7ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3027397797_4f1d305ced.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/386655611_1329495f97.jpg: 480x640 2 dogs, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 386655611_1329495f97.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241346146_f27759296d.jpg: 448x640 9 persons, 1 baseball glove, 8.9ms\n",
      "Speed: 1.7ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241346146_f27759296d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3106787167_e5f2312622.jpg: 512x640 3 persons, 1 frisbee, 9.0ms\n",
      "Speed: 1.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3106787167_e5f2312622.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2282522980_45cfa8e0cf.jpg: 512x640 2 dogs, 7.9ms\n",
      "Speed: 1.7ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2282522980_45cfa8e0cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3546474710_903c3c9fd3.jpg: 480x640 1 person, 9.1ms\n",
      "Speed: 1.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3546474710_903c3c9fd3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2931950813_93145c4746.jpg: 640x416 2 persons, 2 umbrellas, 1 skateboard, 1 chair, 9.2ms\n",
      "Speed: 1.7ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2931950813_93145c4746.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3094278545_febac56382.jpg: 448x640 1 person, 13.1ms\n",
      "Speed: 3.5ms preprocess, 13.1ms inference, 2.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3094278545_febac56382.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3454621502_73af6742fb.jpg: 640x480 1 dog, 11.0ms\n",
      "Speed: 2.8ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3454621502_73af6742fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/792362827_5ab5281b99.jpg: 448x640 6 persons, 1 bicycle, 1 chair, 9.8ms\n",
      "Speed: 2.4ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 792362827_5ab5281b99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3459419203_cd7c68ce4d.jpg: 544x640 1 person, 9.3ms\n",
      "Speed: 2.5ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 3459419203_cd7c68ce4d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3293642024_e136b74a55.jpg: 480x640 1 person, 2 surfboards, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3293642024_e136b74a55.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2596100297_372bd0f739.jpg: 480x640 2 persons, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2596100297_372bd0f739.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/309430053_cc58bcc36a.jpg: 640x480 1 person, 1 boat, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 309430053_cc58bcc36a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2273038287_3004a72a34.jpg: 640x544 1 dog, 9.5ms\n",
      "Speed: 2.4ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 2273038287_3004a72a34.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2769731772_18c44c18e2.jpg: 640x448 1 person, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2769731772_18c44c18e2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2721656220_5f4cda8bc1.jpg: 640x576 2 persons, 1 cup, 9.3ms\n",
      "Speed: 2.7ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2721656220_5f4cda8bc1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3532205154_5674b628ea.jpg: 448x640 3 persons, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3532205154_5674b628ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3653484549_f316590b0f.jpg: 480x640 4 persons, 1 cup, 2 cakes, 2 dining tables, 13.4ms\n",
      "Speed: 3.8ms preprocess, 13.4ms inference, 2.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3653484549_f316590b0f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2661294969_1388b4738c.jpg: 480x640 4 persons, 2 cars, 10.3ms\n",
      "Speed: 2.7ms preprocess, 10.3ms inference, 1.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2661294969_1388b4738c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522000960_47415c3890.jpg: 480x640 1 person, 8.9ms\n",
      "Speed: 2.3ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3522000960_47415c3890.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3630641436_8f9ac5b9b2.jpg: 448x640 1 person, 1 dog, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3630641436_8f9ac5b9b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3102204862_f1d220230b.jpg: 640x448 10 persons, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3102204862_f1d220230b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2443512473_6f5a22eb42.jpg: 576x640 2 dogs, 9.4ms\n",
      "Speed: 2.7ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 2443512473_6f5a22eb42.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522349685_f046f0e250.jpg: 448x640 1 person, 1 cat, 9.2ms\n",
      "Speed: 2.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3522349685_f046f0e250.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/317488612_70ac35493b.jpg: 448x640 1 dog, 8.7ms\n",
      "Speed: 2.1ms preprocess, 8.7ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 317488612_70ac35493b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3596131692_91b8a05606.jpg: 640x448 5 persons, 3 cars, 1 sports ball, 9.2ms\n",
      "Speed: 2.0ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3596131692_91b8a05606.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2929272606_2a5923b38e.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2929272606_2a5923b38e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3028095878_07341efc9c.jpg: 448x640 1 person, 2 frisbees, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3028095878_07341efc9c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3005124440_c096b310fb.jpg: 640x576 4 persons, 1 bicycle, 9.3ms\n",
      "Speed: 2.6ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 3005124440_c096b310fb.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/160541986_d5be2ab4c1.jpg: 640x448 1 person, 11.4ms\n",
      "Speed: 2.1ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 160541986_d5be2ab4c1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3599442049_e448c7c9b2.jpg: 448x640 1 person, 10.2ms\n",
      "Speed: 2.5ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3599442049_e448c7c9b2.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/576075451_5e0f6facb3.jpg: 448x640 1 person, 1 dog, 8.5ms\n",
      "Speed: 2.0ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 576075451_5e0f6facb3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/254527963_3f5824b0e8.jpg: 448x640 1 dog, 8.4ms\n",
      "Speed: 2.0ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 254527963_3f5824b0e8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3478084305_9e1219c3b6.jpg: 640x480 1 person, 1 frisbee, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3478084305_9e1219c3b6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/861795382_5145ad433d.jpg: 576x640 1 person, 1 bicycle, 9.3ms\n",
      "Speed: 2.8ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 861795382_5145ad433d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2257099774_37d0d3aa9a.jpg: 608x640 1 person, 9.4ms\n",
      "Speed: 2.7ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 608, 640)\n",
      "Cropped images saved for 2257099774_37d0d3aa9a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/248174959_2522871152.jpg: 480x640 1 person, 1 backpack, 9.4ms\n",
      "Speed: 2.2ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 248174959_2522871152.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3450776690_38605c667d.jpg: 640x480 3 persons, 9.3ms\n",
      "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3450776690_38605c667d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2248487956_2603f55ab9.jpg: 512x640 2 persons, 1 cell phone, 9.3ms\n",
      "Speed: 2.3ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2248487956_2603f55ab9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3628994466_a12065d29b.jpg: 448x640 8 persons, 2 cars, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3628994466_a12065d29b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/525538142_7348f0bce7.jpg: 640x448 2 persons, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 525538142_7348f0bce7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2689491604_d8760f57b4.jpg: 416x640 1 person, 1 surfboard, 7.3ms\n",
      "Speed: 1.5ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2689491604_d8760f57b4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3320680380_b0d38b3b4a.jpg: 640x448 4 persons, 3 skiss, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3320680380_b0d38b3b4a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/752052256_243d111bf0.jpg: 480x640 8 persons, 1 truck, 2 cups, 3 chairs, 7.0ms\n",
      "Speed: 1.6ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 752052256_243d111bf0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3169394115_2193158cee.jpg: 480x640 2 cars, 1 dog, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3169394115_2193158cee.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2738255684_0324ed062d.jpg: 448x640 1 bird, 1 dog, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2738255684_0324ed062d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3497485793_e36c1d2779.jpg: 480x640 4 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3497485793_e36c1d2779.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/478208896_90e7187b64.jpg: 640x448 1 person, 12.3ms\n",
      "Speed: 3.5ms preprocess, 12.3ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 478208896_90e7187b64.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/493621130_152bdd4e91.jpg: 480x640 1 boat, 1 dog, 10.5ms\n",
      "Speed: 2.7ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 493621130_152bdd4e91.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/719837187_3e7bf1d472.jpg: 448x640 1 dog, 1 frisbee, 1 sports ball, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 719837187_3e7bf1d472.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3329858093_0ec73f2190.jpg: 416x640 3 persons, 9.3ms\n",
      "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3329858093_0ec73f2190.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1731546544_9fbf14617b.jpg: 512x640 1 person, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 1731546544_9fbf14617b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3344233740_c010378da7.jpg: 640x448 3 persons, 1 handbag, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3344233740_c010378da7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3275537015_74e04c0f3e.jpg: 480x640 5 persons, 1 boat, 1 umbrella, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3275537015_74e04c0f3e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2088460083_42ee8a595a.jpg: 640x640 1 train, 10.1ms\n",
      "Speed: 2.9ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 2088460083_42ee8a595a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301811927_a2797339e5.jpg: 480x640 9 persons, 4 baseball bats, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3301811927_a2797339e5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3605100550_01214a1224.jpg: 416x640 1 dog, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 3605100550_01214a1224.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3141613533_595723208d.jpg: 480x640 2 persons, 11.6ms\n",
      "Speed: 2.3ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3141613533_595723208d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2158247955_484f0a1f11.jpg: 640x576 3 dogs, 9.5ms\n",
      "Speed: 3.1ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2158247955_484f0a1f11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3054989420_3e755ca352.jpg: 480x640 4 persons, 1 cow, 9.7ms\n",
      "Speed: 2.3ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3054989420_3e755ca352.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3004290523_d1319dfdb4.jpg: 576x640 5 persons, 1 cell phone, 13.3ms\n",
      "Speed: 4.6ms preprocess, 13.3ms inference, 2.1ms postprocess per image at shape (1, 3, 576, 640)\n",
      "Cropped images saved for 3004290523_d1319dfdb4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1501985304_8c50093004.jpg: 640x512 3 persons, 10.9ms\n",
      "Speed: 3.1ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 1501985304_8c50093004.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3677514746_26f5588150.jpg: 448x640 4 persons, 1 handbag, 9.9ms\n",
      "Speed: 2.4ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3677514746_26f5588150.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/751074141_feafc7b16c.jpg: 480x640 2 persons, 9.1ms\n",
      "Speed: 2.5ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 751074141_feafc7b16c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/795081510_53fd17d101.jpg: 480x640 3 persons, 1 boat, 1 bench, 1 handbag, 8.1ms\n",
      "Speed: 2.2ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 795081510_53fd17d101.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1488937076_5baa73fc2a.jpg: 416x640 3 dogs, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 1488937076_5baa73fc2a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2943023421_e297f05e11.jpg: 448x640 1 car, 8.8ms\n",
      "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2943023421_e297f05e11.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2822148499_eaa46c99d4.jpg: 448x640 1 car, 1 boat, 1 dog, 1 sports ball, 8.1ms\n",
      "Speed: 2.0ms preprocess, 8.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2822148499_eaa46c99d4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3690107455_0fdb4ecee7.jpg: 640x448 1 person, 8.9ms\n",
      "Speed: 2.0ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3690107455_0fdb4ecee7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/392467282_00bb22e201.jpg: 480x640 2 dogs, 12.5ms\n",
      "Speed: 3.5ms preprocess, 12.5ms inference, 2.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 392467282_00bb22e201.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3560726559_4c4bed9f2d.jpg: 448x640 2 persons, 10.5ms\n",
      "Speed: 2.6ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3560726559_4c4bed9f2d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2736508369_fd9ff0b42f.jpg: 480x640 7 persons, 1 sports ball, 3 baseball gloves, 9.5ms\n",
      "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2736508369_fd9ff0b42f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3301744710_b51280eb56.jpg: 640x448 2 persons, 13.1ms\n",
      "Speed: 3.4ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3301744710_b51280eb56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3670907052_c827593564.jpg: 448x640 1 person, 1 motorcycle, 11.1ms\n",
      "Speed: 2.6ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3670907052_c827593564.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3164328039_2c56acf594.jpg: 512x640 2 dogs, 1 sports ball, 13.2ms\n",
      "Speed: 3.7ms preprocess, 13.2ms inference, 2.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3164328039_2c56acf594.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1245022983_fb329886dd.jpg: 640x448 1 person, 1 car, 10.7ms\n",
      "Speed: 2.6ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1245022983_fb329886dd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3648988742_888a16f600.jpg: 448x640 1 person, 9.4ms\n",
      "Speed: 2.1ms preprocess, 9.4ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3648988742_888a16f600.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/539750844_02a07ec524.jpg: 640x640 1 cow, 9.2ms\n",
      "Speed: 3.1ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 539750844_02a07ec524.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3112821789_1f7c3bbb99.jpg: 480x640 3 persons, 1 motorcycle, 9.2ms\n",
      "Speed: 2.2ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3112821789_1f7c3bbb99.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3640661245_c8c419524d.jpg: 480x640 1 person, 1 cell phone, 1 book, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3640661245_c8c419524d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2718049631_e7aa74cb9b.jpg: 448x640 1 person, 1 dog, 1 cow, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2718049631_e7aa74cb9b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1932161768_996eadac87.jpg: 448x640 1 dog, 8.3ms\n",
      "Speed: 2.0ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 1932161768_996eadac87.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3244171699_ace4b5d999.jpg: 480x640 9 persons, 1 tie, 1 cup, 1 knife, 5 bowls, 2 chairs, 1 dining table, 9.0ms\n",
      "Speed: 2.2ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3244171699_ace4b5d999.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2444134813_20895c640c.jpg: 448x640 2 persons, 1 dog, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2444134813_20895c640c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2319197581_94f807b204.jpg: 448x640 1 dog, 1 bear, 8.3ms\n",
      "Speed: 2.1ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2319197581_94f807b204.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3613705104_46d854134e.jpg: 640x448 1 person, 1 skateboard, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3613705104_46d854134e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3666169738_a8c74cf745.jpg: 448x640 2 persons, 1 cup, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3666169738_a8c74cf745.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3568065409_1c381aa854.jpg: 640x448 2 persons, 1 car, 1 baseball bat, 9.3ms\n",
      "Speed: 2.1ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3568065409_1c381aa854.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3207358897_bfa61fa3c6.jpg: 480x640 9 persons, 1 kite, 9.1ms\n",
      "Speed: 2.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3207358897_bfa61fa3c6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1333888922_26f15c18c3.jpg: 640x480 3 persons, 9.2ms\n",
      "Speed: 2.3ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1333888922_26f15c18c3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2938316391_97382d14aa.jpg: 448x640 2 persons, 9.1ms\n",
      "Speed: 2.1ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2938316391_97382d14aa.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1962729184_6996e128e7.jpg: 384x640 10 persons, 1 bicycle, 2 trucks, 9.9ms\n",
      "Speed: 1.8ms preprocess, 9.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 1962729184_6996e128e7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3522025527_c10e6ebd26.jpg: 480x640 9 persons, 3 airplanes, 9.0ms\n",
      "Speed: 2.3ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3522025527_c10e6ebd26.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2450453051_f1d4a78ab4.jpg: 640x576 1 person, 1 dog, 3 frisbees, 7.9ms\n",
      "Speed: 2.9ms preprocess, 7.9ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 2450453051_f1d4a78ab4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2836325261_a3bf5c59be.jpg: 640x416 1 person, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 2836325261_a3bf5c59be.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2343525685_3eba3b6686.jpg: 448x640 1 person, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2343525685_3eba3b6686.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2815755985_0fa55544c0.jpg: 448x640 1 person, 1 bed, 6.8ms\n",
      "Speed: 1.7ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2815755985_0fa55544c0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/512550372_438849ce19.jpg: 448x640 1 person, 6.5ms\n",
      "Speed: 1.6ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 512550372_438849ce19.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3760400645_3ba51d27f9.jpg: 448x640 4 persons, 6.6ms\n",
      "Speed: 1.8ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3760400645_3ba51d27f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/247637795_fdf26a03cf.jpg: 448x640 2 persons, 6.9ms\n",
      "Speed: 1.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 247637795_fdf26a03cf.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3439982121_0afc6d5973.jpg: 512x640 1 person, 1 dog, 7.4ms\n",
      "Speed: 1.9ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 3439982121_0afc6d5973.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2318721455_80c6644441.jpg: 640x448 1 person, 1 couch, 1 bed, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2318721455_80c6644441.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3619806638_7480883039.jpg: 448x640 2 persons, 1 skateboard, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3619806638_7480883039.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3400186336_37043a2f5b.jpg: 480x640 5 persons, 1 baseball glove, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3400186336_37043a2f5b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2665264979_df9c284bf8.jpg: 480x640 1 person, 6.6ms\n",
      "Speed: 1.7ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2665264979_df9c284bf8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2856524322_1d04452a21.jpg: 448x640 1 person, 1 bicycle, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2856524322_1d04452a21.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3341782693_426bf7139b.jpg: 640x448 1 person, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3341782693_426bf7139b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1463732130_a754441289.jpg: 640x480 2 persons, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 1463732130_a754441289.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/282131366_5f9a39c43c.jpg: 640x480 1 person, 6.8ms\n",
      "Speed: 1.8ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 282131366_5f9a39c43c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2597873827_a5cb3e57ba.jpg: 480x640 1 person, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2597873827_a5cb3e57ba.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2917843040_7c9caaaa8a.jpg: 640x448 1 person, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2917843040_7c9caaaa8a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3287969199_08e775d896.jpg: 448x640 1 bird, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3287969199_08e775d896.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/367673290_f8799f3a85.jpg: 640x416 1 person, 7.4ms\n",
      "Speed: 1.6ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 416)\n",
      "Cropped images saved for 367673290_f8799f3a85.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/862177617_c2c0581075.jpg: 448x640 6 persons, 1 handbag, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 862177617_c2c0581075.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2578834476_118585730d.jpg: 640x448 1 person, 1 bicycle, 7.8ms\n",
      "Speed: 1.8ms preprocess, 7.8ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 2578834476_118585730d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3021780428_497542a072.jpg: 640x480 2 persons, 7.5ms\n",
      "Speed: 1.8ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 3021780428_497542a072.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2120411340_104eb610b1.jpg: 352x640 1 dog, 1 cow, 8.3ms\n",
      "Speed: 1.3ms preprocess, 8.3ms inference, 1.1ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Cropped images saved for 2120411340_104eb610b1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/485054073_fef8b80b4b.jpg: 640x576 1 dog, 1 frisbee, 7.5ms\n",
      "Speed: 2.3ms preprocess, 7.5ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 576)\n",
      "Cropped images saved for 485054073_fef8b80b4b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3599124739_b7e60cf477.jpg: 480x640 2 persons, 7.2ms\n",
      "Speed: 1.8ms preprocess, 7.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3599124739_b7e60cf477.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3322389758_394c990b6a.jpg: 448x640 1 dog, 1 frisbee, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3322389758_394c990b6a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3514194772_43ba471982.jpg: 448x640 1 dog, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3514194772_43ba471982.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1287475186_2dee85f1a5.jpg: 640x448 1 person, 7.5ms\n",
      "Speed: 1.7ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 1287475186_2dee85f1a5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/425706089_f138118e12.jpg: 640x640 4 persons, 4 cars, 1 truck, 1 bottle, 7.0ms\n",
      "Speed: 2.4ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 425706089_f138118e12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2673564214_3a9598804f.jpg: 480x640 5 persons, 6.9ms\n",
      "Speed: 1.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2673564214_3a9598804f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/185972340_781d60ccfd.jpg: 640x640 1 person, 1 truck, 7.4ms\n",
      "Speed: 2.3ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 185972340_781d60ccfd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/260392825_ea61660633.jpg: 640x448 1 person, 1 tie, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 260392825_ea61660633.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2090339522_d30d2436f9.jpg: 480x640 1 person, 7.7ms\n",
      "Speed: 1.8ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2090339522_d30d2436f9.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3359587274_4a2b140b84.jpg: 480x640 1 person, 1 kite, 6.5ms\n",
      "Speed: 1.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3359587274_4a2b140b84.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2602866141_be9928408d.jpg: 448x640 7 persons, 2 cars, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2602866141_be9928408d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2858903676_6278f07ee3.jpg: 416x640 2 persons, 1 bench, 7.5ms\n",
      "Speed: 1.6ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2858903676_6278f07ee3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/1521623639_4bda3407cc.jpg: 480x640 1 person, 1 bench, 6.9ms\n",
      "Speed: 1.9ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 1521623639_4bda3407cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3184206563_5435f2b494.jpg: 640x448 1 person, 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3184206563_5435f2b494.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/314603661_51e05e0e24.jpg: 640x640 2 dogs, 7.1ms\n",
      "Speed: 2.3ms preprocess, 7.1ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "Cropped images saved for 314603661_51e05e0e24.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2089350172_dc2cf9fcf6.jpg: 448x640 1 elephant, 2 bears, 7.4ms\n",
      "Speed: 1.7ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2089350172_dc2cf9fcf6.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2723929323_70b93a74ea.jpg: 480x640 3 persons, 7.3ms\n",
      "Speed: 1.8ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 2723929323_70b93a74ea.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2165461920_1a4144eb2b.jpg: 416x640 4 dogs, 1 sheep, 3 cows, 7.2ms\n",
      "Speed: 1.4ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 416, 640)\n",
      "Cropped images saved for 2165461920_1a4144eb2b.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/439492931_a96d590e40.jpg: 544x640 1 dog, 1 cow, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 544, 640)\n",
      "Cropped images saved for 439492931_a96d590e40.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/222759342_98294380fc.jpg: 480x640 (no detections), 7.1ms\n",
      "Speed: 1.8ms preprocess, 7.1ms inference, 0.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 222759342_98294380fc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3185787277_b412d7f5b7.jpg: 448x640 4 persons, 1 handbag, 1 bottle, 1 chair, 3 couchs, 7.3ms\n",
      "Speed: 1.7ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3185787277_b412d7f5b7.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2460823604_7f6f786b1c.jpg: 512x640 1 person, 1 sports ball, 2 baseball gloves, 8.6ms\n",
      "Speed: 2.2ms preprocess, 8.6ms inference, 1.3ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 2460823604_7f6f786b1c.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/191003284_1025b0fb7d.jpg: 480x640 3 persons, 1 bicycle, 1 handbag, 1 potted plant, 8.4ms\n",
      "Speed: 2.2ms preprocess, 8.4ms inference, 1.2ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 191003284_1025b0fb7d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/639865690_d66d480879.jpg: 512x640 2 persons, 1 baseball bat, 7.7ms\n",
      "Speed: 2.1ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 639865690_d66d480879.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2698614194_b4e6e11dff.jpg: 448x640 5 persons, 1 frisbee, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2698614194_b4e6e11dff.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2710280476_dcccb8745a.jpg: 640x480 4 persons, 1 dog, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2710280476_dcccb8745a.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/293327462_20dee0de56.jpg: 480x640 2 dogs, 1 chair, 7.2ms\n",
      "Speed: 1.7ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 293327462_20dee0de56.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3034585889_388d6ffcc0.jpg: 448x640 1 person, 1 motorcycle, 7.3ms\n",
      "Speed: 1.6ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3034585889_388d6ffcc0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3357937209_cf4a9512ac.jpg: 448x640 1 car, 2 dogs, 6 sheeps, 6.6ms\n",
      "Speed: 1.6ms preprocess, 6.6ms inference, 1.2ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3357937209_cf4a9512ac.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3389448506_7025e7cc12.jpg: 480x640 1 bench, 1 dog, 7.1ms\n",
      "Speed: 1.7ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3389448506_7025e7cc12.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/618771382_459bb492e4.jpg: 384x640 2 persons, 8.2ms\n",
      "Speed: 1.3ms preprocess, 8.2ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Cropped images saved for 618771382_459bb492e4.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3394750987_a32ecc477e.jpg: 640x544 1 person, 1 skateboard, 7.1ms\n",
      "Speed: 2.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 544)\n",
      "Cropped images saved for 3394750987_a32ecc477e.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3180806542_49b6de312d.jpg: 640x448 1 person, 1 snowboard, 7.6ms\n",
      "Speed: 1.8ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3180806542_49b6de312d.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/241345844_69e1c22464.jpg: 448x640 13 persons, 1 baseball glove, 7.2ms\n",
      "Speed: 1.6ms preprocess, 7.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 241345844_69e1c22464.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3486154327_8be7c78569.jpg: 640x512 3 persons, 7.3ms\n",
      "Speed: 1.9ms preprocess, 7.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 512)\n",
      "Cropped images saved for 3486154327_8be7c78569.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3356369156_074750c6cc.jpg: 448x640 2 boats, 7.1ms\n",
      "Speed: 1.6ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3356369156_074750c6cc.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/401476986_73918145a3.jpg: 448x640 1 dog, 6.3ms\n",
      "Speed: 1.6ms preprocess, 6.3ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 401476986_73918145a3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3564907603_14ccc655f1.jpg: 448x640 9 persons, 6.7ms\n",
      "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3564907603_14ccc655f1.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2367816288_7c2d11d3c5.jpg: 640x480 1 person, 1 sports ball, 7.4ms\n",
      "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 480)\n",
      "Cropped images saved for 2367816288_7c2d11d3c5.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3610189629_f46de92ab3.jpg: 288x640 2 dogs, 8.7ms\n",
      "Speed: 1.2ms preprocess, 8.7ms inference, 1.2ms postprocess per image at shape (1, 3, 288, 640)\n",
      "Cropped images saved for 3610189629_f46de92ab3.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3626475209_f71cdd06bd.jpg: 448x640 2 dogs, 7.0ms\n",
      "Speed: 1.5ms preprocess, 7.0ms inference, 1.0ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3626475209_f71cdd06bd.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3033668641_5905f73990.jpg: 640x448 1 person, 1 skateboard, 7.7ms\n",
      "Speed: 1.7ms preprocess, 7.7ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3033668641_5905f73990.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3534512991_f9fd66f165.jpg: 448x640 2 persons, 2 cars, 8.1ms\n",
      "Speed: 1.8ms preprocess, 8.1ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3534512991_f9fd66f165.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/383223174_7165a54c30.jpg: 448x640 1 dog, 2 horses, 1 cow, 1 sports ball, 6.2ms\n",
      "Speed: 1.6ms preprocess, 6.2ms inference, 1.1ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 383223174_7165a54c30.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/369360998_ba56fb436f.jpg: 480x640 1 person, 1 backpack, 7.6ms\n",
      "Speed: 1.7ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 369360998_ba56fb436f.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/469617651_278e586e46.jpg: 512x640 3 persons, 1 surfboard, 2 chairs, 2 toilets, 7.5ms\n",
      "Speed: 1.9ms preprocess, 7.5ms inference, 1.1ms postprocess per image at shape (1, 3, 512, 640)\n",
      "Cropped images saved for 469617651_278e586e46.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/140377584_12bdbdf2f8.jpg: 480x640 1 person, 13.8ms\n",
      "Speed: 3.9ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 140377584_12bdbdf2f8.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2446315531_7c9704eec0.jpg: 448x640 3 dogs, 11.6ms\n",
      "Speed: 2.8ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2446315531_7c9704eec0.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3285214689_f0219e9671.jpg: 640x448 3 persons, 1 sports ball, 14.0ms\n",
      "Speed: 3.5ms preprocess, 14.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 448)\n",
      "Cropped images saved for 3285214689_f0219e9671.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3467219837_7d62213dec.jpg: 448x640 4 dogs, 15.0ms\n",
      "Speed: 2.8ms preprocess, 15.0ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3467219837_7d62213dec.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3182405529_7692256746.jpg: 448x640 1 person, 1 bicycle, 9.9ms\n",
      "Speed: 3.0ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 3182405529_7692256746.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/2064792226_97e41d8167.jpg: 448x640 12 persons, 1 backpack, 9.0ms\n",
      "Speed: 2.1ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n",
      "Cropped images saved for 2064792226_97e41d8167.jpg\n",
      "\n",
      "image 1/1 /home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/Images/3155400369_69e3d6d70f.jpg: 480x640 10 persons, 10.3ms\n",
      "Speed: 2.2ms preprocess, 10.3ms inference, 1.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Cropped images saved for 3155400369_69e3d6d70f.jpg\n",
      "end!!!\n"
     ]
    }
   ],
   "source": [
    "%run detect_object.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b76a722",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhlin/.local/lib/python3.9/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/jhlin/.local/lib/python3.9/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/jhlin/.local/lib/python3.9/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/jhlin/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jhlin/.local/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40455, 2)\n",
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \n",
      "0  A child in a pink dress is climbing up a set o...  \n",
      "1              A girl going into a wooden building .  \n",
      "2   A little girl climbing into a wooden playhouse .  \n",
      "3  A little girl climbing the stairs to her playh...  \n",
      "4  A little girl in a pink dress going into a woo...  \n",
      "                                                                   image_seq  \\\n",
      ".                                                                 ./Images/.   \n",
      "detection_data                                       ./Images/detection_data   \n",
      "3398746625_5199beea71.jpg  ./Images/3398746625_5199beea71.jpg,./detection...   \n",
      "255266148_7ba7df1a88.jpg   ./Images/255266148_7ba7df1a88.jpg,./detection_...   \n",
      "3595643050_d312e4b652.jpg  ./Images/3595643050_d312e4b652.jpg,./detection...   \n",
      "\n",
      "                                               image  \n",
      ".                                                  .  \n",
      "detection_data                        detection_data  \n",
      "3398746625_5199beea71.jpg  3398746625_5199beea71.jpg  \n",
      "255266148_7ba7df1a88.jpg    255266148_7ba7df1a88.jpg  \n",
      "3595643050_d312e4b652.jpg  3595643050_d312e4b652.jpg  \n",
      "                       image  \\\n",
      "0  1000268201_693b08cb0e.jpg   \n",
      "1  1000268201_693b08cb0e.jpg   \n",
      "2  1000268201_693b08cb0e.jpg   \n",
      "3  1000268201_693b08cb0e.jpg   \n",
      "4  1000268201_693b08cb0e.jpg   \n",
      "\n",
      "                                             caption  \\\n",
      "0  A child in a pink dress is climbing up a set o...   \n",
      "1              A girl going into a wooden building .   \n",
      "2   A little girl climbing into a wooden playhouse .   \n",
      "3  A little girl climbing the stairs to her playh...   \n",
      "4  A little girl in a pink dress going into a woo...   \n",
      "\n",
      "                                           image_seq  \\\n",
      "0  ./Images/1000268201_693b08cb0e.jpg,./detection...   \n",
      "1  ./Images/1000268201_693b08cb0e.jpg,./detection...   \n",
      "2  ./Images/1000268201_693b08cb0e.jpg,./detection...   \n",
      "3  ./Images/1000268201_693b08cb0e.jpg,./detection...   \n",
      "4  ./Images/1000268201_693b08cb0e.jpg,./detection...   \n",
      "\n",
      "                                         caption_seq  \n",
      "0  [[0], [7894], [436], [503], [7894], [394], [67...  \n",
      "1  [[0], [7894], [5177], [5573], [844], [7894], [...  \n",
      "2  [[0], [7894], [8369], [5177], [8007], [844], [...  \n",
      "3  [[0], [7894], [8369], [5177], [8007], [4923], ...  \n",
      "4  [[0], [7894], [8369], [5177], [503], [7894], [...  \n",
      "(40065, 4)\n",
      "8877 50940\n",
      "Data---------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15000 [00:00<?, ?it/s]/home/jhlin/.local/lib/python3.9/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "/home/jhlin/Image-Captioning-via-YOLOv5-EncoderDecoderwithAttention-main/train.py:342: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  input_tensor = torch.tensor(input_np).float().to(device)\n",
      "  7%|▋         | 1000/15000 [01:52<52:25,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 52s (- 26m 18s) (1000 6%) 4.2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 2001/15000 [03:47<46:45,  4.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3m 47s (- 24m 36s) (2000 13%) 3.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3000/15000 [05:35<41:40,  4.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5m 36s (- 22m 24s) (3000 20%) 3.7252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4001/15000 [07:29<40:24,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7m 29s (- 20m 35s) (4000 26%) 3.6866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 5002/15000 [09:15<29:02,  5.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9m 15s (- 18m 30s) (5000 33%) 3.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6001/15000 [11:02<33:07,  4.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11m 1s (- 16m 32s) (6000 40%) 3.5888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7002/15000 [12:52<22:24,  5.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12m 52s (- 14m 43s) (7000 46%) 3.6348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 8001/15000 [14:43<23:37,  4.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14m 42s (- 12m 52s) (8000 53%) 3.4890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9001/15000 [16:28<20:24,  4.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16m 28s (- 10m 58s) (9000 60%) 3.5312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 10001/15000 [18:19<19:34,  4.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18m 19s (- 9m 9s) (10000 66%) 3.4424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 11002/15000 [20:09<11:48,  5.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20m 9s (- 7m 19s) (11000 73%) 3.3984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 12001/15000 [22:00<10:01,  4.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22m 0s (- 5m 30s) (12000 80%) 3.3550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 13002/15000 [23:51<05:29,  6.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23m 51s (- 3m 40s) (13000 86%) 3.4765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 14001/15000 [25:42<03:41,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25m 42s (- 1m 50s) (14000 93%) 3.4220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [27:28<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27m 28s (- 0m 0s) (15000 100%) 3.4716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%run train.py True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98ebf4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction-------------------------------------------------\n",
      "correct A couple with their newborn baby sitting under a tree facing a lake .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['a'], ['the'], ['the'], ['a'], ['the'], ['the'], ['of'], ['a'], ['the'], ['<EOS>']]\n",
      "-------\n",
      "correct A man and woman care for an infant along the side of a body of water .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['a'], ['the'], ['the'], ['a'], ['the'], ['the'], ['of'], ['a'], ['the'], ['the'], ['<EOS>']]\n",
      "-------\n",
      "correct Couple with a baby sit outdoors next to their stroller .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['a'], ['the'], ['the'], ['the'], ['a'], ['the'], ['the'], ['of'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A black dog running in the surf .\n",
      "prediction- [['<SOS>'], ['the'], ['dog'], ['is'], ['running'], ['the'], ['water'], ['the'], ['water'], ['in'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A black lab with tags frolicks in the water .\n",
      "prediction- [['<SOS>'], ['the'], ['dog'], ['is'], ['running'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A dog splashes in the water\n",
      "prediction- [['<SOS>'], ['the'], ['dog'], ['is'], ['running'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct The black dog runs through the water .\n",
      "prediction- [['<SOS>'], ['the'], ['dog'], ['is'], ['running'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct This is a black dog splashing in the water .\n",
      "prediction- [['<SOS>'], ['the'], ['dog'], ['is'], ['running'], ['the'], ['water'], ['the'], ['water'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A man drilling a hole in the ice .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A man is drilling through the frozen ice of a pond .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A person in the snow drilling a hole in the ice .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct A person standing on a frozen lake .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['water'], ['.'], ['<EOS>']]\n",
      "-------\n",
      "correct Two men are ice fishing .\n",
      "prediction- [['<SOS>'], ['a'], ['man'], ['in'], ['a'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['the'], ['water'], ['.']]\n",
      "-------\n",
      "---------------VALIDATION BLEU SCORE----------------------------------- \n",
      "Mean Bleu Score= 0.04334269538376422\n"
     ]
    }
   ],
   "source": [
    "%run train.py False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e3f212",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c28073a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
